

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>1. Preliminaries &mdash; Study Notes in Machine Learning 0.0.1 documentation</title>
  

  
  
  
  

  

  
  
    

  

  
    <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/coloring.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/eqposfix.css" type="text/css" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="prev" title="Study Notes in Machine Learning" href="../../index.html" /> 

  
  <script src="../../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../../index.html" class="icon icon-home"> Study Notes in Machine Learning
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">1. Preliminaries</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#covariance-matrix">1.1. Covariance Matrix</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#property-1-1-classic-representation-of-covariance-by-expectation-or-mean">1.1.1. <span class="ititle">Property 1-1.</span> <span class="bemp">Classic representation of covariance by expectation (or mean).</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#property-1-2-invariance-to-centralization">1.1.2. <span class="ititle">Property 1-2.</span> <span class="bemp">Invariance to centralization.</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#theorem-1-1-matrix-arithmetics-of-covariance-matrix">1.1.3. <span class="ititle">Theorem 1-1.</span> <span class="bemp">Matrix arithmetics of covariance matrix.</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#theorem-1-2-positive-definiteness-of-covariance-matrix">1.1.4. <span class="ititle">Theorem 1-2.</span> <span class="bemp">Positive definiteness of covariance matrix.</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#property-1-3-covariance-represneted-by-rank-1-sum">1.1.5. <span class="ititle">Property 1-3.</span> <span class="bemp">Covariance represneted by rank-1 sum.</span></a></li>
</ul>
</li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Study Notes in Machine Learning</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> &raquo;</li>
        
      <li>1. Preliminaries</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../_sources/3_ml_methods/00_basics/00_index.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: { extensions: ["color.js","autoload-all.js"] }
  });

      MathJax.Hub.Register.StartupHook("TeX color Ready", function() {
   var color = MathJax.Extension["TeX/color"];
   color.colors["theorem"] = color.getColor('RGB','225,229,153');
       color.colors["result"] = color.getColor('RGB','189,214,238');
       color.colors["fact"] = color.getColor('RGB','255,255,204');
       color.colors["emperical"] = color.getColor('RGB','253,240,207');
       color.colors["comment"] = color.getColor('RGB','204,255,204');
   color.colors["thm"] = color.getColor('RGB','225,229,153');
       color.colors["rlt"] = color.getColor('RGB','189,214,238');
       color.colors["emp"] = color.getColor('RGB','253,240,207');
       color.colors["comm"] = color.getColor('RGB','204,255,204');
       color.colors["conn1"] = color.getColor('RGB','255,0,255');
       color.colors["conn2"] = color.getColor('RGB','237,125,49');
       color.colors["conn2"] = color.getColor('RGB','112,48,160');
      });
</script><div class="section" id="preliminaries">
<h1>1. Preliminaries<a class="headerlink" href="#preliminaries" title="Permalink to this headline">¶</a></h1>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: { extensions: ["color.js","autoload-all.js"] }
  });

      MathJax.Hub.Register.StartupHook("TeX color Ready", function() {
   var color = MathJax.Extension["TeX/color"];
   color.colors["theorem"] = color.getColor('RGB','225,229,153');
       color.colors["result"] = color.getColor('RGB','189,214,238');
       color.colors["fact"] = color.getColor('RGB','255,255,204');
       color.colors["emperical"] = color.getColor('RGB','253,240,207');
       color.colors["comment"] = color.getColor('RGB','204,255,204');
   color.colors["thm"] = color.getColor('RGB','225,229,153');
       color.colors["rlt"] = color.getColor('RGB','189,214,238');
       color.colors["emp"] = color.getColor('RGB','253,240,207');
       color.colors["comm"] = color.getColor('RGB','204,255,204');
       color.colors["conn1"] = color.getColor('RGB','255,0,255');
       color.colors["conn2"] = color.getColor('RGB','237,125,49');
       color.colors["conn2"] = color.getColor('RGB','112,48,160');
      });
</script><div class="section" id="covariance-matrix">
<h2>1.1. Covariance Matrix<a class="headerlink" href="#covariance-matrix" title="Permalink to this headline">¶</a></h2>
<p>Given two RVs <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>, then the covariance matrix
<span class="math notranslate nohighlight">\(\colorbox{fact}{$\operatorname{cov} \left( {X,Y} \right) = \mathbb{E}\left[ {\left( {X - \mathbb{E}X} \right)\left( {Y - \mathbb{E}Y} \right)} \right]$}\)</span>.
Given a RV vector <span class="math notranslate nohighlight">\(X = \left( {\begin{array}{*{20}{c}}{{X_1}} \\   \vdots  \\  {{X_M}}\end{array}} \right)\)</span>,
define <span class="math notranslate nohighlight">\(\mathbb{E}X = \left( {\begin{array}{*{20}{c}}{\mathbb{E}{X_1}} \\\vdots  \\{\mathbb{E}{X_M}}\end{array}} \right)\)</span>
(and similarly the expectation of a RV matrix is to take expectation on each entry of that matrix), then the  <span class="def"> <span class="target" id="index-0"></span>covariance matrix</span> is defined as the following,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
σ \left( X \right) &amp; = \left( {\begin{array}{*{20}{c}}
{\operatorname{cov} \left( {{X_1},{X_1}} \right)}&amp; \cdots &amp;{\operatorname{cov} \left( {{X_1},{X_M}} \right)} \\
\vdots &amp; \ddots &amp; \vdots  \\
{\operatorname{cov} \left( {{X_M},{X_1}} \right)}&amp; \cdots &amp;{\operatorname{cov} \left( {{X_M},{X_M}} \right)}
\end{array}} \right) \hfill \\
&amp; = \left( {\begin{array}{*{20}{c}}
{\mathbb{E}\left[ {\left( {{X_1} - \mathbb{E}{X_1}} \right)\left( {{X_1} - \mathbb{E}{X_1}} \right)} \right]}&amp; \cdots &amp;{\mathbb{E}\left[ {\left( {{X_1} - \mathbb{E}{X_1}} \right)\left( {{X_M} - \mathbb{E}{X_M}} \right)} \right]} \\
\vdots &amp; \ddots &amp; \vdots  \\
{\mathbb{E}\left[ {\left( {{X_M} - \mathbb{E}{X_M}} \right)\left( {{X_1} - \mathbb{E}{X_1}} \right)} \right]}&amp; \cdots &amp;{\mathbb{E}\left[ {\left( {{X_M} - \mathbb{E}{X_M}} \right)\left( {{X_M} - \mathbb{E}{X_M}} \right)} \right]}
\end{array}} \right) \hfill \\
&amp;=\colorbox{fact}{$\mathbb{E}\left[ {\left( {X - \mathbb{E}X} \right){{\left( {X - \mathbb{E}X} \right)}^{\text{T}}}} \right]$} \hfill \\
\end{align}\end{split}\]</div>
<p>where the diagonal elements are  <span class="exdef"> <span class="target" id="index-1"></span>variances</span>. We <span class="emp">note</span> there is difference that for two scalar RVs <span class="math notranslate nohighlight">\(X,Y\)</span>, <span class="math notranslate nohighlight">\(\operatorname{cov}⁡(X,Y)\)</span> is a value, but <span class="math notranslate nohighlight">\(σ⁡(X,Y)\)</span> is a <span class="math notranslate nohighlight">\(2×2\)</span> matrix.</p>
<p><span style="padding-left:20px"></span> On the other hand, given two RVs <span class="math notranslate nohighlight">\(X,Y\)</span> and draw samples <span class="math notranslate nohighlight">\({{\bf x}} = \left( {{x_1}, \ldots ,{x_N}} \right)\sim X\)</span>
and <span class="math notranslate nohighlight">\({\mathbf{y}} = \left( {{y_1}, \ldots ,{y_N}} \right)\sim Y\)</span>, then we define the  <span class="def"> <span class="target" id="index-2"></span>sample covariance</span> of them as
<span class="math notranslate nohighlight">\(\colorbox{fact}{$\operatorname{cov} \left( {{{\bf x}},{\mathbf{y}}} \right) = \frac{1}{{N - 1}}{\left( {{{\bf x}} - {\mathbf{\bar x}}} \right)^{\text{T}}}\left( {{\mathbf{y}} - {\mathbf{\bar y}}} \right)$}\)</span>.
Given a RV vector <span class="math notranslate nohighlight">\(X = \left( {\begin{array}{*{20}{c}}{{X_1}} \\\vdots  \\{{X_M}}\end{array}} \right)\)</span>,
we can draw  <span class="def"> <span class="target" id="index-3"></span>samples</span> <span class="math notranslate nohighlight">\({{\bf x}}_1^{\text{T}} = \left( {{x_{1,1}}, \ldots ,{x_{1,N}}} \right)\sim {X_1}, \ldots , {{\bf x}}_M^{\text{T}} = \left( {{x_{M,1}}, \ldots ,{x_{M,M}}} \right)\sim {X_M}\)</span>,
and form a  <span class="exdef"> <span class="target" id="index-4"></span>sample matrix</span> <span class="math notranslate nohighlight">\({{\bf X}} = \left( {\begin{array}{*{20}{c}}{{{\bf x}}_1^{\text{T}}} \\\vdots  \\{{{\bf x}}_M^{\text{T}}}\end{array}} \right)\)</span>.
In the machine learning context, the rows of <span class="math notranslate nohighlight">\(𝐗\)</span> are also referred to as  <span class="def"> <span class="target" id="index-5"></span>feature vectors</span> because it treats the RVs <span class="math notranslate nohighlight">\(X_1,…,X_M\)</span> as representing <span class="math notranslate nohighlight">\(M\)</span> random features of a data point;
and the columns of <span class="math notranslate nohighlight">\(𝐗\)</span> are called  <span class="def"> <span class="target" id="index-6"></span>data entries</span>, because they are actually observed data.
Then the  <span class="def"> <span class="target" id="index-7"></span>sample covariance matrix</span> is defined <span class="red">w.r.t. the feature vectors</span> as</p>
<div class="math notranslate nohighlight" id="equation-eq-sample-cov">
<span class="eqno">(1.1)<a class="headerlink" href="#equation-eq-sample-cov" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned}
{\Sigma }\left( {{\bf X}} \right) &amp;= \frac{1}{{N - 1}}\left( {\begin{array}{*{20}{c}}
{\operatorname{cov} \left( {{{{\bf x}}_1},{{{\bf x}}_1}} \right)}&amp; \cdots &amp;{\operatorname{cov} \left( {{{{\bf x}}_1},{{{\bf x}}_M}} \right)} \\
\vdots &amp; \ddots &amp; \vdots  \\
{\operatorname{cov} \left( {{{{\bf x}}_M},{{{\bf x}}_1}} \right)}&amp; \cdots &amp;{\operatorname{cov} \left( {{{{\bf x}}_M},{{{\bf x}}_M}} \right)}
\end{array}} \right) \hfill \\
&amp;= \frac{1}{{N - 1}}\left( {\begin{array}{*{20}{c}}
{{{\left( {{{{\bf x}}_1} - \overline {{{{\bf x}}_1}} } \right)}^{\text{T}}}\left( {{{{\bf x}}_1} - \overline {{{{\bf x}}_1}} } \right)}&amp; \cdots &amp;{{{\left( {{{{\bf x}}_1} - \overline {{{{\bf x}}_1}} } \right)}^{\text{T}}}\left( {{{{\bf x}}_M} - \overline {{{{\bf x}}_M}} } \right)} \\
\vdots &amp; \ddots &amp; \vdots  \\
{{{\left( {{{{\bf x}}_M} - \overline {{{{\bf x}}_M}} } \right)}^{\text{T}}}\left( {{{{\bf x}}_1} - \overline {{{{\bf x}}_1}} } \right)}&amp; \cdots &amp;{{{\left( {{{{\bf x}}_M} - \overline {{{{\bf x}}_M}} } \right)}^{\text{T}}}\left( {{{{\bf x}}_M} - \overline {{{{\bf x}}_M}} } \right)}
\end{array}} \right) \hfill \\
&amp;= \frac{1}{{N - 1}}\left( {{{\bf X}} - {\mathbf{\bar X}}} \right){\left( {{{\bf X}} - {\mathbf{\bar X}}} \right)^{\text{T}}} \hfill \\
\end{aligned}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\({\overline {{{{\bf x}}_i}} ^{\text{T}}} = \frac{1}{N}\mathop \sum \limits_{j = 1}^N {x_{i,j}}{1^{\text{T}}} = \left( {\frac{1}{N}\mathop \sum \limits_{j = 1}^N {x_{i,j}}, \ldots ,\frac{1}{N}\mathop \sum \limits_{j = 1}^N {x_{i,j}}} \right)\)</span>
(the same mean value repeating itself for <span class="math notranslate nohighlight">\(N\)</span> times)
and <span class="math notranslate nohighlight">\({\mathbf{\bar X}} = \left( {\begin{array}{*{20}{c}}{\overline {{{{\bf x}}_1}} } \\\vdots  \\{\overline {{{{\bf x}}_M}} }\end{array}} \right)\)</span>,
and the diagonal elements are  <span class="exdef"> <span class="target" id="index-8"></span>sample variances</span>. The sum of variances, or the trace of the covariance matrix, is called the  <span class="def"> <span class="target" id="index-9"></span>total variance</span> of <span class="math notranslate nohighlight">\(X\)</span>.
In addition, <span class="emp">note</span> <span class="math notranslate nohighlight">\(\operatorname{cov} \left( {{{\bf x}},{\mathbf{y}}} \right)\)</span> is a value, while <span class="math notranslate nohighlight">\(Σ(x,y)\)</span> is a <span class="math notranslate nohighlight">\(2×2\)</span> matrix.</p>
<div class="admonition caution">
<p class="first admonition-title">Caution</p>
<p>In machine learning problems, we are often given a data matrix <span class="math notranslate nohighlight">\({{\bf X}} = \left( {{{{\bf x}}_1}, \ldots ,{{{\bf x}}_N}} \right)\)</span>
with the columns <span class="math notranslate nohighlight">\({{{\bf x}}_1}, \ldots ,{{{\bf x}}_N}\)</span> as data entries.
The bold small-letter symbol “<span class="math notranslate nohighlight">\({\bf x}\)</span>” very often represents a data entry in the content of machine learning,
but in statistics it often instead represents a feature vector (as in above discussion), and sometimes this causes confusion.
Therefore, we note it is necessary to understand what “<span class="math notranslate nohighlight">\({\bf x}\)</span>” represents in the context.</p>
<p class="last">The other possible confusion is about the “samples”. It is possible both the data entries and feature vectors are referred to as samples in different contexts.
We again <span class="emp">note</span> <span class="red">sample covariance is always w.r.t. the feature vectors, not data entries</span>.
Therefore, the “samples” in <a class="reference internal" href="#equation-eq-sample-cov">Eq.1.1</a> refers to feature vectors.</p>
</div>
<div class="section" id="property-1-1-classic-representation-of-covariance-by-expectation-or-mean">
<span id="property-cov-to-expectation"></span><h3 style="display: inline; font-size:16px"><span class="ititle">Property 1-1.</span> <span class="bemp">Classic representation of covariance by expectation (or mean).</span><a class="headerlink" href="#property-1-1-classic-representation-of-covariance-by-expectation-or-mean" title="Permalink to this headline">¶</a></h3>Using the fact that <span class="fact-highlight"> <span class="math notranslate nohighlight">\(\operatorname{cov}\left( X,Y \right)\mathbb{= E}XY - \mathbb{E}X\mathbb{E}Y\)</span></span> for scalar RVs <span class="math notranslate nohighlight">\(X,Y\)</span>, the covariance matrix has another form
<div class="math notranslate nohighlight">
\[\begin{split}\operatorname{σ}\left( X \right) = \begin{pmatrix}
\mathbb{E}X_{1}^{2} - \mathbb{E}^{2}X_{1} &amp; \cdots &amp; \mathbb{E}{X_{1}X_{M}}\mathbb{- E}X_{1}\mathbb{E}X_{M} \\
\vdots &amp; \ddots &amp; \vdots \\
\mathbb{E}{X_{M}X_{1}}\mathbb{- E}X_{M}\mathbb{E}X_{1} &amp; \cdots &amp; \mathbb{E}X_{M}^{2} - \mathbb{E}^{2}X_{M} \\
\end{pmatrix} = \colorbox{rlt}{$\mathbb{E}\mathrm{\lbrack}XX^{\mathrm{T}}\mathrm{\rbrack} - \mathbb{E}X\mathbb{E}^{\mathrm{T}}X$}\end{split}\]</div>
<p>For two samples <span class="math notranslate nohighlight">\({\bf x}\sim X,\mathbf{y}\sim Y\)</span> where
<span class="math notranslate nohighlight">\(\mathbf{x =}\left( x_{1}\mathbf{,\ldots,}x_{N} \right)\mathbf{,}\mathbf{y = (}y_{1}\mathbf{,\ldots,}y_{N}\mathbf{)}\)</span>,
we have</p>
<div class="math notranslate nohighlight">
\[\left( {\bf x} - \overline{{\bf x}} \right)^{\mathrm{T}}\left( \mathbf{y} - \overline{\mathbf{y}} \right) = {\bf x}^{\mathrm{T}}\mathbf{y} - {\bf x}^{\mathrm{T}}\overline{\mathbf{y}} - {\overline{{\bf x}}}^{\mathrm{T}}\mathbf{y} + {\overline{{\bf x}}}^{\mathrm{T}}\overline{\mathbf{y}}\]</div>
<p>Let <span class="math notranslate nohighlight">\(\overline{x} = \frac{\sum_{i = 1}^{N}{\mathbf{x(}i\mathbf{)}}}{N}\)</span>
and <span class="math notranslate nohighlight">\(\overline{y} = \frac{\sum_{i = 1}^{N}{\mathbf{y(}i\mathbf{)}}}{N}\)</span>,
then</p>
<div class="math notranslate nohighlight">
\[{{\bf x}^{\mathrm{T}}\overline{\mathbf{y}} = \sum_{i = 1}^{N}{\overline{y}\mathbf{x(}i\mathbf{)}} = \overline{y}\sum_{i = 1}^{N}{\mathbf{x(}i\mathbf{)}} = N\overline{x}\overline{y}}\]</div>
<div class="math notranslate nohighlight">
\[{{\overline{{\bf x}}}^{\mathrm{T}}\mathbf{y} = \sum_{i = 1}^{N}{\overline{x}\mathbf{y(}i\mathbf{)}} = \overline{x}\sum_{i = 1}^{N}{\mathbf{y(}i\mathbf{)}} = N\overline{x}\overline{y}}\]</div>
<div class="math notranslate nohighlight">
\[{{\overline{{\bf x}}}^{\mathrm{T}}\overline{\mathbf{y}} = \sum_{i = 1}^{N}{\overline{x}\overline{y}} = N\overline{x}\overline{y}}\]</div>
<p>Thus</p>
<div class="math notranslate nohighlight">
\[\left( {\bf x} - \overline{{\bf x}} \right)^{\mathrm{T}}\left( \mathbf{y} - \overline{\mathbf{y}} \right) = {\bf x}^{\mathrm{T}}\mathbf{x +}N\overline{x}\overline{y} - 2N\overline{x}\overline{y} = {\bf x}^{\mathrm{T}}\mathbf{x -}N\overline{x}\overline{y} = {\bf x}^{\mathrm{T}}\mathbf{y -}{\overline{{\bf x}}}^{\mathrm{T}}\overline{\mathbf{y}}\]</div>
<p>which implies</p>
<div class="math notranslate nohighlight">
\[\begin{split}\Sigma({\bf X}) = \frac{1}{m - 1}\begin{pmatrix}
{\bf x}_{1}^{\mathrm{T}}{\bf x}_{1}{\bf -}{\overline{{\bf x}_{1}}}^{\mathrm{T}}\overline{{\bf x}_{1}} &amp; \cdots &amp; {\bf x}_{1}^{\mathrm{T}}{\bf x}_{M}{\bf -}{\overline{{\bf x}_{1}}}^{\mathrm{T}}\overline{{\bf x}_{M}} \\
\vdots &amp; \ddots &amp; \vdots \\
{\bf x}_{M}^{\mathrm{T}}{\bf x}_{1}{\bf -}{\overline{{\bf x}_{M}}}^{\mathrm{T}}\overline{{\bf x}_{1}} &amp; \cdots &amp; {\bf x}_{M}^{\mathrm{T}}{\bf x}_{M}{\bf -}{\overline{{\bf x}_{M}}}^{\mathrm{T}}\overline{{\bf x}_{M}} \\
\end{pmatrix} = \colorbox{rlt}{$\frac{1}{N - 1}\left( {\bf X}{\bf X}^{\mathrm{T}}{\bf -}\overline{{\bf X}}{\overline{{\bf X}}}^{\mathrm{T}} \right)$}\end{split}\]</div>
</div>
<div class="section" id="property-1-2-invariance-to-centralization">
<span id="property-cov-invariant-to-centralization"></span><h3 style="display: inline; font-size:16px"><span class="ititle">Property 1-2.</span> <span class="bemp">Invariance to centralization.</span><a class="headerlink" href="#property-1-2-invariance-to-centralization" title="Permalink to this headline">¶</a></h3>For any random vector <span class="math notranslate nohighlight">\(X\)</span>, we have <span class="result-highlight"> <span class="math notranslate nohighlight">\(σ\left( X - \mathbb{E}X \right) = σ(X)\)</span></span>, since <span class="math notranslate nohighlight">\(\mathbb{E}\left\lbrack X - \mathbb{E}X \right\rbrack = \mathbf{0}\)</span> and
<div class="math notranslate nohighlight">
\[σ\left( X - \mathbb{E}X \right)
= \mathbb{E}{\lbrack{\left( X - \mathbb{E}X - \mathbb{E}\left\lbrack X - \mathbb{E}X \right\rbrack \right)\left( X - \mathbb{E}X - \mathbb{E}\left\lbrack X - \mathbb{E}X \right\rbrack \right)}^{\mathrm{T}}\rbrack}
= \mathbb{E}{\lbrack{\left( X - \mathbb{E}X \right)\left( X - \mathbb{E}X \right)}^{\mathrm{T}}\rbrack} = σ(X)\]</div>
<p>Similarly <span class="result-highlight"> <span class="math notranslate nohighlight">\(\Sigma\left( {\bf X} - \overline{{\bf X}} \right) = \Sigma\left( {\bf X} \right)\)</span></span>,
because <span class="math notranslate nohighlight">\({\bf x} - \overline{{\bf x}} - \overline{{\bf x} - \overline{{\bf x}}} = {\bf x} - \overline{{\bf x}}\)</span>
for any sample <span class="math notranslate nohighlight">\({\bf x}\)</span>, and the result following by applying this on <a class="reference internal" href="#equation-eq-sample-cov">Eq.1.1</a>.</p>
</div>
<div class="section" id="theorem-1-1-matrix-arithmetics-of-covariance-matrix">
<span id="theorem-cov-matrix-arithmetic-rules"></span><h3 style="display: inline; font-size:16px"><span class="ititle">Theorem 1-1.</span> <span class="bemp">Matrix arithmetics of covariance matrix.</span><a class="headerlink" href="#theorem-1-1-matrix-arithmetics-of-covariance-matrix" title="Permalink to this headline">¶</a></h3><span class="theorem-highlight"> Given <span class="math notranslate nohighlight">\(X = \left( X_{1},\ldots,X_{n} \right)^{\mathrm{T}}\)</span>,
<span class="math notranslate nohighlight">\(σ\left(\mathrm{\mathbf{α}}^{\mathrm{T}}X \right) = σ\left( X^{\mathrm{T}}\mathrm{\mathbf{α}} \right) = \mathrm{\mathbf{α}}^{\mathrm{T}}\operatorname{σ}\left( X \right)\mathrm{\mathbf{α}}\)</span></span>.
<span class="emp">Note</span> <span class="math notranslate nohighlight">\(α^{\rm{T}}σ{X}\)</span> is a scalar random variable, and
<div class="math notranslate nohighlight">
\[\mathbb{E}\left( \mathrm{\mathbf{α}}^{\mathrm{T}}X \right)\mathbb{= E}\left( X^{\mathrm{T}}\mathrm{\mathbf{α}} \right) = \mathrm{\mathbf{α}}^{\mathrm{T}}\mathbb{E}X = \left( \mathbb{E}^{\mathrm{T}}X \right)\mathrm{\mathbf{α}}\]</div>
<p>Also <span class="math notranslate nohighlight">\(\mathrm{\mathbf{α}}^{\mathrm{T}}X\)</span> is a scalar RV, and so
<span class="math notranslate nohighlight">\(\left( \mathrm{\mathbf{α}}^{\mathrm{T}}X \right)^{2}={\left( \mathrm{\mathbf{α}}^{\mathrm{T}}X \right)\left( \mathrm{\mathbf{α}}^{\mathrm{T}}X \right)}^{\mathrm{T}} = \mathrm{\mathbf{α}}^{\mathrm{T}}XX^{\mathrm{T}}\mathrm{\mathbf{α}}\)</span>.
Recall <span class="math notranslate nohighlight">\(σ\left( X \right)\mathbb{= E}X^{2} - \mathbb{E}^{2}X\)</span>, then</p>
<div class="math notranslate nohighlight">
\[σ\left( \mathrm{\mathbf{α}}^{\mathrm{T}}X \right) = \mathbb{E}\mathrm{\lbrack}\mathrm{\mathbf{α}}^{\mathrm{T}}X\left( \mathrm{\mathbf{α}}^{\mathrm{T}}X \right)^{\mathrm{T}}\mathrm{\rbrack} - \mathbb{E}\left\lbrack \mathrm{\mathbf{α}}^{\mathrm{T}}X \right\rbrack\mathbb{E}^{\mathrm{T}}\left\lbrack \mathrm{\mathbf{α}}^{\mathrm{T}}X \right\rbrack = \mathrm{\mathbf{α}}^{\mathrm{T}}\mathbb{E}\mathrm{\lbrack}XX^{\mathrm{T}}\mathrm{\rbrack}\mathrm{\mathbf{α}} - \mathrm{\mathbf{α}}^{\mathrm{T}}\mathbb{E}X\mathbb{E}^{\mathrm{T}}X\mathrm{\mathbf{α}}=\mathrm{\mathbf{α}}^{\mathrm{T}}\mathbf{(}\mathbb{E}\mathrm{\lbrack}XX^{\mathrm{T}}\mathrm{\rbrack} - \mathbb{E}X\mathbb{E}^{\mathrm{T}}X\mathrm{)}\mathrm{\mathbf{α}}=\mathrm{\mathbf{α}}^{\mathrm{T}}\operatorname{σ}\left( X \right)\mathrm{\mathbf{α}}\]</div>
<p>Similarly, using
<span class="math notranslate nohighlight">\(σ\left( X,Y \right)\mathbb{= E}XY - \mathbb{E}X\mathbb{E}Y\)</span>, we
can have
<span class="theorem-highlight"> <span class="math notranslate nohighlight">\(σ\left( \mathrm{\mathbf{α}}^{\mathrm{T}}X,\mathbf{β}^\mathrm{T}Y \right) = \mathrm{\mathbf{α}}^\mathrm{T}\operatorname{σ}\left( X \right)\mathbf{β}\)</span></span>.
Further, if we let
<span class="math notranslate nohighlight">\(\mathbf{A}\mathbf{= (}\mathbf{a}_{1}\mathbf{,\ldots,}\mathbf{a}_{n}\mathbf{)}\)</span>,
then
<span class="theorem-highlight"> <span class="math notranslate nohighlight">\(σ\left( \mathbf{A}^\mathrm{T}X \right) = \mathbf{A}^\mathrm{T}\operatorname{σ}\left( X \right)\mathbf{A}\)</span></span>,
since
<span class="math notranslate nohighlight">\(σ\left( \mathrm{\mathbf{α}}_{i}X\mathbf{,}\mathrm{\mathbf{α}}_{j}X \right) = \mathrm{\mathbf{α}}_{i}^\mathrm{T}\operatorname{σ}\left( X \right)\mathrm{\mathbf{α}}_{j}\)</span>.</p>
<p>On the other hand, given
<span class="math notranslate nohighlight">\(\mathbf{X}=(\mathbf{x}_{1},\ldots,\mathbf{x}_{n}\mathbf{)}\)</span>, we have that
<span class="math notranslate nohighlight">\(\Sigma \left( \mathbf{\text{Xα}} \right) = \mathrm{\mathbf{α}}^\mathrm{T}\Sigma\left( {\bf X} \right)\mathrm{\mathbf{α}}\)</span>.
First check</p>
<div class="math notranslate nohighlight">
\[\begin{split}\left\{ \begin{matrix}
\overline{\mathbf{\text{Xα}}} = \overline{\sum_{i = 1}^{n}{\mathrm{\mathbf{α}}\left( i \right)\mathbf{x}_{i}}} = \frac{1}{m}\sum_{j = 1}^{m}{\sum_{i = 1}^{n}{\mathrm{\mathbf{α}}\left( i \right)\mathbf{x}_{i}(j)}} \\
\overline{{\bf X}}\mathrm{\mathbf{α}}=\sum_{i = 1}^{n}{\mathrm{\mathbf{α}}\left( i \right)\overline{\mathbf{x}_{i}}} = \sum_{i = 1}^{n}\left( \mathrm{\mathbf{α}}\left( i \right) \times \frac{1}{m}\sum_{j = 1}^{m}{\mathbf{x}_{i}\left( j \right)} \right) = \frac{1}{m}\sum_{i = 1}^{n}\left( \sum_{j = 1}^{m}{\mathrm{\mathbf{α}}\left( i \right)\mathbf{x}_{i}\left( j \right)} \right) \\
\end{matrix} \Rightarrow \colorbox{rlt}{$\overline{\mathbf{\text{Xα}}} = \overline{{\bf X}}\mathrm{\mathbf{α}}$} \right.\end{split}\]</div>
<p>Then we have</p>
<div class="math notranslate nohighlight">
\[\Sigma\left( \mathbf{\text{Xα}} \right) = \frac{1}{n + 1}\left( \left( \mathbf{\text{Xα}} \right)\mathrm{T}\left( \mathbf{\text{Xα}} \right)\mathbf{-}{\overline{\mathbf{\text{Xα}}}}^\mathrm{T}\overline{\mathbf{\text{Xα}}} \right) = \frac{1}{n + 1}\left( \mathrm{\mathbf{α}}^\mathrm{T}{\bf X}^\mathrm{T}\mathbf{Xα -}\mathrm{\mathbf{α}}^\mathrm{T}{\overline{{\bf X}}}^\mathrm{T}\overline{{\bf X}}\mathrm{\mathbf{α}} \right) = \mathrm{\mathbf{α}}^\mathrm{T}\Sigma\left( {\bf X} \right)\mathrm{\mathbf{α}}\]</div>
<p>By similar computations,
<span class="math notranslate nohighlight">\(\Sigma\left( \mathbf{Xα,Xβ} \right) = \mathrm{\mathbf{α}}^\mathrm{T}\Sigma\left( {\bf X} \right)\mathbf{β}\)</span>.
Let <span class="math notranslate nohighlight">\(\mathbf{Y} = \mathbf{\text{XA}}\)</span> for any matrix
<span class="math notranslate nohighlight">\(\mathbf{A}\mathbf{= (}\mathbf{a}_{1}\mathbf{,\ldots,}\mathbf{a}_{n}\mathbf{)}\)</span>,
then
<span class="theorem-highlight"> <span class="math notranslate nohighlight">\(\Sigma(\mathbf{\text{XA}}) = \mathbf{A}^\mathrm{T}\Sigma({\bf X})\mathbf{A}\)</span></span>,
since <span class="math notranslate nohighlight">\(\Sigma\left( {\bf X}^\mathrm{T}\mathrm{\mathbf{a}}_{i}\mathbf{,}{\bf X}^\mathrm{T}\mathrm{\mathbf{a}}_{j} \right) = \mathrm{\mathbf{a}}_{i}^\mathrm{T}\Sigma\left( {\bf X} \right)\mathrm{\mathbf{a}}_{j}\)</span>.</p>
</div>
<div class="section" id="theorem-1-2-positive-definiteness-of-covariance-matrix">
<span id="theorem-cov-semipositiveness"></span><h3 style="display: inline; font-size:16px"><span class="ititle">Theorem 1-2.</span> <span class="bemp">Positive definiteness of covariance matrix.</span><a class="headerlink" href="#theorem-1-2-positive-definiteness-of-covariance-matrix" title="Permalink to this headline">¶</a></h3><span class="theorem-highlight"> Covariance matrix is clearly symmetric, and moreover they are semi-positive definite</span>, since for any constant vector <span class="math notranslate nohighlight">\(\mathbf{α}\)</span>, using <a class="reference internal" href="01_cov.html#theorem-cov-matrix-arithmetic-rules"><span class="std std-ref">Theorem 1-1</span></a>, we have
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
{{\mathbf{α }}^{\text{T}}}\left( {σ \left( X \right)} \right){\mathbf{α }} &amp;= σ \left( {{{\mathbf{α }}^{\text{T}}}X} \right) \hfill \\
&amp;= \mathbb{E}\left[ {\left( {{{\mathbf{α }}^{\text{T}}}X - \mathbb{E}{{\mathbf{α }}^{\text{T}}}X} \right){{\left( {{{\mathbf{α }}^{\text{T}}}X - \mathbb{E}{{\mathbf{α }}^{\text{T}}}X} \right)}^{\text{T}}}} \right] = \mathbb{E}\left[ {\left( {{{\mathbf{α }}^{\text{T}}}X - {{\mathbf{α }}^{\text{T}}}\mathbb{E}X} \right){{\left( {{{\mathbf{α }}^{\text{T}}}X - {{\mathbf{α }}^{\text{T}}}\mathbb{E}X} \right)}^{\text{T}}}} \right] \hfill \\
&amp;= \mathbb{E}\left[ {{{\mathbf{α }}^{\text{T}}}\left( {X - \mathbb{E}X} \right){{\left( {X - \mathbb{E}X} \right)}^{\text{T}}}{\mathbf{α }}} \right] = \mathbb{E}\left[ {{{\left( {{{\left( {X - \mathbb{E}X} \right)}^{\text{T}}}{\mathbf{α }}} \right)}^{\text{T}}}\left( {{{\left( {X - \mathbb{E}X} \right)}^{\text{T}}}{\mathbf{α }}} \right)} \right] \geqslant 0 \hfill \\
\end{aligned}\end{split}\]</div>
<p>For sample covariance matrix, check that</p>
<div class="math notranslate nohighlight">
\[\mathbf{α}^{\rm{T}}\left( \Sigma\left( \mathbf{X} \right) \right)\mathbf{α}
=\Sigma\left( \mathbf{\text{Xα}} \right) \propto \left( \mathbf{\text{Xα}} - \overline{\mathbf{\text{Xα}}} \right)^{\rm{T}}\left( \mathbf{\text{Xα}} - \overline{\mathbf{\text{Xα}}} \right) \geq 0\]</div>
</div>
<div class="section" id="property-1-3-covariance-represneted-by-rank-1-sum">
<span id="property-cov-as-rank1-sum"></span><h3 style="display: inline; font-size:16px"><span class="ititle">Property 1-3.</span> <span class="bemp">Covariance represneted by rank-1 sum.</span><a class="headerlink" href="#property-1-3-covariance-represneted-by-rank-1-sum" title="Permalink to this headline">¶</a></h3>Recall <span class="math notranslate nohighlight">\(\mathbf{X} = \begin{pmatrix} \mathbf{x}_{1}^{\rm{T}} \\  \vdots \\ \mathbf{x}_{M}^{\rm{T}} \\ \end{pmatrix}\)</span> are feature vectors, and the covariance matrix in <a class="reference internal" href="#equation-eq-sample-cov">Eq.1.1</a> is defined w.r.t. the feature vectors. Suppose
<span class="math notranslate nohighlight">\(\mathbf{X} = \left( \mathbf{𝓍}_{1},\ldots,\mathbf{𝓍}_{N} \right)\)</span>
where <span class="math notranslate nohighlight">\(\mathbf{𝓍}_{1},\ldots,\mathbf{𝓍}_{N}\)</span> are columns of
<span class="math notranslate nohighlight">\(\mathbf{X}\)</span> as data entries,
and let <span class="math notranslate nohighlight">\(\overline{\mathbf{𝓍}} = \frac{1}{N}\sum_{i = 1}^{N}\mathbf{𝓍}_{i}\)</span> be
the mean vector of all data entries. Then we show
<span class="math notranslate nohighlight">\(\Sigma\left( \mathbf{X} \right)\)</span> can also be represented by
sum of rank-1 addends dependent on the data entries (rather than the
feature vectors) as
<div class="math notranslate nohighlight">
\[\Sigma\left( \mathbf{X} \right)
= \frac{1}{N - 1}\sum_{k = 1}^{N}{\left( \mathbf{𝓍}_{k} - \overline{\mathbf{𝓍}} \right)\left( \mathbf{𝓍}_{k} - \overline{\mathbf{𝓍}} \right)^{\rm{T}}}\]</div>
<p>Note <span class="math notranslate nohighlight">\(\mathbf{𝓍}_{j}\left( i \right) = x_{i,j} = \mathbf{x}_{i}\left( j \right)\)</span>
and <span class="math notranslate nohighlight">\(\overline{\mathbf{𝓍}}\left( i \right)\mathbf{=}\overline{x_{i}} = \frac{1}{N}\sum_{j = 1}^{N}x_{i,j}\)</span>, let <span class="math notranslate nohighlight">\(\mathbf{\Sigma}:=\Sigma(\mathbf{X})\)</span>,
then</p>
<div class="math notranslate nohighlight">
\[\mathbf{\Sigma}\left( i,j \right)
= \sum_{k = 1}^{N}{\left( \left( \mathbf{𝓍}_{k} - \overline{\mathbf{𝓍}} \right)\left( \mathbf{𝓍}_{k} - \overline{\mathbf{𝓍}} \right)^{\rm{T}} \right)\left( i,j \right)}
= \sum_{k = 1}^{N}{\left( \mathbf{𝓍}_{k}\left( i \right) - \overline{x_{i}} \right)\left( \mathbf{𝓍}_{k}\left( j \right) - \overline{x_{j}} \right)}
= \sum_{k = 1}^{N}{\left( \mathbf{x}_{i}\left( k \right) - \overline{x_{i}} \right)\left( \mathbf{x}_{j}\left( k \right) - \overline{x_{j}} \right)}
= \left( \mathbf{x}_{i} - \overline{\mathbf{x}_{i}} \right)^{\rm{T}}\left( \mathbf{x}_{j} - \overline{\mathbf{x}_{j}} \right)\]</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
      
        <a href="../../index.html" class="btn btn-neutral" title="Study Notes in Machine Learning" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Tony Chen, Drexel University.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../../',
            VERSION:'0.0.1',
            LANGUAGE:'None',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../../_static/jquery.js"></script>
      <script type="text/javascript" src="../../_static/underscore.js"></script>
      <script type="text/javascript" src="../../_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="../../_static/js/theme.js"></script>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>