Methods in Machine Learning
===========================

+---------+---------+---------+---------+---------+---------+---------+
| **Conce | **Refer | Cite    | **Earli | **Title | *Hint*  | Assumpt |
| pt**    | ence**  |         | er      | **      |         | ion     |
|         |         |         | Concept |         |         |         |
|         |         |         | **      |         |         | Attenti |
|         |         |         |         |         |         | on      |
|         |         |         | **Exter |         |         |         |
|         |         |         | nal     |         |         |         |
|         |         |         | Concept |         |         |         |
|         |         |         | **      |         |         |         |
+=========+=========+=========+=========+=========+=========+=========+
| Theorem | Lemma/P | Fact    | Empiric | Advanta | Connect | Connect |
|         | roperty |         | al      | ge      | ion     | ion     |
|         |         |         | Fact    |         |         |         |
|         |         |         |         | Limitat |         |         |
|         |         |         |         | ion     |         |         |
+---------+---------+---------+---------+---------+---------+---------+

+-----------------+-----------------+-----------------+-----------------+
| RV:             | random variable |                 |                 |
+-----------------+-----------------+-----------------+-----------------+
| .. math:: \math | single data     | .. math:: \math | data matrix     |
| bf{x}           | entry as column | bf{X}           | with each       |
|                 | vector          |                 | column as one   |
|                 |                 |                 | data entry      |
+-----------------+-----------------+-----------------+-----------------+
| .. math:: \math | weights         | .. math:: \math | weight matrix   |
| bf{w}           |                 | bf{W}           | with each       |
|                 |                 |                 | column as one   |
|                 |                 |                 | set of weights  |
+-----------------+-----------------+-----------------+-----------------+
| .. math:: \math | optimal         |                 |                 |
| bf{w}^{\mathbf{ | solution is     |                 |                 |
| *}}             | marked by       |                 |                 |
|                 | :math:`*`       |                 |                 |
+-----------------+-----------------+-----------------+-----------------+
|                 |                 |                 |                 |
+-----------------+-----------------+-----------------+-----------------+

**Basic Models**
----------------

+-----------------------+-----------------------+-----------------------+
|                       |                       |                       |
+-----------------------+-----------------------+-----------------------+
| Binary Logistic       | sigmoid, i.i.d.       | closed form, gradient |
|                       | Bernoulli             | ascent, Newton        |
+-----------------------+-----------------------+-----------------------+

Preliminaries
~~~~~~~~~~~~~

**Covariance Matrix**
^^^^^^^^^^^^^^^^^^^^^

Given two RVs :math:`X,Y`, then the covariance
:math:`\operatorname{cov}\left( X,Y \right)\mathbb{= E\lbrack}\left( X - \mathbb{E}X \right)\left( Y - \mathbb{E}Y \right)\rbrack`.
Given a RV vector :math:`X = \begin{pmatrix}
X_{1} \\
 \vdots \\
X_{M} \\
\end{pmatrix}`, define :math:`\mathbb{E}X = \begin{pmatrix}
\mathbb{E}X_{1} \\
 \vdots \\
\mathbb{E}X_{M} \\
\end{pmatrix}` (and similarly the expectation of a RV matrix is to take
expectation on each entry of that matrix), then the **covariance
matrix** is defined as the following,

.. math::

   \operatorname{\sigma}\left( X \right) = \begin{pmatrix}
   \operatorname{cov}{(X_{1},X_{1})} & \cdots & \operatorname{cov}{(X_{1},X_{M})} \\
    \vdots & \ddots & \vdots \\
   \operatorname{cov}{(X_{M},X_{1})} & \cdots & \operatorname{cov}{(X_{M},X_{M})} \\
   \end{pmatrix} = \begin{pmatrix}
   \operatorname{E}{\lbrack\left( X_{1}\mathbb{- E}X_{1} \right)\left( X_{1}\mathbb{- E}X_{1} \right)\rbrack} & \cdots & \operatorname{E}{\lbrack\left( X_{1}\mathbb{- E}X_{1} \right)\left( X_{M}\mathbb{- E}X_{M} \right)\rbrack} \\
    \vdots & \ddots & \vdots \\
   \operatorname{E}{\lbrack\left( X_{M}\mathbb{- E}X_{M} \right)\left( X_{1}\mathbb{- E}X_{1} \right)\rbrack} & \cdots & \operatorname{E}{\lbrack\left( X_{M}\mathbb{- E}X_{M} \right)\left( X_{M}\mathbb{- E}X_{M} \right)\rbrack} \\
   \end{pmatrix} = \operatorname{E}{\lbrack{\left( X - \mathbb{E}X \right)\left( X - \mathbb{E}X \right)}^{T}\rbrack}

where the diagonal elements are variances. We *note* there is difference
that :math:`\operatorname{cov}\left( X,Y \right)` is a value, but
:math:`\operatorname{\sigma}\left( X,Y \right)` is a :math:`2 \times 2`
matrix.

On the other hand, given two RVs :math:`X,Y` and draw samples
:math:`\mathbf{x =}\left( x_{1}\mathbf{,\ldots,}x_{N} \right)\sim X,\mathbf{y =}\left( y_{1},\ldots,y_{N} \right)\sim Y`,
then we define the **sample** **covariance** of them as
:math:`\operatorname{cov}\left( \mathbf{x,y} \right) = \frac{1}{N - 1}\left( \mathbf{x} - \overline{\mathbf{x}} \right)^{T}\left( \mathbf{y} - \overline{\mathbf{y}} \right)`.
Given a RV vector :math:`X = \begin{pmatrix}
X_{1} \\
 \vdots \\
X_{M} \\
\end{pmatrix}`, we can draw **samples**
:math:`\mathbf{x}_{1}^{T} = \left( x_{1,1},\ldots,x_{1,N} \right)\sim X_{1},\ldots,\mathbf{x}_{M}^{T} = \left( x_{M,1},\ldots,x_{M,M} \right)\sim X_{M}`,
and form a **sample matrix** :math:`\mathbf{X} = \begin{pmatrix}
\mathbf{x}_{1}^{T} \\
 \vdots \\
\mathbf{x}_{M}^{T} \\
\end{pmatrix}`. In the machine learning context, the rows of
:math:`\mathbf{X}` are also referred to as **feature vectors**, and the
columns of :math:`\mathbf{X}` are called **data entries**. Then the
**sample covariance matrix** is defined w.r.t. the feature vectors as

+-----------------------+-----------------------+-----------------------+
| .. math::             |                       | (1‑1)                 |
|                       |                       |                       |
|    \Sigma\left( \math |                       |                       |
| bf{X} \right) = \frac |                       |                       |
| {1}{N - 1}\begin{pmat |                       |                       |
| rix}                  |                       |                       |
|    \operatorname{cov} |                       |                       |
| {(\mathbf{x}_{1},\mat |                       |                       |
| hbf{x}_{1})} & \cdots |                       |                       |
|  & \operatorname{cov} |                       |                       |
| {(\mathbf{x}_{1},\mat |                       |                       |
| hbf{x}_{M})} \\       |                       |                       |
|     \vdots & \ddots & |                       |                       |
|  \vdots \\            |                       |                       |
|    \operatorname{cov} |                       |                       |
| {(\mathbf{x}_{M},\mat |                       |                       |
| hbf{x}_{1})} & \cdots |                       |                       |
|  & \operatorname{cov} |                       |                       |
| {(\mathbf{x}_{M},\mat |                       |                       |
| hbf{x}_{M})} \\       |                       |                       |
|    \end{pmatrix} = \f |                       |                       |
| rac{1}{N - 1}\begin{p |                       |                       |
| matrix}               |                       |                       |
|    \left( \mathbf{x}_ |                       |                       |
| {1} - \overline{\math |                       |                       |
| bf{x}_{1}} \right)^{T |                       |                       |
| }\left( \mathbf{x}_{1 |                       |                       |
| } - \overline{\mathbf |                       |                       |
| {x}_{1}} \right) & \c |                       |                       |
| dots & \left( \mathbf |                       |                       |
| {x}_{1} - \overline{\ |                       |                       |
| mathbf{x}_{1}} \right |                       |                       |
| )^{T}\left( \mathbf{x |                       |                       |
| }_{M} - \overline{\ma |                       |                       |
| thbf{x}_{M}} \right)  |                       |                       |
| \\                    |                       |                       |
|     \vdots & \ddots & |                       |                       |
|  \vdots \\            |                       |                       |
|    \left( \mathbf{x}_ |                       |                       |
| {M} - \overline{\math |                       |                       |
| bf{x}_{M}} \right)^{T |                       |                       |
| }\left( \mathbf{x}_{1 |                       |                       |
| } - \overline{\mathbf |                       |                       |
| {x}_{1}} \right) & \c |                       |                       |
| dots & \left( \mathbf |                       |                       |
| {x}_{M} - \overline{\ |                       |                       |
| mathbf{x}_{M}} \right |                       |                       |
| )^{T}\left( \mathbf{x |                       |                       |
| }_{M} - \overline{\ma |                       |                       |
| thbf{x}_{M}} \right)  |                       |                       |
| \\                    |                       |                       |
|    \end{pmatrix} = \f |                       |                       |
| rac{1}{N - 1}\left( \ |                       |                       |
| mathbf{X} - \overline |                       |                       |
| {\mathbf{X}} \right)\ |                       |                       |
| left( \mathbf{X} - \o |                       |                       |
| verline{\mathbf{X}} \ |                       |                       |
| right)^{T}            |                       |                       |
+-----------------------+-----------------------+-----------------------+

where
:math:`{\overline{\mathbf{x}_{i}}}^{T} = \frac{1}{N}\sum_{j = 1}^{N}x_{i,j}\mathbf{1}^{T} = \left( \frac{1}{N}\sum_{j = 1}^{N}x_{i,j},\ldots,\frac{1}{N}\sum_{j = 1}^{N}x_{i,j} \right)`
(the same mean value repeats itself for :math:`N` times) and
:math:`\overline{\mathbf{X}} = \begin{pmatrix}
\overline{\mathbf{x}_{1}} \\
 \vdots \\
\overline{\mathbf{x}_{M}} \\
\end{pmatrix}`, and the diagonal elements are **sample variances**. The
sum of variances, or the trace of the covariance matrix, is called the
**total variance** of :math:`\mathbf{X}`. In addition, *note*
:math:`\operatorname{cov}\left( \mathbf{x,y} \right)` is a value, while
:math:`\Sigma(\mathbf{x,y})` is a :math:`2 \times 2` matrix.

+-----------------------------------------------------------------------+
| In machine learning problems, we are often given a data matrix        |
| :math:`\mathbf{X} = \left( \mathbf{x}_{1},\ldots,\mathbf{x}_{N} \righ |
| t)`                                                                   |
| with the columns :math:`\mathbf{x}_{1},\ldots,\mathbf{x}_{N}` as      |
| **data entries**. The symbol “\ :math:`\mathbf{x}`\ ” very often      |
| represents a data entry in machine learning, but in statistics it     |
| often instead represents a feature vector. This difference sometimes  |
| causes confusion. Therefore, we *note* it is necessary to understand  |
| what “\ :math:`\mathbf{x}`\ ” represents from the context.            |
|                                                                       |
| The other possible confusion is about the "samples". It is possible   |
| both the data entries and feature vectors are referred to as samples  |
| in different contexts. We again *note* sample covariance is w.r.t.    |
| the features, not data entries. Therefore, the “samples” in the       |
| context (*1*\ ‑\ *1*) refers to feature vectors.                      |
+-----------------------------------------------------------------------+

-  **Property** **1‑1** Using the fact that
   :math:`\sigma\left( X,Y \right)\mathbb{= E}XY - \mathbb{E}X\mathbb{E}Y`,
   the covariance matrix has another form

.. math::

   \operatorname{\sigma}\left( X \right) = \begin{pmatrix}
   \operatorname{E}X_{1}^{2} - \mathbb{E}^{2}X_{1} & \cdots & \operatorname{E}{X_{1}X_{M}}\mathbb{- E}X_{1}\mathbb{E}X_{M} \\
    \vdots & \ddots & \vdots \\
   \operatorname{E}{X_{M}X_{1}}\mathbb{- E}X_{M}\mathbb{E}X_{1} & \cdots & \operatorname{E}X_{M}^{2} - \mathbb{E}^{2}X_{M} \\
   \end{pmatrix} = \mathbf{E}\mathrm{\lbrack}XX^{\mathrm{T}}\mathrm{\rbrack} - \mathbf{E}X\mathbf{E}^{T}X

For two samples :math:`\mathbf{x}\sim X,\mathbf{y}\sim Y` where
:math:`\mathbf{x =}\left( x_{1}\mathbf{,\ldots,}x_{N} \right)\mathbf{,}\mathbf{y = (}y_{1}\mathbf{,\ldots,}y_{N}\mathbf{)}`,
we have

.. math:: \left( \mathbf{x} - \overline{\mathbf{x}} \right)^{T}\left( \mathbf{y} - \overline{\mathbf{y}} \right) = \mathbf{x}^{T}\mathbf{y} - \mathbf{x}^{T}\overline{\mathbf{y}} - {\overline{\mathbf{x}}}^{T}\mathbf{y} + {\overline{\mathbf{x}}}^{T}\overline{\mathbf{y}}

Let
:math:`\overline{x} = \frac{\sum_{i = 1}^{N}{\mathbf{x(}i\mathbf{)}}}{N}`
and
:math:`\overline{y} = \frac{\sum_{i = 1}^{N}{\mathbf{y(}i\mathbf{)}}}{N}`,
then

.. math::

   {\mathbf{x}^{T}\overline{\mathbf{y}} = \sum_{i = 1}^{N}{\overline{y}\mathbf{x(}i\mathbf{)}} = \overline{y}\sum_{i = 1}^{N}{\mathbf{x(}i\mathbf{)}} = N\overline{x}\overline{y}
   }{{\overline{\mathbf{x}}}^{T}\mathbf{y} = \sum_{i = 1}^{N}{\overline{x}\mathbf{y(}i\mathbf{)}} = \overline{x}\sum_{i = 1}^{N}{\mathbf{y(}i\mathbf{)}} = N\overline{x}\overline{y}
   }{{\overline{\mathbf{x}}}^{T}\overline{\mathbf{y}} = \sum_{i = 1}^{N}{\overline{x}\overline{y}} = N\overline{x}\overline{y}}

Thus

.. math:: \left( \mathbf{x} - \overline{\mathbf{x}} \right)^{T}\left( \mathbf{y} - \overline{\mathbf{y}} \right) = \mathbf{x}^{T}\mathbf{x +}N\overline{x}\overline{y} - 2N\overline{x}\overline{y} = \mathbf{x}^{T}\mathbf{x -}N\overline{x}\overline{y} = \mathbf{x}^{T}\mathbf{y -}{\overline{\mathbf{x}}}^{T}\overline{\mathbf{y}}

which implies

.. math::

   \Sigma(\mathbf{X}) = \frac{1}{m - 1}\begin{pmatrix}
   \mathbf{x}_{1}^{T}\mathbf{x}_{1}\mathbf{-}{\overline{\mathbf{x}_{1}}}^{T}\overline{\mathbf{x}_{1}} & \cdots & \mathbf{x}_{1}^{T}\mathbf{x}_{M}\mathbf{-}{\overline{\mathbf{x}_{1}}}^{T}\overline{\mathbf{x}_{M}} \\
    \vdots & \ddots & \vdots \\
   \mathbf{x}_{M}^{T}\mathbf{x}_{1}\mathbf{-}{\overline{\mathbf{x}_{M}}}^{T}\overline{\mathbf{x}_{1}} & \cdots & \mathbf{x}_{M}^{T}\mathbf{x}_{M}\mathbf{-}{\overline{\mathbf{x}_{M}}}^{T}\overline{\mathbf{x}_{M}} \\
   \end{pmatrix} = \frac{1}{N - 1}\left( \mathbf{X}\mathbf{X}^{T}\mathbf{-}\overline{\mathbf{X}}{\overline{\mathbf{X}}}^{T} \right)

-  **Property** **1‑2**
   :math:`\sigma\left( X - \mathbb{E}X \right) = \sigma(X)`, since
   :math:`\mathbb{E}\left\lbrack X - \mathbb{E}X \right\rbrack = \mathbf{0}`
   and

.. math:: \sigma\left( X - \mathbb{E}X \right) = \operatorname{E}{\lbrack{\left( X - \mathbb{E}X - \mathbb{E}\left\lbrack X - \mathbb{E}X \right\rbrack \right)\left( X - \mathbb{E}X - \mathbb{E}\left\lbrack X - \mathbb{E}X \right\rbrack \right)}^{T}\rbrack} = \operatorname{E}{\lbrack{\left( X - \mathbb{E}X \right)\left( X - \mathbb{E}X \right)}^{T}\rbrack} = \sigma(X)

Also
:math:`\Sigma\left( \mathbf{X} - \overline{\mathbf{X}} \right) = \Sigma\left( \mathbf{X} \right)`,
because
:math:`\mathbf{x} - \overline{\mathbf{x}} - \overline{\mathbf{x} - \overline{\mathbf{x}}} = \mathbf{x} - \overline{\mathbf{x}}`
for any sample :math:`\mathbf{x}`, and the result following by applying
this on (*1*\ ‑\ *1*).

-  **Theorem** **1‑1** Given
   :math:`X = \left( X_{1},\ldots,X_{n} \right)^{T}`,
   :math:`\sigma\left( \mathbf{\alpha}^{T}X \right) = \sigma\left( X^{T}\mathbf{\alpha} \right) = \mathbf{\alpha}^{T}\operatorname{\sigma}\left( X \right)\mathbf{\alpha}`.
   Note

.. math:: \mathbb{E}\left( \mathbf{\alpha}^{T}X \right)\mathbb{= E}\left( X^{T}\mathbf{\alpha} \right) = \mathbf{\alpha}^{T}\mathbb{E}X = \left( \mathbb{E}^{T}X \right)\mathbf{\alpha}

Also :math:`\mathbf{\alpha}^{\mathrm{T}}X` is a scalar RV, and so
:math:`\left( \mathbf{\alpha}^{\mathrm{T}}X \right)^{2}\mathbf{=}{\left( \mathbf{\alpha}^{\mathrm{T}}X \right)\left( \mathbf{\alpha}^{\mathrm{T}}X \right)}^{T} = \mathbf{\alpha}^{\mathrm{T}}XX^{\mathrm{T}}\mathbf{\alpha}`.
Recall
:math:`\sigma\left( X \right)\mathbb{= E}X^{2} - \mathbb{E}^{2}X`, then

.. math:: \sigma\left( \mathbf{\alpha}^{\mathrm{T}}X \right) = \mathbf{E}\mathrm{\lbrack}\mathbf{\alpha}^{\mathrm{T}}X\left( \mathbf{\alpha}^{\mathrm{T}}X \right)^{\mathrm{T}}\mathrm{\rbrack} - \mathbf{E}\left\lbrack \mathbf{\alpha}^{\mathrm{T}}X \right\rbrack\mathbf{E}^{T}\left\lbrack \mathbf{\alpha}^{\mathrm{T}}X \right\rbrack = \mathbf{\alpha}^{\mathrm{T}}\mathbf{E}\mathrm{\lbrack}XX^{\mathrm{T}}\mathrm{\rbrack}\mathbf{\alpha} - \mathbf{\alpha}^{\mathrm{T}}\mathbf{E}X\mathbf{E}^{T}X\mathbf{\alpha}\mathbf{=}\mathbf{\alpha}^{\mathrm{T}}\mathbf{(}\mathbf{E}\mathrm{\lbrack}XX^{\mathrm{T}}\mathrm{\rbrack} - \mathbf{E}X\mathbf{E}^{T}X\mathrm{)}\mathbf{\alpha}\mathbf{=}\mathbf{\alpha}^{T}\operatorname{\sigma}\left( X \right)\mathbf{\alpha}

   Similarly, using
   :math:`\sigma\left( X,Y \right)\mathbb{= E}XY - \mathbb{E}X\mathbb{E}Y`,
   we can have
   :math:`\sigma\left( \mathbf{\alpha}^{\mathrm{T}}X,\mathbf{\beta}^{T}Y \right) = \mathbf{\alpha}^{T}\operatorname{\sigma}\left( X \right)\mathbf{\beta}`.
   Further, if we let
   :math:`\mathbf{A}\mathbf{= (}\mathbf{a}_{1}\mathbf{,\ldots,}\mathbf{a}_{n}\mathbf{)}`,
   then
   :math:`\sigma\left( \mathbf{A}^{T}X \right) = \mathbf{A}^{T}\operatorname{\sigma}\left( X \right)\mathbf{A}`,
   since
   :math:`\sigma\left( \mathbf{\alpha}_{i}X\mathbf{,}\mathbf{\alpha}_{j}X \right) = \mathbf{\alpha}_{i}^{T}\operatorname{\sigma}\left( X \right)\mathbf{\alpha}_{j}`.

   On the other hand, given
   :math:`\mathbf{X = (}\mathbf{x}_{1},\ldots,\mathbf{x}_{n}\mathbf{)}`,
   we have that
   :math:`\mathcal{S}\left( \mathbf{\text{Xα}} \right) = \mathbf{\alpha}^{T}\operatorname{S}\left( \mathbf{X} \right)\mathbf{\alpha}`.
   First check

.. math::

   \left\{ \begin{matrix}
   \overline{\mathbf{\text{Xα}}} = \overline{\sum_{i = 1}^{n}{\mathbf{\alpha}\left( i \right)\mathbf{x}_{i}}} = \frac{1}{m}\sum_{j = 1}^{m}{\sum_{i = 1}^{n}{\mathbf{\alpha}\left( i \right)\mathbf{x}_{i}(j)}} \\
   \overline{\mathbf{X}}\mathbf{\alpha}\mathbf{=}\sum_{i = 1}^{n}{\mathbf{\alpha}\left( i \right)\overline{\mathbf{x}_{i}}} = \sum_{i = 1}^{n}\left( \mathbf{\alpha}\left( i \right) \times \frac{1}{m}\sum_{j = 1}^{m}{\mathbf{x}_{i}\left( j \right)} \right) = \frac{1}{m}\sum_{i = 1}^{n}\left( \sum_{j = 1}^{m}{\mathbf{\alpha}\left( i \right)\mathbf{x}_{i}\left( j \right)} \right) \\
   \end{matrix} \Rightarrow \overline{\mathbf{\text{Xα}}} = \overline{\mathbf{X}}\mathbf{\alpha} \right.\

   Then we have

.. math:: \mathcal{S}\left( \mathbf{\text{Xα}} \right) = \frac{1}{n + 1}\left( \left( \mathbf{\text{Xα}} \right)^{T}\left( \mathbf{\text{Xα}} \right)\mathbf{-}{\overline{\mathbf{\text{Xα}}}}^{T}\overline{\mathbf{\text{Xα}}} \right) = \frac{1}{n + 1}\left( \mathbf{\alpha}^{T}\mathbf{X}^{T}\mathbf{X\alpha -}\mathbf{\alpha}^{T}{\overline{\mathbf{X}}}^{T}\overline{\mathbf{X}}\mathbf{\alpha} \right) = \mathbf{\alpha}^{T}\operatorname{S}\left( \mathbf{X} \right)\mathbf{\alpha}

By similar computations,
:math:`\mathcal{S}\left( \mathbf{X\alpha,X\beta} \right) = \mathbf{\alpha}^{T}\operatorname{S}\left( \mathbf{X} \right)\mathbf{\beta}`.
Let :math:`\mathbf{Y} = \mathbf{\text{XA}}` for any matrix
:math:`\mathbf{A}\mathbf{= (}\mathbf{a}_{1}\mathbf{,\ldots,}\mathbf{a}_{n}\mathbf{)}`,
then
:math:`\mathcal{S(}\mathbf{\text{XA}}) = \mathbf{A}^{T}\mathcal{S(}\mathbf{X})\mathbf{A}`,
since\ :math:`\mathcal{\text{\ S}}\left( \mathbf{X}^{T}\mathbf{\alpha}_{i}\mathbf{,}\mathbf{X}^{T}\mathbf{\alpha}_{j} \right) = \mathbf{\alpha}_{i}^{T}\operatorname{S}\left( \mathbf{X} \right)\mathbf{\alpha}_{j}`.

-  **Theorem** **1‑2** Covariance matrix is clearly symmetric, and
   moreover they are semi-positive definite, since for any constant
   vector :math:`\mathbf{\alpha}`

.. math:: \mathbf{\alpha}^{T}\left( \operatorname{\sigma}\left( X \right) \right)\mathbf{\alpha =}\sigma\left( \mathbf{\alpha}^{T}X \right)\mathbf{=}\operatorname{E}{\lbrack{\left( \mathbf{\alpha}^{T}X - \mathbb{E}\mathbf{\alpha}^{T}X \right)\left( \mathbf{\alpha}^{T}X - \mathbb{E}\mathbf{\alpha}^{T}X \right)}^{T}\rbrack}\mathbf{=}\operatorname{E}{\lbrack{\left( \mathbf{\alpha}^{T}X - \mathbf{\alpha}^{T}\mathbb{E}X \right)\left( \mathbf{\alpha}^{T}X - \mathbf{\alpha}^{T}\mathbb{E}X \right)}^{T}\rbrack}\mathbf{=}\operatorname{E}{\lbrack\mathbf{\alpha}^{T}\left( X - \mathbb{E}X \right)\left( X - \mathbb{E}X \right)^{T}\mathbf{\alpha}\rbrack} = \operatorname{E}\left\lbrack \left( \left( X - \mathbb{E}X \right)^{T}\mathbf{\alpha} \right)^{T}\left( \left( X - \mathbb{E}X \right)^{T}\mathbf{\alpha} \right) \right\rbrack \geq 0

   For sample covariance matrix, check that

.. math:: \mathbf{\alpha}^{T}\left( \operatorname{S}\left( \mathbf{X} \right) \right)\mathbf{\alpha =}\mathcal{S}\left( \mathbf{\text{Xα}} \right) \propto \left( \mathbf{\text{Xα}} - \overline{\mathbf{\text{Xα}}} \right)^{T}\left( \mathbf{\text{Xα}} - \overline{\mathbf{\text{Xα}}} \right) \geq 0

-  **Property** **1‑3** Suppose
   :math:`\mathbf{X} = \left( \mathcal{x}_{1},\ldots,\mathcal{x}_{N} \right)`
   where :math:`\mathcal{x}_{1},\ldots,\mathcal{x}_{N}` are data
   entries, and
   :math:`\overline{\mathcal{x}} = \frac{1}{N}\sum_{i = 1}^{N}\mathcal{x}_{i}`,
   then :math:`\Sigma\left( \mathbf{X} \right)` can be decomposed as

.. math:: \Sigma\left( \mathbf{X} \right) = \frac{1}{N - 1}\sum_{k = 1}^{N}{\left( \mathcal{x}_{k} - \overline{\mathcal{x}} \right)\left( \mathcal{x}_{k} - \overline{\mathcal{x}} \right)^{T}}

Note
:math:`\mathcal{x}_{j}\left( i \right) = x_{i,j} = \mathbf{x}_{i}\left( j \right)`
and
:math:`\overline{\mathcal{x}}\left( i \right)\mathbf{=}\overline{x_{i}} = \frac{1}{N}\sum_{j = 1}^{N}x_{i,j}`,
then

+-----------------------+-----------------------+-----------------------+
| .. math:: \mathbf{\Si |                       |    (1‑2)              |
| gma}\left( i,j \right |                       |                       |
| ) = \sum_{k = 1}^{N}{ |                       |                       |
| \left( \left( \mathca |                       |                       |
| l{x}_{k} - \overline{ |                       |                       |
| \mathcal{x}} \right)\ |                       |                       |
| left( \mathcal{x}_{k} |                       |                       |
|  - \overline{\mathcal |                       |                       |
| {x}} \right)^{T} \rig |                       |                       |
| ht)\left( i,j \right) |                       |                       |
| } = \sum_{k = 1}^{N}{ |                       |                       |
| \left( \mathcal{x}_{k |                       |                       |
| }\left( i \right) - \ |                       |                       |
| overline{x_{i}} \righ |                       |                       |
| t)\left( \mathcal{x}_ |                       |                       |
| {k}\left( j \right) - |                       |                       |
|  \overline{x_{j}} \ri |                       |                       |
| ght)} = \sum_{k = 1}^ |                       |                       |
| {N}{\left( \mathbf{x} |                       |                       |
| _{i}\left( k \right)  |                       |                       |
| - \overline{x_{i}} \r |                       |                       |
| ight)\left( \mathbf{x |                       |                       |
| }_{j}\left( k \right) |                       |                       |
|  - \overline{x_{j}} \ |                       |                       |
| right)} = \left( \mat |                       |                       |
| hbf{x}_{i} - \overlin |                       |                       |
| e{\mathbf{x}_{i}} \ri |                       |                       |
| ght)^{T}\left( \mathb |                       |                       |
| f{x}_{j} - \overline{ |                       |                       |
| \mathbf{x}_{j}} \righ |                       |                       |
| t)                    |                       |                       |
+-----------------------+-----------------------+-----------------------+

The result following when comparing above with (1‑1).

**Corollary** **1‑1**.
:math:`\mathbf{X}\mathbf{X}^{T} = \left( N - 1 \right)\Sigma\left( \mathbf{X} \right) + N\overline{\mathcal{x}}{\overline{\mathcal{x}}}^{T}`.
Check that :math:`\mathbf{\text{XX}}^{T} = \begin{pmatrix}
\mathbf{x}_{1}^{T}\mathbf{x}_{1} & \cdots & \mathbf{x}_{1}^{T}\mathbf{x}_{M} \\
 \vdots & \ddots & \vdots \\
\mathbf{x}_{M}^{T}\mathbf{x}_{1} & \cdots & \mathbf{x}_{M}^{T}\mathbf{x}_{M} \\
\end{pmatrix}`, and by above (*1*\ ‑\ *2*) we have

.. math:: \mathbf{\text{XX}}^{T} = \sum_{k = 1}^{N}{\mathcal{x}_{k}{\mathcal{x}_{k}}^{T}} = \sum_{k = 1}^{N}{\left( \mathcal{x}_{k} - \overline{\mathcal{x}} \right)({\mathcal{x}_{k} - \overline{\mathcal{x}})}^{T}} + \sum_{k = 1}^{N}\left( \overline{\mathcal{x}}\mathcal{x}_{k}^{T} + \mathcal{x}_{k}{\overline{\mathcal{x}}}^{T} - \overline{\mathcal{x}}{\overline{\mathcal{x}}}^{T} \right) = \left( N - 1 \right)\Sigma\left( \mathbf{X} \right)\mathbf{+}\overline{\mathcal{x}}\left( \sum_{k = 1}^{N}\mathcal{x}_{k}^{T} \right) + \left( \sum_{k = 1}^{N}\mathcal{x}_{k} \right){\overline{\mathcal{x}}}^{T} - N\overline{\mathcal{x}}{\overline{\mathcal{x}}}^{T} = \left( N - 1 \right)\Sigma\left( \mathbf{X} \right)\mathbf{+}N\overline{\mathcal{x}}{\overline{\mathcal{x}}}^{T} + N\overline{\mathcal{x}}{\overline{\mathcal{x}}}^{T} - N\overline{\mathcal{x}}{\overline{\mathcal{x}}}^{T} = \left( N - 1 \right)\Sigma\left( \mathbf{X} \right)\mathbf{+}N\overline{\mathcal{x}}{\overline{\mathcal{x}}}^{T}

**Theorem** **1‑3**\ **Block decomposition**. Again consider
:math:`\mathbf{X} = \left( \mathcal{x}_{1},\ldots,\mathcal{x}_{N} \right)`
where :math:`\mathcal{x}_{1},\ldots,\mathcal{x}_{N}` are data entries,
and
:math:`\overline{\mathcal{x}} = \frac{1}{N}\sum_{i = 1}^{N}\mathcal{x}_{i}`.
Suppose :math:`\mathcal{x}_{1},\ldots,\mathcal{x}_{N}` are categorized
into :math:`K` non-overlapping groups :math:`G_{1},\ldots,G_{k}`, then

.. math:: \left( N - 1 \right)\Sigma\left( \mathbf{X} \right) = \sum_{i = 1}^{K}{\left( N_{i} - 1 \right)\Sigma\left( \mathbf{X}_{i} \right)} + \sum_{i = 1}^{K}{N_{i}\left( {\overline{\mathcal{x}}}^{i} - \overline{\mathcal{x}} \right)\left( {\overline{\mathcal{x}}}^{i} - \overline{\mathcal{x}} \right)^{T}}

**Multivariate Gaussian Distribution**
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

:math:`m`-dimensional Multivariate Gaussian distribution is defined by
an :math:`m`-dimensional mean :math:`\mathbf{\mu}` and a
:math:`m \times m` non-singular covariance :math:`\mathbf{\Sigma}`,
denoted by
:math:`\operatorname{Gaussian}\left( \mathbf{\mu},\mathbf{\Sigma} \right)`,
whose density function can be written as

.. math:: p\left( \mathbf{x} \right) = \frac{1}{\left( 2\pi \right)^{\frac{m}{2}}}\frac{1}{\left| \mathbf{\Sigma} \right|^{\frac{1}{2}}}\exp\left\{ - \frac{1}{2}\mathcal{d}_{M}\left( \mathbf{x};\mathbf{\mu,}\mathbf{\Sigma} \right) \right\} = \frac{1}{\left( 2\pi \right)^{\frac{m}{2}}}\frac{1}{\left| \mathbf{\Sigma} \right|^{\frac{1}{2}}}\exp\left\{ - \frac{1}{2}\left( \mathbf{x} - \mathbf{\mu} \right)^{T}\mathbf{\Sigma}^{- 1}\left( \mathbf{x} - \mathbf{\mu} \right) \right\}

where :math:`\left| \mathbf{\Sigma} \right|` is the determinant of
:math:`\mathbf{\Sigma}`, and
:math:`\mathcal{d}_{M}\left( \mathbf{x};\mathbf{\mu,\Sigma} \right) = \left( \mathbf{x} - \mathbf{\mu} \right)^{T}\mathbf{\Sigma}^{- 1}\left( \mathbf{x} - \mathbf{\mu} \right)`
is called the **Mahalanobis distance**, which measures the distance
between an observation :math:`\mathbf{x}` and a multivariate
distribution with mean :math:`\mathbf{\mu}` and covariance
:math:`\mathbf{\Sigma}` (not necessarily Gaussian). *Note*
:math:`\mathcal{d}_{M}` is reduced to Euclidean distance from
observation :math:`\mathbf{x}` to mean :math:`\mathbf{\mu}` when the
covariance is identity. Recall the basic fact that co-variance matrix is
symmetric and positive semidefinite, and in the case of Gaussian it must
be positive definite, thus
:math:`\mathcal{d}_{M}\left( \mathbf{x};\mathbf{\mu,\Sigma} \right) \geq 0`
and
:math:`\mathcal{d}_{M}\left( \mathbf{x};\mathbf{\mu,\Sigma} \right) = 0`
iff :math:`\mathbf{x} = \mathbf{\mu}`. The inverse of covariance matrix
:math:`\mathbf{\Sigma}^{- 1}` is also named the **precision matrix**,
denoted by :math:`\mathbf{Ⲗ}`, and the density can be written in terms
of precision matrix as

.. math:: p\left( \mathbf{x} \right) = \frac{\left| \mathbf{Ⲗ} \right|^{\frac{1}{2}}}{\left( 2\pi \right)^{\frac{m}{2}}}\exp\left\{ - \frac{1}{2}\left( \mathbf{x} - \mathbf{\mu} \right)^{T}\mathbf{Ⲗ}\left( \mathbf{x} - \mathbf{\mu} \right) \right\}

The *limitation* of Gaussian is its quadratic number of parameters which
could be a problem for high-dimensional computation, and its very
limited unimodal shape which could not represent complicated real-world
distributions.

-  We state useful gradient results from matrix derivatives, which are
   frequently used in finding analytical solution for optimization
   problems. They are soon applied to prove the maximum likelihood of
   multivariate Gaussian.

   **Fact** **1‑1** Affine transformation
   :math:`\mathbf{a}^{T}\mathbf{x} + b\mathbf{:}\mathbb{R}^{n}\mathbb{\rightarrow R}`
   where :math:`\mathbf{a} \in \mathbb{R}^{n},b\mathbb{\in R}` has
   gradient
   :math:`\nabla\left( \mathbf{a}^{T}\mathbf{x} + b \right) = \nabla\left( \mathbf{x}^{T}\mathbf{a} + b \right) = \mathbf{a}`.

   **Fact** **1‑2** Determinant
   :math:`\left| \mathbf{X} \right|:\mathbb{R}^{n \times n}\mathbb{\rightarrow R}`
   is a scalar-valued matrix function, and we have
   :math:`\nabla\log\left| \mathbf{X} \right| = \mathbf{X}^{\mathbf{-}T}`.

   **Fact** **1‑3** The gradient of
   :math:`f\left( \mathbf{X} \right)\mathbf{=}\mathbf{u}^{T}\mathbf{\text{Xv}}:\mathbb{R}^{m \times n}\mathbb{\rightarrow R}`,
   where
   :math:`\mathbf{u} \in \mathbb{R}^{m},\mathbf{v} \in \mathbb{R}^{n}`
   are constant vectors, is
   :math:`\nabla\mathbf{u}^{T}\mathbf{\text{Xv}} \equiv \mathbf{u}\mathbf{v}^{T}`.

   **Fact** **1‑4** The gradient
   of\ :math:`\text{\ f}\left( \mathbf{x} \right) = \mathbf{x}^{T}\mathbf{\text{Ux}}:\mathbf{x} \in \mathbb{R}^{n}\mathbb{\rightarrow R}`,
   where :math:`\mathbf{U} \in \mathbb{R}^{n \times n}` is a constant
   matrix, is
   :math:`\nabla\mathbf{x}^{T}\mathbf{\text{Ux}} = (\mathbf{U} + \mathbf{U}^{T})\mathbf{x}`.

   **Theorem** **1‑4**\ **Maximum likelihood estimators**. Given
   observations :math:`\mathbf{x}_{1},\ldots,\mathbf{x}_{N}` and assume
   they are i.i.d. Gaussian, the log-likelihood is

.. math::

   L \propto \sum_{i = 1}^{N}{- \frac{1}{2}\log\left| \mathbf{\Sigma} \right| - \frac{1}{2}\left( \mathbf{x}_{i} - \mathbf{\mu} \right)^{T}\mathbf{\Sigma}^{- 1}\left( \mathbf{x}_{i} - \mathbf{\mu} \right)} = \frac{N}{2}\log\left| \mathbf{\Sigma}^{- 1} \right| - \frac{1}{2}\sum_{i = 1}^{N}{\left( \mathbf{x}_{i} - \mathbf{\mu} \right)^{T}\mathbf{\Sigma}^{- 1}\left( \mathbf{x}_{i} - \mathbf{\mu} \right)}

+-----------------------+-----------------------+-----------------------+
| .. math:: \frac{\part |                       | (1‑3)                 |
| ial L}{\partial\mathb |                       |                       |
| f{\Sigma}^{- 1}} = \f |                       |                       |
| rac{N}{2}\mathbf{\Sig |                       |                       |
| ma}^{T} - \frac{1}{2} |                       |                       |
| \sum_{i = 1}^{N}{\lef |                       |                       |
| t( \mathbf{x}_{i} - \ |                       |                       |
| mathbf{\mu} \right)\l |                       |                       |
| eft( \mathbf{x}_{i} - |                       |                       |
|  \mathbf{\mu} \right) |                       |                       |
| ^{T}} = 0 \Rightarrow |                       |                       |
|  \mathbf{\Sigma}\math |                       |                       |
| bf{=}\frac{1}{N}\sum_ |                       |                       |
| {i = 1}^{N}{\left( \m |                       |                       |
| athbf{x}_{i} - \mathb |                       |                       |
| f{\mu} \right)\left(  |                       |                       |
| \mathbf{x}_{i} - \mat |                       |                       |
| hbf{\mu} \right)^{T}} |                       |                       |
+-----------------------+-----------------------+-----------------------+

Then for :math:`\mathbf{\mu}`, using *Fact 1‑4*,

+-----------------------+-----------------------+-----------------------+
| .. math:: L \propto \ |                       | (1‑4)                 |
| sum_{i = 1}^{N}{\math |                       |                       |
| bf{x}_{i}^{T}\mathbf{ |                       |                       |
| \Sigma}^{- 1}\mathbf{ |                       |                       |
| \mu} - \frac{1}{2}\ma |                       |                       |
| thbf{\mu}^{T}\mathbf{ |                       |                       |
| \Sigma}^{- 1}\mathbf{ |                       |                       |
| \mu}} \Rightarrow \fr |                       |                       |
| ac{\partial L}{\parti |                       |                       |
| al\mathbf{\mu}} = \su |                       |                       |
| m_{i = 1}^{N}{\mathbf |                       |                       |
| {x}_{i}^{T}\mathbf{\S |                       |                       |
| igma}^{- 1} - \mathbf |                       |                       |
| {\Sigma}^{- 1}\mathbf |                       |                       |
| {\mu}} = \mathbf{0} \ |                       |                       |
| Rightarrow \mathbf{\S |                       |                       |
| igma}^{- 1}\left( \su |                       |                       |
| m_{i = 1}^{N}\mathbf{ |                       |                       |
| x}_{i} \right) = N\ma |                       |                       |
| thbf{\Sigma}^{- 1}\ma |                       |                       |
| thbf{\mu}\mathbf{\Rig |                       |                       |
| htarrow}\mathbf{\mu}_ |                       |                       |
| {\text{ML}}\mathbf{=} |                       |                       |
| \frac{\sum_{i = 1}^{N |                       |                       |
| }\mathbf{x}_{i}}{N} = |                       |                       |
|  \overline{\mathbf{x} |                       |                       |
| }                     |                       |                       |
+-----------------------+-----------------------+-----------------------+

Plug back to :math:`\mathbf{\Sigma}^{- 1}` in (1‑3) and we have

.. math:: \mathbf{\Sigma}_{\text{ML}}\mathbf{=}\frac{1}{N}\sum_{i = 1}^{N}{\left( \mathbf{x}_{i} - \overline{\mathbf{x}} \right)\left( \mathbf{x}_{i} - \overline{\mathbf{x}} \right)^{T}}

By *Property 1‑3*, :math:`\mathbf{\Sigma}_{\text{ML}}` equals the biased
sample covariance, or :math:`\frac{N}{N - 1}\mathbf{\Sigma}_{\text{ML}}`
equals the sample covariance.

-  **Lemma** **1‑1** Recall given two RVs :math:`\mathbf{x,y}` the
   **covariance matrix** is defined as

.. math:: \operatorname{cov}\mathbf{x}\mathbb{= E}\left\lbrack \left( \mathbf{x}\mathbb{- E}\left\lbrack \mathbf{x} \right\rbrack \right)\left( \mathbf{x}\mathbb{- E}\left\lbrack \mathbf{x} \right\rbrack \right)^{T} \right\rbrack

Expand it and we have the generalized version of the well-known formula
“\ :math:`\operatorname{cov}\left( X,Y \right)\mathbb{= E}\left\lbrack \text{XY} \right\rbrack\mathbb{- E}X\mathbb{E}Y`\ ”
for a scalar random variable.

.. math:: \operatorname{cov}\left( \mathbf{x,y} \right)\mathbf{= E}\left\lbrack \mathbf{x}\mathbf{y}^{T} - \mathbf{x}\mathbb{E}^{T}\left\lbrack \mathbf{y} \right\rbrack - \mathbf{E}\left\lbrack \mathbf{y} \right\rbrack\mathbf{x}^{T} + \mathbf{E}\left\lbrack \mathbf{x} \right\rbrack\mathbb{E}^{T}\left\lbrack \mathbf{y} \right\rbrack \right\rbrack\mathbf{\Rightarrow}\operatorname{cov}\left( \mathbf{x,y} \right)\mathbf{=}\mathbb{E}\left\lbrack \mathbf{x}\mathbf{y}^{T} \right\rbrack\mathbf{-}\mathbf{E}\left\lbrack \mathbf{x} \right\rbrack\mathbb{E}^{T}\left\lbrack \mathbf{y} \right\rbrack

Given any RV :math:`\mathbf{x}`, with mean :math:`\mathbf{\mu}` and
covariance :math:`\mathbf{\Sigma}`, then we have

:math:`\mathbf{\Sigma =}\mathbb{E}\left\lbrack \mathbf{x}\mathbf{x}^{T} \right\rbrack\mathbf{-}\mathbf{\mu}\mathbf{\mu}^{T}`
or
:math:`\mathbb{E}\left\lbrack \mathbf{x}\mathbf{x}^{T} \right\rbrack\mathbf{=}\mathbf{\mu}\mathbf{\mu}^{T}\mathbf{+ \Sigma}`

**Theorem** **1‑5**\ **Bias of ML estimators**. Given i.i.d. RVs
:math:`\mathbf{x}_{1},\ldots,\mathbf{x}_{N}` with mean
:math:`\mathbf{\mu}` and covariance :math:`\mathbf{\Sigma}`, note
:math:`\mathbb{E}\left\lbrack \mathbf{x}_{i}\mathbf{x}_{j}^{T} \right\rbrack = \left\{ \begin{matrix}
\mathbf{\mu}\mathbf{\mu}^{T} & i \neq j \\
\mathbf{\mu}\mathbf{\mu}^{T}\mathbf{+}\mathbf{\Sigma} & i = j \\
\end{matrix} \right.\ `, then we have

.. math:: \mathbb{E}\left\lbrack \mathbf{x}_{i}{\overline{\mathbf{x}}}^{T} \right\rbrack = \frac{1}{N}\sum_{i = 1}^{N}{\mathbb{E}\left\lbrack \mathbf{x}_{i}\mathbf{x}_{j}^{T} \right\rbrack} = \frac{1}{N}\left( N\mathbf{\mu}\mathbf{\mu}^{T} + \mathbf{\Sigma} \right) = \mathbf{\mu}\mathbf{\mu}^{T} + \frac{\mathbf{\Sigma}}{N}\mathbb{= E}\left\lbrack \overline{\mathbf{x}}\mathbf{x}_{i}^{T} \right\rbrack

.. math:: \mathbb{E}\left\lbrack \overline{\mathbf{x}}{\overline{\mathbf{x}}}^{T} \right\rbrack = \frac{1}{N^{2}}\sum_{i = 1}^{N}{\sum_{j = 1}^{N}{\mathbb{E}\left\lbrack \mathbf{x}_{i}\mathbf{x}_{j}^{T} \right\rbrack}} = \frac{1}{N^{2}}\left. （N^{2}\mathbf{\mu}\mathbf{\mu}^{T} + N\mathbf{\Sigma} \right.） = \mathbf{\mu}\mathbf{\mu}^{T} + \frac{\mathbf{\Sigma}}{N}

For Gaussian i.i.d. RVs :math:`\mathbf{x}_{1},\ldots,\mathbf{x}_{N}` as
in

.. math:: \mathbf{E}\left\lbrack \mathbf{\Sigma}_{\text{ML}} \right\rbrack\mathbf{=}\frac{1}{N}\sum_{i = 1}^{N}{\mathbb{E}\left( \mathbf{x}_{i} - \overline{\mathbf{x}} \right)\left( \mathbf{x}_{i} - \overline{\mathbf{x}} \right)^{T}} = \frac{1}{N}\sum_{i = 1}^{N}{\mathbb{E}\left\lbrack \mathbf{x}_{i}\mathbf{x}_{i}^{T} - \mathbf{x}_{i}{\overline{\mathbf{x}}}^{T} - \overline{\mathbf{x}}\mathbf{x}_{i}^{T} + \overline{\mathbf{x}}{\overline{\mathbf{x}}}^{T} \right\rbrack} = \frac{1}{N}\sum_{i = 1}^{N}\left( \boxed{\mathbf{\mu}\mathbf{\mu}^{T}}\mathbf{+ \Sigma} - 2\left( \boxed{\mathbf{\mu}\mathbf{\mu}^{T}} + \frac{\mathbf{\Sigma}}{N} \right) + \boxed{\mathbf{\mu}\mathbf{\mu}^{T}} + \frac{\mathbf{\Sigma}}{N} \right) = \frac{N - 1}{N}\mathbf{\Sigma}

Thus,
:math:`\mathbf{\Sigma}_{\text{ML}} = \frac{N - 1}{N}\mathbf{\Sigma}` is
a biased estimator. :math:`\mathbf{\mu}_{\text{ML}}` is trivially an
unbiased estimator, since
:math:`\mathbf{E}\left\lbrack \mathbf{\mu}_{\text{ML}} \right\rbrack\mathbf{=}\mathbb{E}\left\lbrack \overline{\mathbf{x}} \right\rbrack = \mathbf{\mu}`.

-  **Fact** **1‑5** Every matrix :math:`\mathbf{A}` s.t.
   :math:`\operatorname{rank}\mathbf{A} = k` can be written as the sum
   of :math:`k` rank-1 matrices, i.e.
   :math:`\mathbf{A} = \sum_{i = 1}^{k}{\sigma_{i}^{2}\mathbf{u}_{i}\mathbf{v}_{i}^{T}}`,
   where :math:`\mathbf{u}_{i}` and :math:`\mathbf{v}_{i}` are columns
   from two matrices :math:`\mathbf{U},\mathbf{V}` that come from the
   **reduced SVD**
   :math:`\mathbf{A} = \mathbf{\text{UΛ}}\mathbf{V}^{T}`; in particular,
   if :math:`\mathbf{A}` is symmetric and positive semidefinite, then
   the singular values are eigenvalues of :math:`\mathbf{A}`, and the
   reduced SVD becomes reduced eigen-decomposition
   :math:`\mathbf{A} = \mathbf{\text{QΛ}}\mathbf{Q}^{T}` where
   :math:`\mathbf{Q}` consists of :math:`k` orthonormal eigenvectors
   :math:`\mathbf{q}_{1},\ldots,\mathbf{q}_{k}`, and thus
   :math:`\mathbf{A =}\sum_{i = 1}^{k}{\lambda_{i}\mathbf{q}_{i}\mathbf{q}_{i}^{T}}`;
   moreover,
   :math:`\mathbf{A}^{- 1}\mathbf{=}\sum_{i = 1}^{k}{\lambda_{i}^{- 1}\mathbf{q}_{i}\mathbf{q}_{i}^{T}}`
   because :math:`\mathbf{A}^{- 1}` and :math:`\mathbf{A}` share the
   same eigenspace for each eigenvalue.

   **Property** **1‑4** As a result of *Fact 1‑5*,
   :math:`\mathcal{d}_{M}\left( \mathbf{x};\mathbf{\mu,\Sigma} \right) = \sum_{i = 1}^{k}z_{i}^{2} = \mathbf{z}^{T}\mathbf{z}`
   where
   :math:`z_{i} = \frac{1}{\sqrt{\lambda_{i}}}\mathbf{q}_{i}^{T}\left( \mathbf{x} - \mathbf{\mu} \right)`
   and
   :math:`\mathbf{z} = \mathbf{\Lambda}^{- \frac{1}{2}}\mathbf{Q}\left( \mathbf{x} - \mathbf{\mu} \right) = \left( z_{1},\ldots,z_{m} \right)^{T}`,
   and
   :math:`\mathbf{x} = \mathbf{\Lambda}^{\frac{1}{2}}\mathbf{Q}^{T}\mathbf{z}\mathbf{+}\mathbf{\mu}`.

   **Shape of contours**. Recall an orthonormal matrix represents a
   rotation (oriented rotation, to be exact), then
   :math:`\mathbf{\Lambda}^{\frac{1}{2}}\mathbf{Q}^{T}\left( \cdot \right)\mathbf{+}\mathbf{\mu}`
   geometrically transforms the standard frame on a unit circle to a
   frame on an ellipsoid centered at :math:`\mathbf{\mu}`, and
   :math:`\mathbf{z}` is the coordinate of :math:`\mathbf{x}` w.r.t. to
   the ellipsoid frame. Conversely, given a contour level
   :math:`p\left( \mathbf{x} \right) = p_{0}`, we have
   :math:`\mathbf{z}^{T}\mathbf{z =}c \Rightarrow \left\| \mathbf{z} \right\|_{2} = \sqrt{c}`
   for some constant :math:`c`, and then any :math:`\mathbf{x}` s.t.
   :math:`\left\| \mathbf{z} \right\|_{2} = \sqrt{c}` will be on an
   ellipsoid centered at :math:`\mathbf{\mu}`. Therefore, the contours
   of multivariate Gaussian density are ellipsoids.

-  **Lemma** **1‑2** If a density function
   :math:`p\left( \mathbf{x} \right) \propto \exp\left\{ - \frac{1}{2}\mathbf{x}^{T}\mathbf{Ax +}\mathbf{x}^{T}\mathbf{\text{Ay}} \right\}`
   where :math:`\mathbf{A}` is symmetric positive semidefinite, then
   :math:`p` must be Gaussian with precision
   :math:`\mathbf{Ⲗ}\mathbf{= A}` and mean
   :math:`\mathbf{\mu} = \mathbf{y}`. This is simply we can rearrange it
   as

.. math:: p\left( \mathbf{x} \right) \propto \exp\left\{ - \frac{1}{2}\left( \mathbf{x - y} \right)^{T}\mathbf{A}\left( \mathbf{x}\mathbf{-}\mathbf{y} \right)\mathbf{-}\mathbf{y}^{T}\mathbf{\text{Ay}} \right\} \propto \exp\left\{ - \frac{1}{2}\left( \mathbf{x - y} \right)^{T}\mathbf{A}\left( \mathbf{x}\mathbf{-}\mathbf{y} \right) \right\}

There is no need to worry about normalization, since we have assumed
:math:`p` is a density, where its normalization is guaranteed.

**Fact** **1‑6** If a block matrix :math:`\begin{pmatrix}
\mathbf{A} & \mathbf{B} \\
\mathbf{C} & \mathbf{D} \\
\end{pmatrix}` is inversible, then if :math:`\mathbf{A}` is
non-singular, we have the following with all inverses valid

.. math::

   \begin{pmatrix}
   \mathbf{A} & \mathbf{B} \\
   \mathbf{C} & \mathbf{D} \\
   \end{pmatrix}^{- 1} = \begin{pmatrix}
   \mathbf{A}^{- 1}\left( \mathbf{I}\mathbf{+}\mathbf{\text{BMC}}\mathbf{A}^{- 1} \right) & - \mathbf{A}^{- 1}\mathbf{\text{BM}} \\
    - \mathbf{\text{MC}}\mathbf{A}^{- 1} & \mathbf{M} \\
   \end{pmatrix},\mathbf{M} = \left( \mathbf{D} - \mathbf{C}\mathbf{A}^{- 1}\mathbf{B} \right)^{- 1}

and if :math:`\mathbf{D}` is non-singular, we have the following with
all inverses valid,

.. math::

   \begin{pmatrix}
   \mathbf{A} & \mathbf{B} \\
   \mathbf{C} & \mathbf{D} \\
   \end{pmatrix}^{- 1} = \begin{pmatrix}
   \mathbf{M} & - \mathbf{\text{MB}}\mathbf{D}^{- 1} \\
    - \mathbf{D}^{- 1}\mathbf{\text{CM}} & \mathbf{D}^{- 1}\left( \mathbf{I} + \mathbf{\text{CMB}}\mathbf{D}^{- 1} \right) \\
   \end{pmatrix},\mathbf{M} = \left( \mathbf{A} - \mathbf{B}\mathbf{D}^{- 1}\mathbf{C} \right)^{- 1}

**Fact** **1‑7** Given a density :math:`p\left( x,y \right)`, then the
**conditional density** :math:`p\left( x;y = y_{0} \right)`, also
denoted as :math:`p\left( x;y_{0} \right)`, equals the normalized
**partial density** :math:`p\left( x,y_{0} \right)`, i.e.
:math:`p\left( x;y_{0} \right)` differs from
:math:`p\left( x,y_{0} \right)` only at their scale.

**Theorem** **1‑6**\ **Conditional density**. Given
:math:`\mathbf{x}\sim\operatorname{Gaussian}\left( \mathbf{\mu},\mathbf{\Sigma} \right)`,
WLOG, partition :math:`\mathbf{x} = \begin{pmatrix}
\mathbf{x}_{a} \\
\mathbf{x}_{b} \\
\end{pmatrix}`, where :math:`\mathbf{x}_{a} \in \mathbb{R}^{d}` is the
unknown, and :math:`\mathbf{x}_{b} \in \mathbb{R}^{m - d}` is the
condition, and we want to find the conditional density
:math:`p\left( \mathbf{x}_{a}\mathbf{|}\mathbf{x}_{b} \right)`.
Partition the mean and covariance accordingly as :math:`\mathbf{\mu} =`
:math:`\begin{pmatrix}
\mathbf{\mu}_{a} \\
\mathbf{\mu}_{b} \\
\end{pmatrix}`, :math:`\mathbf{\Sigma} = \begin{pmatrix}
\mathbf{\Sigma}_{\text{aa}} & \mathbf{\Sigma}_{\text{ab}} \\
\mathbf{\Sigma}_{\text{ba}} & \mathbf{\Sigma}_{\text{bb}} \\
\end{pmatrix}` and :math:`\mathbf{Ⲗ} = \begin{pmatrix}
\mathbf{Ⲗ}_{\text{aa}} & \mathbf{Ⲗ}_{\text{ab}} \\
\mathbf{Ⲗ}_{\text{ba}} & \mathbf{Ⲗ}_{\text{bb}} \\
\end{pmatrix}`. Assume :math:`\mathbf{\Sigma}_{\text{aa}}` is
non-singular, then we have

+-----------------------+-----------------------+-----------------------+
| .. math::             |                       | (1‑4)                 |
|                       |                       |                       |
|    - \frac{1}{2}\left |                       |                       |
| ( \mathbf{x} - \mathb |                       |                       |
| f{\mu} \right)^{T}\ma |                       |                       |
| thbf{Ⲗ}\left( \mathbf |                       |                       |
| {x} - \mathbf{\mu} \r |                       |                       |
| ight) = \begin{pmatri |                       |                       |
| x}                    |                       |                       |
|    \mathbf{x}_{a}\mat |                       |                       |
| hbf{-}\mathbf{\mu}_{a |                       |                       |
| } \\                  |                       |                       |
|    \mathbf{x}_{b}\mat |                       |                       |
| hbf{-}\mathbf{\mu}_{b |                       |                       |
| } \\                  |                       |                       |
|    \end{pmatrix}^{T}\ |                       |                       |
| begin{pmatrix}        |                       |                       |
|    \mathbf{Ⲗ}_{\text{ |                       |                       |
| aa}} & \mathbf{Ⲗ}_{\t |                       |                       |
| ext{ab}} \\           |                       |                       |
|    \mathbf{Ⲗ}_{\text{ |                       |                       |
| ba}} & \mathbf{Ⲗ}_{\t |                       |                       |
| ext{bb}} \\           |                       |                       |
|    \end{pmatrix}\begi |                       |                       |
| n{pmatrix}            |                       |                       |
|    \mathbf{x}_{a}\mat |                       |                       |
| hbf{-}\mathbf{\mu}_{a |                       |                       |
| } \\                  |                       |                       |
|    \mathbf{x}_{b}\mat |                       |                       |
| hbf{-}\mathbf{\mu}_{b |                       |                       |
| } \\                  |                       |                       |
|    \end{pmatrix} = -  |                       |                       |
| \frac{1}{2}\left( \ma |                       |                       |
| thbf{x}_{a}\mathbf{-} |                       |                       |
| \mathbf{\mu}_{a} \rig |                       |                       |
| ht)^{T}\mathbf{Ⲗ}_{\t |                       |                       |
| ext{aa}}\left( \mathb |                       |                       |
| f{x}_{a}\mathbf{-}\ma |                       |                       |
| thbf{\mu}_{a} \right) |                       |                       |
|  - \left( \mathbf{x}_ |                       |                       |
| {a}\mathbf{-}\mathbf{ |                       |                       |
| \mu}_{a} \right)^{T}\ |                       |                       |
| mathbf{Ⲗ}_{\text{ab}} |                       |                       |
| \left( \mathbf{x}_{b} |                       |                       |
| \mathbf{-}\mathbf{\mu |                       |                       |
| }_{b} \right)\mathbf{ |                       |                       |
| -}\frac{1}{2}\left( \ |                       |                       |
| mathbf{x}_{b}\mathbf{ |                       |                       |
| -}\mathbf{\mu}_{b} \r |                       |                       |
| ight)^{T}\mathbf{Ⲗ}_{ |                       |                       |
| \text{bb}}\left( \mat |                       |                       |
| hbf{x}_{b}\mathbf{-}\ |                       |                       |
| mathbf{\mu}_{b} \righ |                       |                       |
| t)\mathbf{=} - \frac{ |                       |                       |
| 1}{2}\mathbf{x}_{a}^{ |                       |                       |
| T}\mathbf{Ⲗ}_{\text{a |                       |                       |
| a}}\mathbf{x}_{a} + \ |                       |                       |
| mathbf{x}_{a}^{T}\mat |                       |                       |
| hbf{Ⲗ}_{\text{aa}}\ma |                       |                       |
| thbf{\mu}_{a}\mathbf{ |                       |                       |
| -}\mathbf{x}_{a}^{T}\ |                       |                       |
| mathbf{Ⲗ}_{\text{ab}} |                       |                       |
| \left( \mathbf{x}_{b} |                       |                       |
| \mathbf{-}\mathbf{\mu |                       |                       |
| }_{b} \right)\mathbf{ |                       |                       |
| +}\text{constant} = - |                       |                       |
|  \frac{1}{2}\mathbf{x |                       |                       |
| }_{a}^{T}\mathbf{Ⲗ}_{ |                       |                       |
| \text{aa}}\mathbf{x}_ |                       |                       |
| {a} + \mathbf{x}_{a}^ |                       |                       |
| {T}\mathbf{Ⲗ}_{\text{ |                       |                       |
| aa}}\left( \mathbf{\m |                       |                       |
| u}_{a}\mathbf{-}\math |                       |                       |
| bf{Ⲗ}_{\text{aa}}^{-  |                       |                       |
| 1}\mathbf{Ⲗ}_{\text{a |                       |                       |
| b}}\left( \mathbf{x}_ |                       |                       |
| {b}\mathbf{-}\mathbf{ |                       |                       |
| \mu}_{b} \right) \rig |                       |                       |
| ht)\mathbf{+}\text{co |                       |                       |
| nstant}               |                       |                       |
+-----------------------+-----------------------+-----------------------+

By *Lemma 1‑2*, we proved that if :math:`\mathbf{\Sigma}_{\text{aa}}` is
non-singular, then
:math:`p\left( \mathbf{x}_{a}\mathbf{|}\mathbf{x}_{b} \right)` is
Gaussian. Denote
:math:`\mathbf{x}_{a}\mathbf{|}\mathbf{x}_{b}\mathbf{\sim}\operatorname{Gaussian}\left( \mathbf{\mu}_{a|b}\mathbf{,}\mathbf{\Sigma}_{a|b} \right)`,
we have

+-----------------------+-----------------------+-----------------------+
| .. math:: \mathbf{\mu |                       | (1‑5)                 |
| }_{a|b}\mathbf{=}\mat |                       |                       |
| hbf{\mu}_{a}\mathbf{- |                       |                       |
| }\mathbf{Ⲗ}_{\text{aa |                       |                       |
| }}^{- 1}\mathbf{Ⲗ}_{\ |                       |                       |
| text{ab}}\left( \math |                       |                       |
| bf{x}_{b}\mathbf{-}\m |                       |                       |
| athbf{\mu}_{b} \right |                       |                       |
| )\mathbf{,}\mathbf{\S |                       |                       |
| igma}_{a|b}\mathbf{=} |                       |                       |
| \mathbf{Ⲗ}_{\text{aa} |                       |                       |
| }^{- 1}               |                       |                       |
+-----------------------+-----------------------+-----------------------+

If in addition :math:`\mathbf{\Sigma}_{\text{bb}}` is non-singular,
using *Fact 1‑6* and :math:`\begin{pmatrix}
\mathbf{\Sigma}_{\text{aa}} & \mathbf{\Sigma}_{\text{ab}} \\
\mathbf{\Sigma}_{\text{ba}} & \mathbf{\Sigma}_{\text{bb}} \\
\end{pmatrix}^{- 1} = \begin{pmatrix}
\mathbf{Ⲗ}_{\text{aa}} & \mathbf{Ⲗ}_{\text{ab}} \\
\mathbf{Ⲗ}_{\text{ba}} & \mathbf{Ⲗ}_{\text{bb}} \\
\end{pmatrix}`, we have

.. math::

   \left\{ \begin{matrix}
   {\mathbf{Ⲗ}_{\text{aa}}\mathbf{=}\left( \mathbf{\Sigma}_{\text{aa}} - \mathbf{\Sigma}_{\text{ab}}\mathbf{\Sigma}_{\text{bb}}^{- 1}\mathbf{\Sigma}_{\text{ba}} \right)}^{- 1} \\
   \mathbf{Ⲗ}_{\text{ab}}\mathbf{=} - \mathbf{Ⲗ}_{\text{aa}}\mathbf{\Sigma}_{\text{ab}}\mathbf{\Sigma}_{\text{bb}}^{- 1} \\
   \end{matrix} \right.\ \mathbf{\Rightarrow}\left\{ \begin{matrix}
   \mathbf{Ⲗ}_{\text{aa}}^{- 1}\mathbf{Ⲗ}_{\text{ab}}\mathbf{= -}\mathbf{\Sigma}_{\text{ab}}\mathbf{\Sigma}_{\text{bb}}^{- 1} \\
   \mathbf{Ⲗ}_{\text{aa}}^{- 1}\mathbf{=}\mathbf{\Sigma}_{\text{aa}} - \mathbf{\Sigma}_{\text{ab}}\mathbf{\Sigma}_{\text{bb}}^{- 1}\mathbf{\Sigma}_{\text{ba}} \\
   \end{matrix} \right.\

Plug into **Error! Reference source not found.** we have

.. math:: \mathbf{\mu}_{a|b}\mathbf{=}\mathbf{\mu}_{a}\mathbf{+}\mathbf{\Sigma}_{\text{ab}}\mathbf{\Sigma}_{\text{bb}}^{- 1}\left( \mathbf{x}_{b}\mathbf{-}\mathbf{\mu}_{b} \right)\mathbf{,}\mathbf{\Sigma}_{a|b}\mathbf{=}\mathbf{\Sigma}_{\text{aa}} - \mathbf{\Sigma}_{\text{ab}}\mathbf{\Sigma}_{\text{bb}}^{- 1}\mathbf{\Sigma}_{\text{ba}}

Similarly, if :math:`\mathbf{\Sigma}_{\text{bb}}` is non-singular, then
:math:`p\left( \mathbf{x}_{b}|\mathbf{x}_{a} \right)` is Gaussian, and
from (1‑4) we have

.. math:: - \frac{1}{2}\mathbf{x}_{b}^{T}\mathbf{Ⲗ}_{\text{bb}}\mathbf{x}_{b} + \mathbf{x}_{b}^{T}\mathbf{Ⲗ}_{\text{bb}}\mathbf{\mu}_{b}\mathbf{-}\left( \mathbf{x}_{a}\mathbf{-}\mathbf{\mu}_{a} \right)^{T}\mathbf{Ⲗ}_{\text{ab}}\mathbf{x}_{b}\mathbf{+}\text{constant} = - \frac{1}{2}\mathbf{x}_{b}^{T}\mathbf{Ⲗ}_{\text{bb}}\mathbf{x}_{b} + \mathbf{x}_{b}^{T}\mathbf{Ⲗ}_{\text{bb}}\mathbf{\mu}_{b}\mathbf{-}\mathbf{x}_{b}^{T}\mathbf{Ⲗ}_{\text{ba}}\left( \mathbf{x}_{a}\mathbf{-}\mathbf{\mu}_{a} \right)\mathbf{+}\text{constant} = - \frac{1}{2}\mathbf{x}_{b}^{T}\mathbf{Ⲗ}_{\text{bb}}\mathbf{x}_{b} + \mathbf{x}_{b}^{T}\mathbf{Ⲗ}_{\text{bb}}\left( \mathbf{\mu}_{b}\mathbf{-}\mathbf{Ⲗ}_{\text{bb}}^{- 1}\mathbf{Ⲗ}_{\text{ba}}\left( \mathbf{x}_{a}\mathbf{-}\mathbf{\mu}_{a} \right) \right)\mathbf{+}\text{constant}

If in addition :math:`\mathbf{\Sigma}_{\text{aa}}` is non-singular,
using *Fact 1‑6* we have

.. math:: \mathbf{Ⲗ}_{\text{bb}}^{- 1}\mathbf{Ⲗ}_{\text{ba}}\mathbf{= -}\mathbf{\Sigma}_{\text{ba}}\mathbf{\Sigma}_{\text{aa}}^{- 1}\mathbf{,}\mathbf{Ⲗ}_{\text{bb}}^{- 1}\mathbf{=}\mathbf{\Sigma}_{\text{bb}}\mathbf{-}\mathbf{\Sigma}_{\text{ba}}\mathbf{\Sigma}_{\text{aa}}^{- 1}\mathbf{\Sigma}_{\text{ab}}

Finally,

+-----------------------+-----------------------+-----------------------+
| .. math:: \mathbf{\mu |                       | (1‑6)                 |
| }_{b|a}\mathbf{=}\mat |                       |                       |
| hbf{\mu}_{b}\mathbf{- |                       |                       |
| }\mathbf{Ⲗ}_{\text{bb |                       |                       |
| }}^{- 1}\mathbf{Ⲗ}_{\ |                       |                       |
| text{ba}}\left( \math |                       |                       |
| bf{x}_{a}\mathbf{-}\m |                       |                       |
| athbf{\mu}_{a} \right |                       |                       |
| )\mathbf{=}\mathbf{\m |                       |                       |
| u}_{b}\mathbf{+}\math |                       |                       |
| bf{\Sigma}_{\text{ba} |                       |                       |
| }\mathbf{\Sigma}_{\te |                       |                       |
| xt{aa}}^{- 1}\left( \ |                       |                       |
| mathbf{x}_{a}\mathbf{ |                       |                       |
| -}\mathbf{\mu}_{a} \r |                       |                       |
| ight)                 |                       |                       |
|                       |                       |                       |
| .. math:: \mathbf{\Si |                       |                       |
| gma}_{b|a}\mathbf{=}\ |                       |                       |
| mathbf{Ⲗ}_{\text{bb}} |                       |                       |
| ^{- 1}\mathbf{=}\math |                       |                       |
| bf{\Sigma}_{\text{bb} |                       |                       |
| }\mathbf{-}\mathbf{\S |                       |                       |
| igma}_{\text{ba}}\mat |                       |                       |
| hbf{\Sigma}_{\text{aa |                       |                       |
| }}^{- 1}\mathbf{\Sigm |                       |                       |
| a}_{\text{ab}}        |                       |                       |
+-----------------------+-----------------------+-----------------------+

We *note* 1) the conditional mean and variance is simpler in terms of
precision, as seen in (1‑5) and (1‑6); 2) given the unknowns and
conditions, the conditional mean :math:`\mathbf{\mu}_{a|b}` is
independent of :math:`\mathbf{x}_{a}`, :math:`\mathbf{\mu}_{b|a}` is
independent of :math:`\mathbf{x}_{b}`, and both
:math:`\mathbf{\Sigma}_{a|b}\mathbf{,}\mathbf{\Sigma}_{b|a}` are
independent of :math:`\mathbf{x}`.

-  **Theorem** **1‑7**\ **Marginal density**. Given
   :math:`\mathbf{x}\sim\operatorname{Gaussian}\left( \mathbf{\mu},\mathbf{\Sigma} \right)`,
   partition :math:`\mathbf{x} = \begin{pmatrix}
   \mathbf{x}_{a} \\
   \mathbf{x}_{b} \\
   \end{pmatrix}`, :math:`\mathbf{\mu} =` :math:`\begin{pmatrix}
   \mathbf{\mu}_{a} \\
   \mathbf{\mu}_{b} \\
   \end{pmatrix}`, :math:`\mathbf{\Sigma} = \begin{pmatrix}
   \mathbf{\Sigma}_{\text{aa}} & \mathbf{\Sigma}_{\text{ab}} \\
   \mathbf{\Sigma}_{\text{ba}} & \mathbf{\Sigma}_{\text{bb}} \\
   \end{pmatrix}` and :math:`\mathbf{Ⲗ} = \begin{pmatrix}
   \mathbf{Ⲗ}_{\text{aa}} & \mathbf{Ⲗ}_{\text{ab}} \\
   \mathbf{Ⲗ}_{\text{ba}} & \mathbf{Ⲗ}_{\text{bb}} \\
   \end{pmatrix}` as in *Theorem 1‑6*, and we want to find the marginal
   density
   :math:`p\left( \mathbf{x}_{a} \right) = \int_{}^{}{p\left( \mathbf{x}_{a},\mathbf{x}_{b} \right)d\mathbf{x}_{b}}`.
   Let’s break down the terms in the exponential into two parts based on
   the result of (1‑4) and (1‑6), one dependent on
   :math:`\mathbf{x}_{b}` and can be integrated as a constant, and the
   other is dependent on :math:`\mathbf{x}_{a}`,

.. math:: - \frac{1}{2}\left( \mathbf{x} - \mathbf{\mu} \right)^{T}\mathbf{Ⲗ}\left( \mathbf{x} - \mathbf{\mu} \right) = - \frac{1}{2}\left( \mathbf{x}_{b} - \mathbf{\mu}_{b|a} \right)^{T}\mathbf{Ⲗ}_{\text{bb}}\left( \mathbf{x}_{b} - \mathbf{\mu}_{b|a} \right) + \frac{1}{2}\mathbf{\mu}_{b|a}^{T}\mathbf{Ⲗ}_{\text{bb}}\mathbf{\mu}_{b|a} - \frac{1}{2}\mathbf{x}_{a}^{T}\mathbf{Ⲗ}_{\text{aa}}\mathbf{x}_{a} + \mathbf{x}_{a}^{T}\mathbf{Ⲗ}_{\text{aa}}\mathbf{\mu}_{a} + \mathbf{x}_{a}^{T}\mathbf{Ⲗ}_{\text{ab}}\mathbf{\mu}_{b}\mathbf{+}\text{constant}

Assume :math:`\mathbf{\Sigma}_{\text{bb}}` is non-singular. Since the
first term will be integrated out as a constant, we only need to concern
remaining terms. Further, we clean up
:math:`\frac{1}{2}\mathbf{\mu}_{b|a}^{T}\mathbf{Ⲗ}_{\text{bb}}\mathbf{\mu}_{b|a}`
and have

.. math:: \frac{1}{2}\mathbf{\mu}_{b|a}^{T}\mathbf{Ⲗ}_{\text{bb}}\mathbf{\mu}_{b|a} = \frac{1}{2}\left( \mathbf{\mu}_{b}\mathbf{-}\mathbf{Ⲗ}_{\text{bb}}^{- 1}\mathbf{Ⲗ}_{\text{ba}}\left( \mathbf{x}_{a}\mathbf{-}\mathbf{\mu}_{a} \right) \right)^{T}\mathbf{Ⲗ}_{\text{bb}}\left( \mathbf{\mu}_{b}\mathbf{-}\mathbf{Ⲗ}_{\text{bb}}^{- 1}\mathbf{Ⲗ}_{\text{ba}}\left( \mathbf{x}_{a}\mathbf{-}\mathbf{\mu}_{a} \right) \right) = \frac{1}{2}\left( \mathbf{Ⲗ}_{\text{bb}}^{- 1}\mathbf{Ⲗ}_{\text{ba}}\mathbf{x}_{a}\mathbf{-}\mathbf{Ⲗ}_{\text{bb}}^{- 1}\mathbf{Ⲗ}_{\text{ba}}\mathbf{\mu}_{a} \right)^{T}\mathbf{Ⲗ}_{\text{bb}}\left( \mathbf{Ⲗ}_{\text{bb}}^{- 1}\mathbf{Ⲗ}_{\text{ba}}\mathbf{x}_{a}\mathbf{-}\mathbf{Ⲗ}_{\text{bb}}^{- 1}\mathbf{Ⲗ}_{\text{ba}}\mathbf{\mu}_{a} \right) - \mathbf{\mu}_{b}^{T}\boxed{\mathbf{Ⲗ}_{\text{bb}}\mathbf{Ⲗ}_{\text{bb}}^{- 1}}\mathbf{Ⲗ}_{\text{ba}}\left( \mathbf{x}_{a}\mathbf{-}\mathbf{\mu}_{a} \right)\mathbf{+}\text{constant} = \frac{1}{2}\left( \mathbf{Ⲗ}_{\text{bb}}^{- 1}\mathbf{Ⲗ}_{\text{ba}}\mathbf{x}_{a} \right)^{T}\mathbf{Ⲗ}_{\text{bb}}\left( \mathbf{Ⲗ}_{\text{bb}}^{- 1}\mathbf{Ⲗ}_{\text{ba}}\mathbf{x}_{a} \right)\mathbf{-}\left( \mathbf{Ⲗ}_{\text{bb}}^{- 1}\mathbf{Ⲗ}_{\text{ba}}\mathbf{x}_{a} \right)^{T}\mathbf{Ⲗ}_{\text{bb}}\left( \mathbf{Ⲗ}_{\text{bb}}^{- 1}\mathbf{Ⲗ}_{\text{ba}}\mathbf{\mu}_{a} \right) - \mathbf{\mu}_{b}^{T}\mathbf{Ⲗ}_{\text{ba}}\mathbf{x}_{a}\mathbf{+}\text{constant} = \frac{1}{2}\mathbf{x}_{a}^{T}\mathbf{Ⲗ}_{\text{ab}}\mathbf{Ⲗ}_{\text{bb}}^{- 1}\mathbf{Ⲗ}_{\text{ba}}\mathbf{x}_{a}\mathbf{-}\mathbf{x}_{a}^{T}\mathbf{Ⲗ}_{\text{ab}}\mathbf{Ⲗ}_{\text{bb}}^{- 1}\mathbf{Ⲗ}_{\text{ba}}\mathbf{\mu}_{a} - \mathbf{x}_{a}^{T}\mathbf{Ⲗ}_{\text{ab}}\mathbf{\mu}_{b}\mathbf{+}\text{constant}

Then the terms dependent on :math:`\mathbf{x}_{a}` are

.. math:: \frac{1}{2}\mathbf{x}_{a}^{T}\mathbf{Ⲗ}_{\text{ab}}\mathbf{Ⲗ}_{\text{bb}}^{- 1}\mathbf{Ⲗ}_{\text{ba}}\mathbf{x}_{a}\mathbf{-}\mathbf{x}_{a}^{T}\mathbf{Ⲗ}_{\text{ab}}\mathbf{Ⲗ}_{\text{bb}}^{- 1}\mathbf{Ⲗ}_{\text{ba}}\mathbf{\mu}_{a}\boxed{- \mathbf{x}_{a}^{T}\mathbf{Ⲗ}_{\text{ab}}\mathbf{\mu}_{b}} - \frac{1}{2}\mathbf{x}_{a}^{T}\mathbf{Ⲗ}_{\text{aa}}\mathbf{x}_{a}\boxed{+ \mathbf{x}_{a}^{T}\mathbf{Ⲗ}_{\text{aa}}\mathbf{\mu}_{a}} + \mathbf{x}_{a}^{T}\mathbf{Ⲗ}_{\text{ab}}\mathbf{\mu}_{b}\mathbf{=} - \frac{1}{2}\mathbf{x}_{a}^{T}\left( \mathbf{Ⲗ}_{\text{aa}}\mathbf{-}\mathbf{Ⲗ}_{\text{ab}}\mathbf{Ⲗ}_{\text{bb}}^{- 1}\mathbf{Ⲗ}_{\text{ba}} \right)\mathbf{x}_{a}\mathbf{+}\mathbf{x}_{a}^{T}\left( \mathbf{Ⲗ}_{\text{aa}}\mathbf{-}\mathbf{Ⲗ}_{\text{ab}}\mathbf{Ⲗ}_{\text{bb}}^{- 1}\mathbf{Ⲗ}_{\text{ba}} \right)\mathbf{\mu}_{a}

By *Lemma 1‑2*, *Fact 1‑6* and :math:`\begin{pmatrix}
\mathbf{Ⲗ}_{\text{aa}} & \mathbf{Ⲗ}_{\text{ab}} \\
\mathbf{Ⲗ}_{\text{ba}} & \mathbf{Ⲗ}_{\text{bb}} \\
\end{pmatrix}^{- 1} = \begin{pmatrix}
\mathbf{\Sigma}_{\text{aa}} & \mathbf{\Sigma}_{\text{ab}} \\
\mathbf{\Sigma}_{\text{ba}} & \mathbf{\Sigma}_{\text{bb}} \\
\end{pmatrix}`, we conclude if :math:`\mathbf{\Sigma}_{\text{bb}}` is
non-singular, :math:`p\left( \mathbf{x}_{a} \right)` is Gaussian with
mean :math:`\mathbf{\mu}_{a}` and covariance
:math:`\mathbf{\Sigma}_{a} = \left( \mathbf{Ⲗ}_{\text{aa}}\mathbf{-}\mathbf{Ⲗ}_{\text{ab}}\mathbf{Ⲗ}_{\text{bb}}^{- 1}\mathbf{Ⲗ}_{\text{ba}} \right)^{- 1}\mathbf{=}\mathbf{\Sigma}_{\text{aa}}`;
likewise, if :math:`\mathbf{\Sigma}_{\text{aa}}` is non-singular,
:math:`p\left( \mathbf{x}_{b} \right)` is Gaussian with mean
:math:`\mathbf{\mu}_{b}` and covariance
:math:`\mathbf{\Sigma}_{b} = \left( \mathbf{Ⲗ}_{\text{bb}}\mathbf{-}\mathbf{Ⲗ}_{\text{ba}}\mathbf{Ⲗ}_{\text{aa}}^{- 1}\mathbf{Ⲗ}_{\text{ab}} \right)^{- 1}\mathbf{=}\mathbf{\Sigma}_{\text{bb}}`.

-  **Theorem** **1‑8**\ **Bayesian Theorem for Gaussian**. Given prior
   distribution
   :math:`\mathbf{x}\sim\operatorname{Gaussian}\left( \mathbf{\mu},\mathbf{Ⲗ}_{x}^{- 1} \right)`
   (the prior for Gaussian is chosen as Gaussian because of
   **conjugacy**) and condition
   :math:`\mathbf{y}|\mathbf{x}\sim\operatorname{Gaussian}\left( \mathbf{\text{Ax}} + \mathbf{b},\mathbf{Ⲗ}_{y|x}^{- 1} \right)`,
   the key goal is to derive the posterior distribution
   :math:`p\left( \mathbf{x} \middle| \mathbf{y} \right)` and the
   marginal distribution :math:`p\left( \mathbf{y} \right)`. The
   approach is to first derive joint distribution
   :math:`p\left( \mathbf{x},\mathbf{y} \right)`, then
   :math:`\mathbf{x}|\mathbf{y}` is conditional distribution of
   :math:`p\left( \mathbf{x},\mathbf{y} \right)` (*Theorem 1‑6*), and
   :math:`p\left( \mathbf{y} \right)` is the marginal distribution of
   :math:`p\left( \mathbf{x},\mathbf{y} \right)` (*Theorem 1‑7*). First,
   we have

.. math:: \ln{p\left( \mathbf{x},\mathbf{y} \right)} = \ln{p\left( \mathbf{x} \right)p\left( y|\mathbf{x} \right)} \propto - \frac{1}{2}\left( \mathbf{x} - \mathbf{\mu} \right)^{T}\mathbf{Ⲗ}_{x}\left( \mathbf{x} - \mathbf{\mu} \right) - \frac{1}{2}\left( \mathbf{y} - \mathbf{\text{Ax}} - \mathbf{b} \right)^{T}\mathbf{Ⲗ}_{y|x}\left( \mathbf{y} - \mathbf{\text{Ax}} - \mathbf{b} \right)

where the RHS is the terms in the exponential of
:math:`p\left( \mathbf{x},\mathbf{y} \right)`. Continue the
simplification: remove terms not dependent on either :math:`\mathbf{x}`
or :math:`\mathbf{y}`,

.. math::

   \ln{p\left( \mathbf{x},\mathbf{y} \right)} \propto - \frac{1}{2}\left( \mathbf{x}^{T}\mathbf{Ⲗ}_{x}\mathbf{x +}\mathbf{x}^{T}\mathbf{A}\mathbf{Ⲗ}_{y|x}\mathbf{\text{Ax}} \right)\mathbf{+}\frac{1}{2}\mathbf{x}^{T}\mathbf{A}^{T}\mathbf{Ⲗ}_{y|x}\mathbf{y} + \frac{1}{2}\mathbf{y}^{T}\mathbf{Ⲗ}_{y|x}\mathbf{\text{Ax}} - \frac{1}{2}\mathbf{y}^{T}\mathbf{Ⲗ}_{y|x}\mathbf{y}\mathbf{+}\mathbf{x}^{T}\mathbf{Ⲗ}_{x}\mathbf{\mu}\mathbf{-}\mathbf{x}^{T}\mathbf{A}^{T}\mathbf{Ⲗ}_{y|x}\mathbf{b +}\mathbf{y}^{T}\mathbf{Ⲗ}_{y|x}\mathbf{b}\mathbf{= -}\frac{1}{2}\begin{pmatrix}
   \mathbf{x} \\
   \mathbf{y} \\
   \end{pmatrix}^{T}\begin{pmatrix}
   \mathbf{Ⲗ}_{x}\mathbf{+}\mathbf{A}^{T}\mathbf{Ⲗ}_{y|x}\mathbf{A} & - \mathbf{A}^{T}\mathbf{Ⲗ}_{y|x} \\
    - \mathbf{Ⲗ}_{y|x}\mathbf{A} & \mathbf{Ⲗ}_{y|x} \\
   \end{pmatrix}\begin{pmatrix}
   \mathbf{x} \\
   \mathbf{y} \\
   \end{pmatrix} + \begin{pmatrix}
   \mathbf{x} \\
   \mathbf{y} \\
   \end{pmatrix}^{\mathbf{T}}\begin{pmatrix}
   \mathbf{Ⲗ}_{\mathbf{x}}\mathbf{\mu}\mathbf{-}\mathbf{A}^{\mathbf{T}}\mathbf{Ⲗ}_{\mathbf{y|x}}\mathbf{b} \\
   \mathbf{Ⲗ}_{\mathbf{y|x}}\mathbf{b} \\
   \end{pmatrix}

Then by *Lemma 1‑2*, :math:`p\left( \mathbf{x},\mathbf{y} \right)` is a
Gaussian with precision and mean

+-----------------------+-----------------------+-----------------------+
| .. math::             |                       | (1‑7)                 |
|                       |                       |                       |
|    \mathbf{Ⲗ}_{x,y}\m |                       |                       |
| athbf{=}\begin{pmatri |                       |                       |
| x}                    |                       |                       |
|    \mathbf{Ⲗ}_{x}\mat |                       |                       |
| hbf{+}\mathbf{A}^{T}\ |                       |                       |
| mathbf{Ⲗ}_{y|x}\mathb |                       |                       |
| f{A} & - \mathbf{A}^{ |                       |                       |
| T}\mathbf{Ⲗ}_{y|x} \\ |                       |                       |
|     - \mathbf{Ⲗ}_{y|x |                       |                       |
| }\mathbf{A} & \mathbf |                       |                       |
| {Ⲗ}_{y|x} \\          |                       |                       |
|    \end{pmatrix},\mat |                       |                       |
| hbf{\mu}_{x,y}\mathbf |                       |                       |
| {=}\mathbf{Ⲗ}_{x,y}^{ |                       |                       |
| - 1}\begin{pmatrix}   |                       |                       |
|    \mathbf{Ⲗ}_{\mathb |                       |                       |
| f{x}}\mathbf{\mu}\mat |                       |                       |
| hbf{-}\mathbf{A}^{\ma |                       |                       |
| thbf{T}}\mathbf{Ⲗ}_{\ |                       |                       |
| mathbf{y|x}}\mathbf{b |                       |                       |
| } \\                  |                       |                       |
|    \mathbf{Ⲗ}_{\mathb |                       |                       |
| f{y|x}}\mathbf{b} \\  |                       |                       |
|    \end{pmatrix}      |                       |                       |
+-----------------------+-----------------------+-----------------------+

..

   provided that :math:`\mathbf{Ⲗ}_{x,y}` is non-singular. Since
   :math:`\mathbf{Ⲗ}_{y|x}` is non-singular, using *Fact 1‑6*, we have

.. math::

   \mathbf{Ⲗ}_{x,y}^{- 1}\mathbf{=}\begin{pmatrix}
   \left( \mathbf{Ⲗ}_{x}\mathbf{+ A}\mathbf{Ⲗ}_{y|x}\mathbf{A -}\mathbf{A}^{T}\boxed{\mathbf{Ⲗ}_{y|x}\mathbf{Ⲗ}_{y|x}^{- 1}}\mathbf{Ⲗ}_{y|x}\mathbf{A} \right)^{- 1}\mathbf{=}\mathbf{Ⲗ}_{x}^{- 1} & \mathbf{Ⲗ}_{x}^{- 1}\mathbf{A}^{T}\boxed{\mathbf{Ⲗ}_{y|x}\mathbf{Ⲗ}_{y|x}^{- 1}} = \mathbf{Ⲗ}_{x}^{- 1}\mathbf{A}^{T} \\
   \boxed{\mathbf{Ⲗ}_{y|x}^{- 1}\mathbf{Ⲗ}_{y|x}}\mathbf{A}\mathbf{Ⲗ}_{x}^{- 1}\mathbf{=}\mathbf{A}\mathbf{Ⲗ}_{x}^{- 1} & \mathbf{Ⲗ}_{y|x}^{- 1}\mathbf{+}\boxed{\mathbf{Ⲗ}_{y|x}^{- 1}\mathbf{Ⲗ}_{y|x}}\mathbf{A}\mathbf{Ⲗ}_{x}^{- 1}\mathbf{A}^{T} = \mathbf{Ⲗ}_{y|x}^{- 1}\mathbf{+}\mathbf{A}\mathbf{Ⲗ}_{x}^{- 1}\mathbf{A}^{T} \\
   \end{pmatrix}\mathbf{\Rightarrow}\mathbf{\Sigma}_{x,y}\mathbf{=}\mathbf{Ⲗ}_{x,y}^{- 1}\mathbf{=}\begin{pmatrix}
   \mathbf{Ⲗ}_{x}^{- 1} & \mathbf{Ⲗ}_{x}^{- 1}\mathbf{A}^{T} \\
   \mathbf{A}\mathbf{Ⲗ}_{x}^{- 1} & \mathbf{Ⲗ}_{y|x}^{- 1}\mathbf{+}\mathbf{A}\mathbf{Ⲗ}_{x}^{- 1}\mathbf{A}^{T} \\
   \end{pmatrix}

and therefore,

.. math::

   \mathbf{\mu}_{x,y} = \mathbf{Ⲗ}_{x,y}^{- 1}\mathbf{\ }\begin{pmatrix}
   \mathbf{Ⲗ}_{x}\mathbf{\mu}\mathbf{-}\mathbf{A}^{T}\mathbf{Ⲗ}_{y|x}\mathbf{b} \\
   \mathbf{Ⲗ}_{y|x}\mathbf{b} \\
   \end{pmatrix} = \begin{pmatrix}
   \mathbf{\mu} \\
   \mathbf{\text{Aμ}} + \mathbf{b} \\
   \end{pmatrix}

Note the posterior :math:`p\left( \mathbf{x}|\mathbf{y} \right)` is the
conditional distribution we found in *Theorem 1‑6*, and thus by (1‑5) we
have

+-----------------------+-----------------------+-----------------------+
| .. math::             |                       | (1‑8)                 |
|                       |                       |                       |
|    \left\{ \begin{mat |                       |                       |
| rix}                  |                       |                       |
|    \mathbf{\mu}_{x|y} |                       |                       |
| \mathbf{=}\mathbf{\mu |                       |                       |
| }\mathbf{+}\left( \ma |                       |                       |
| thbf{Ⲗ}_{x}\mathbf{+} |                       |                       |
| \mathbf{A}^{T}\mathbf |                       |                       |
| {Ⲗ}_{y|x}\mathbf{A} \ |                       |                       |
| right)^{- 1}\mathbf{A |                       |                       |
| }^{T}\mathbf{Ⲗ}_{y|x} |                       |                       |
| \left( \mathbf{y}\mat |                       |                       |
| hbf{-}\mathbf{\text{A |                       |                       |
| μ}} - \mathbf{b} \rig |                       |                       |
| ht) \\                |                       |                       |
|    \mathbf{\Sigma}_{x |                       |                       |
| |y}\mathbf{=}\left( \ |                       |                       |
| mathbf{Ⲗ}_{x}\mathbf{ |                       |                       |
| +}\mathbf{A}^{T}\math |                       |                       |
| bf{Ⲗ}_{y|x}\mathbf{A} |                       |                       |
|  \right)^{- 1} \\     |                       |                       |
|    \end{matrix} \righ |                       |                       |
| t.\                   |                       |                       |
+-----------------------+-----------------------+-----------------------+

We can also rearrange the terms as

+-----------------------+-----------------------+-----------------------+
| .. math:: \mathbf{\mu |                       | (1‑9)                 |
| }_{x|y}\mathbf{=}\lef |                       |                       |
| t( \mathbf{Ⲗ}_{x}\mat |                       |                       |
| hbf{+}\mathbf{A}^{T}\ |                       |                       |
| mathbf{Ⲗ}_{y|x}\mathb |                       |                       |
| f{A} \right)^{- 1}\le |                       |                       |
| ft( \left( \mathbf{Ⲗ} |                       |                       |
| _{x}\mathbf{+}\mathbf |                       |                       |
| {A}^{T}\mathbf{Ⲗ}_{y| |                       |                       |
| x}\mathbf{A} \right)\ |                       |                       |
| mathbf{\mu}\mathbf{+} |                       |                       |
| \mathbf{A}^{T}\mathbf |                       |                       |
| {Ⲗ}_{y|x}\left( \math |                       |                       |
| bf{y}\mathbf{-}\mathb |                       |                       |
| f{\text{Aμ}} - \mathb |                       |                       |
| f{b} \right) \right)\ |                       |                       |
| mathbf{=}\left( \math |                       |                       |
| bf{Ⲗ}_{x}\mathbf{+}\m |                       |                       |
| athbf{A}^{T}\mathbf{Ⲗ |                       |                       |
| }_{y|x}\mathbf{A} \ri |                       |                       |
| ght)^{- 1}\left( \mat |                       |                       |
| hbf{Ⲗ}_{x}\mathbf{\mu |                       |                       |
|  +}\boxed{\mathbf{A}^ |                       |                       |
| {T}\mathbf{Ⲗ}_{y|x}\m |                       |                       |
| athbf{\text{Aμ}}}\mat |                       |                       |
| hbf{+}\mathbf{A}^{T}\ |                       |                       |
| mathbf{Ⲗ}_{y|x}\mathb |                       |                       |
| f{y}\mathbf{-}\boxed{ |                       |                       |
| \mathbf{A}^{T}\mathbf |                       |                       |
| {Ⲗ}_{y|x}\mathbf{\tex |                       |                       |
| t{Aμ}}}\mathbf{-}\mat |                       |                       |
| hbf{A}^{T}\mathbf{Ⲗ}_ |                       |                       |
| {y|x}\mathbf{b} \righ |                       |                       |
| t)\mathbf{\Rightarrow |                       |                       |
| }\mathbf{\mu}_{x|y}\m |                       |                       |
| athbf{=}\left( \mathb |                       |                       |
| f{Ⲗ}_{x}\mathbf{+}\ma |                       |                       |
| thbf{A}^{T}\mathbf{Ⲗ} |                       |                       |
| _{y|x}\mathbf{A} \rig |                       |                       |
| ht)^{- 1}\left( \math |                       |                       |
| bf{Ⲗ}_{x}\mathbf{\mu  |                       |                       |
| +}\mathbf{A}^{T}\math |                       |                       |
| bf{Ⲗ}_{y|x}\left( \ma |                       |                       |
| thbf{y}\mathbf{-}\mat |                       |                       |
| hbf{b} \right) \right |                       |                       |
| )                     |                       |                       |
+-----------------------+-----------------------+-----------------------+

By *Theorem 1‑7*, we have
:math:`p\left( \mathbf{y} \right)\sim\operatorname{Gaussian}\left( \mathbf{\mu}_{y},\mathbf{\Sigma}_{y} \right)`
where

+-----------------------+-----------------------+-----------------------+
| .. math::             |                       | (1‑10)                |
|                       |                       |                       |
|    \left\{ \begin{mat |                       |                       |
| rix}                  |                       |                       |
|    \mathbf{\mu}_{y} = |                       |                       |
|  \mathbf{\text{Aμ}} + |                       |                       |
|  \mathbf{b} \\        |                       |                       |
|    \mathbf{\Sigma}_{y |                       |                       |
| } = \mathbf{Ⲗ}_{y|x}^ |                       |                       |
| {- 1}\mathbf{+}\mathb |                       |                       |
| f{A}\mathbf{Ⲗ}_{x}^{- |                       |                       |
|  1}\mathbf{A}^{T} = \ |                       |                       |
| mathbf{\Sigma}_{y|x}\ |                       |                       |
| mathbf{+}\mathbf{A}\m |                       |                       |
| athbf{\Sigma}_{x}\mat |                       |                       |
| hbf{A}^{T} \\         |                       |                       |
|    \end{matrix} \righ |                       |                       |
| t.\                   |                       |                       |
+-----------------------+-----------------------+-----------------------+

**Corollary** **1‑2**. In the case of
:math:`\mathbf{A} = \mathbf{I},\mathbf{b} = \mathbf{0}` and
:math:`\mathbf{x},\mathbf{y}` have the same dimension, i.e.
:math:`\mathbf{x}\sim\operatorname{Gaussian}\left( \mathbf{\mu},\mathbf{Ⲗ}_{x}^{- 1} \right)`
and
:math:`\mathbf{y}|\mathbf{x}\sim\operatorname{Gaussian}\left( \mathbf{x},\mathbf{Ⲗ}_{y|x}^{- 1} \right)`
then

.. math::

   {\left\{ \begin{matrix}
   \mathbf{\mu}_{x|y}\mathbf{=}\mathbf{\mu}\mathbf{+}\left( \mathbf{Ⲗ}_{x}\mathbf{+}\mathbf{Ⲗ}_{y|x} \right)^{- 1}\mathbf{Ⲗ}_{y|x}\left( \mathbf{y}\mathbf{-}\mathbf{\mu} \right)\mathbf{=}\left( \mathbf{Ⲗ}_{x}\mathbf{+}\mathbf{Ⲗ}_{y|x} \right)^{- 1}\left( \mathbf{Ⲗ}_{x}\mathbf{\mu +}\mathbf{Ⲗ}_{y|x}\mathbf{y} \right) \\
   \mathbf{\Sigma}_{x|y}\mathbf{=}\left( \mathbf{Ⲗ}_{x}\mathbf{+}\mathbf{Ⲗ}_{y|x} \right)^{- 1} \\
   \end{matrix} \right.\
   }\left\{ \begin{matrix}
   \mathbf{\mu}_{y} = \mathbf{\mu} \\
   \mathbf{\Sigma}_{y} = \mathbf{Ⲗ}_{y|x}^{- 1}\mathbf{+}\mathbf{Ⲗ}_{x}^{- 1} = \mathbf{\Sigma}_{y|x}\mathbf{+}\mathbf{\Sigma}_{x} \\
   \end{matrix} \right.\

   **Corollary** **1‑3**. The joint distribution of two independent
   Gaussian RVs :math:`\mathbf{x},\mathbf{y}` s.t.
   :math:`\mathbf{x}\sim\operatorname{Gaussian}\left( \mathbf{\mu}_{x},\mathbf{\Sigma}_{x} \right)`
   and
   :math:`\mathbf{y}\sim\operatorname{Gaussian}\left( \mathbf{\mu}_{y},\mathbf{\Sigma}_{t} \right)`
   is Gaussian. This is equivalent to :math:`\mathbf{A} = \mathbf{O}`
   and :math:`\mathbf{b} = \mathbf{\mu}_{2}`, and we have

.. math::

   \mathbf{\mu}_{x,y} = \begin{pmatrix}
   \mathbf{\mu}_{x} \\
   \mathbf{\mu}_{y} \\
   \end{pmatrix},\mathbf{\Sigma}_{x,y}\mathbf{=}\begin{pmatrix}
   \mathbf{\Sigma}_{1} & \mathbf{O} \\
   \mathbf{O} & \mathbf{\Sigma}_{y} \\
   \end{pmatrix}

**Example** **1‑1**. Given prior distribution
:math:`\mathbf{x}\sim\operatorname{Gaussian}\left( \mathbf{\mu},\mathbf{Ⲗ}_{x}^{- 1} \right)`,
and we observe multiple i.i.d. realizations
:math:`\mathbf{y} = \left( y_{1},\ldots,y_{N} \right)` from
:math:`\operatorname{Gaussian}\left( \mathbf{a}^{T}\mathbf{x +}b,\sigma^{2} \right)`
. By previous corollary the joint distribution is
:math:`\mathbf{y\sim}\operatorname{Gaussian}\left( \mathbf{1}\mathbf{a}^{T}\mathbf{x + b},\sigma^{2}\mathbf{I} \right)`.
Then the prior
:math:`p\left( \mathbf{x}|\mathbf{y} \right)\sim\operatorname{Gaussian}\left( \mathbf{\mu}_{x|y}\mathbf{,}\mathbf{\Sigma}_{x|y} \right)`,
where

.. math:: \mathbf{\Sigma}_{x|y} = \left( \mathbf{Ⲗ}_{x} + \frac{1}{\sigma^{2}}\mathbf{a}\mathbf{1}^{T}\mathbf{1}\mathbf{a}^{T} \right)^{- 1} = \left( \mathbf{Ⲗ}_{x} + \frac{N}{\sigma^{2}}\mathbf{a}\mathbf{a}^{T} \right)^{- 1}

.. math:: \mathbf{\mu}_{x|y}\mathbf{=}\mathbf{\mu}\mathbf{+}\frac{1}{\sigma^{2}}\mathbf{\Sigma}_{x|y}\mathbf{a}\mathbf{1}^{T}\left( \mathbf{y}\mathbf{-}\mathbf{1}\mathbf{a}^{T}\mathbf{\mu} - \mathbf{b} \right)\mathbf{=}\mathbf{\mu}\mathbf{+}\frac{1}{\sigma^{2}}\mathbf{\Sigma}_{x|y}\left( \mathbf{a}\mathbf{1}^{T}\mathbf{y}\mathbf{-}\mathbf{a}\mathbf{1}^{T}\mathbf{1}\mathbf{a}^{T}\mathbf{\mu} - \mathbf{a}\mathbf{1}^{T}\mathbf{b} \right)\mathbf{=}\mathbf{\mu}\mathbf{+}\frac{1}{\sigma^{2}}\mathbf{\Sigma}_{x|y}\left( \left( \sum_{i = 1}^{N}{y_{i} - b} \right)\mathbf{a}\mathbf{-}N\mathbf{a}\mathbf{a}^{T}\mathbf{\mu} \right)

From (*1*\ ‑\ *9*) we also have,

.. math:: \mathbf{\mu}_{x|y}\mathbf{=}\mathbf{\Sigma}_{x|y}\left( \mathbf{Ⲗ}_{x}\mathbf{\mu +}\frac{1}{\sigma^{2}}\mathbf{a}\mathbf{1}^{T}\left( \mathbf{y}\mathbf{-}\mathbf{b} \right) \right)\mathbf{=}\mathbf{\Sigma}_{x|y}\left( \mathbf{Ⲗ}_{x}\mathbf{\mu +}\frac{\sum_{i = 1}^{N}{y_{i} - b}}{\sigma^{2}}\mathbf{a} \right)

As a summary,

.. math::

   \left\{ \begin{matrix}
   \mathbf{\mu}_{x|y}\mathbf{=}\mathbf{\mu}\mathbf{+}\frac{1}{\sigma^{2}}\mathbf{\Sigma}_{x|y}\left( \left( \sum_{i = 1}^{N}{y_{i} - b} \right)\mathbf{a}\mathbf{-}N\mathbf{a}\mathbf{a}^{T}\mathbf{\mu} \right)\mathbf{=}\mathbf{\Sigma}_{x|y}\left( \mathbf{Ⲗ}_{x}\mathbf{\mu +}\frac{\sum_{i = 1}^{N}{y_{i} - b}}{\sigma^{2}}\mathbf{a} \right) \\
   \mathbf{\Sigma}_{x|y} = \left( \mathbf{Ⲗ}_{x} + \frac{N}{\sigma^{2}}\mathbf{a}\mathbf{a}^{T} \right)^{- 1} \\
   \end{matrix} \right.\

**Student-T and F distribution**
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

**MLE & MAP**
^^^^^^^^^^^^^

**Entropy & Cross-Entropy**
^^^^^^^^^^^^^^^^^^^^^^^^^^^

**Exponential Family**
^^^^^^^^^^^^^^^^^^^^^^

Linear Models
~~~~~~~~~~~~~

**Linear Regression**
^^^^^^^^^^^^^^^^^^^^^^

We assume the data we are dealing with can be transformed or coded into
a **data set** :math:`\left( \mathbf{X},\mathbf{y} \right)`, where the
**data matrix**
:math:`\mathbf{X} = \left( \mathbf{x}_{1},\ldots,\mathbf{x}_{N} \right) \in \mathbb{R}^{m \times N}`
contains :math:`m`-dimensional **data entries**, each data entry viewed
as a column vector, and the **observation vector** or **target vector**
or **label vector**
:math:`\mathbf{y =}\left( y_{1},\ldots,y_{N} \right) \in \mathbb{R}^{N}`
contains :math:`n`-dimensional corresponding **properties** or
**labels**. The **data size** is :math:`N`.

We first introduce the **deterministic regression model**. The goal is
to find a **regression function**
:math:`f\left( \mathbf{x} \right):\mathbb{R}^{m}\mathbb{\rightarrow R}`.
The argument :math:`\mathbf{x}` for :math:`f` is named **independent
variable** or the **explanatory variable**, and the output
:math:`\widetilde{y} = f\left( \mathbf{x} \right)` is named the
**response variable** or **dependent variable**. Input data entries
:math:`\mathbf{X}`, the regression function generates a **response
vector** :math:`\widehat{\mathbf{y}} = \begin{pmatrix}
{\widehat{y}}_{1} \\
 \vdots \\
{\widehat{y}}_{N} \\
\end{pmatrix} = \begin{pmatrix}
f\left( \mathbf{x}_{1} \right) \\
 \vdots \\
f\left( \mathbf{x}_{N} \right) \\
\end{pmatrix}`. For a **future variable** :math:`\mathcal{x}`, :math:`f`
provides **prediction**
:math:`\mathcal{y} = f\left( \mathcal{x} \right)`.

The difference between the responses and the observations,
:math:`\mathbf{\epsilon} = \mathbf{y} - \widehat{\mathbf{y}}` is call
the **error** **vector**, some distance (very often a mathematical
**metric**) between the response and the observation, denoted by
:math:`d\left( \mathbf{y},\widehat{\mathbf{y}} \right)`, is called the
**error**, where a typical error is the **squared error**
:math:`d\left( \mathbf{y},\widehat{\mathbf{y}} \right) = \left\| \mathbf{y} - \widehat{\mathbf{y}} \right\|_{2}^{2}`,
i.e. the 2-norm of the error vector. For **linear regression**, a linear
regression function :math:`f` is defined as an affine function

+-----------------------+-----------------------+-----------------------+
| .. math:: \widehat{y} |                       | (2‑1)                 |
|  = f\left( \mathbf{x} |                       |                       |
|  \right) = \mathbf{w} |                       |                       |
| ^{T}\mathbf{x +}b = \ |                       |                       |
| mathbf{x}^{T}\mathbf{ |                       |                       |
| w} + b                |                       |                       |
+-----------------------+-----------------------+-----------------------+

where :math:`\mathbf{w}\mathbf{=}\begin{pmatrix}
w_{1} \\
\mathbf{\vdots} \\
w_{m} \\
\end{pmatrix}` is a **coefficient vector** or **weight vector**, and
:math:`b` the **bias** uniquely determines the linear regression
function :math:`f`. For the entire data set, we have

.. math:: \widehat{\mathbf{y}}\mathbf{=}f\left( \mathbf{X} \right)\mathbf{=}\mathbf{X}^{T}\mathbf{w +}b

where we put it “\ :math:`\mathbf{X}^{T}\mathbf{w}`\ ” rather than
:math:`\mathbf{w}^{T}\mathbf{X}` to keep :math:`\widehat{\mathbf{y}}` a
column vector.

Given a set of functions
:math:`\phi_{j}\left( \mathbf{x} \right):\mathbb{R}^{m}\mathbb{\rightarrow R,}j = 1,\ldots,\mathbb{R}`,
we say they are **linearly independent functions** if
:math:`\sum_{j = 1}^{M}{k_{j}\phi_{j}}\left( \mathbf{x} \right) = 0,\forall\mathbf{x} \in \mathbb{R}^{m}`
iff :math:`k_{j} = 0,\forall j = 1,\ldots,M`, or *in plain words* any
function cannot be written as a linear combination of other functions.
In this case, the set of all linear combinations of
:math:`\phi_{j},j = 1,\ldots,M` form a **functional space**, and
:math:`\phi_{j}`\ s are referred to as **basis functions**, analogous to
the classic concept of **vector space** defined for real numbers. Let
:math:`\phi\mathbf{=}\left( \phi_{1},\ldots,\phi_{M} \right)\mathbf{:}\mathbb{R}^{m} \rightarrow \mathbb{R}^{M}`,
a simple *example* of basis function is
:math:`\phi:\mathbb{R}^{m} \rightarrow \mathbb{R}^{m + 1}` s.t.
:math:`\phi_{j}\left( \mathbf{x} \right) = \left\{ \begin{matrix}
1 & j = 1 \\
x_{j - 1} & j \neq 1 \\
\end{matrix} \right.\  \Rightarrow \phi\left( \mathbf{x} \right) = \begin{pmatrix}
1 \\
x_{1} \\
 \vdots \\
x_{m} \\
\end{pmatrix} = \begin{pmatrix}
1 \\
\mathbf{x} \\
\end{pmatrix}`; another simple example is
:math:`\phi:\mathbb{R}^{m} \rightarrow \mathbb{R}^{m}` s.t.
:math:`\phi_{j}\left( \mathbf{x} \right) = x_{j}^{j} \Rightarrow \phi\left( \mathbf{x} \right) = \begin{pmatrix}
x_{1} \\
x_{2}^{2} \\
 \vdots \\
x_{m}^{m} \\
\end{pmatrix}`. We can generalize (2‑1) to **linear regression model**,
which can be written as

+-----------------------+-----------------------+-----------------------+
| .. math:: \widehat{y} |                       | (2‑2)                 |
|  = \mathbf{w}^{T}\phi |                       |                       |
| \left( \mathbf{x} \ri |                       |                       |
| ght)\mathbf{=}\phi^{T |                       |                       |
| }\left( \mathbf{x} \r |                       |                       |
| ight)\mathbf{w}       |                       |                       |
+-----------------------+-----------------------+-----------------------+

where
:math:`\phi\mathbf{=}\left( \phi_{1},\ldots,\phi_{M} \right)\mathbf{:}\mathbb{R}^{m} \rightarrow \mathbb{R}^{M}`
are basis functions that transform :math:`\mathbf{x}` according to
modeling needs, and :math:`\mathbf{w}` becomes a coefficient vector of
size :math:`M`. When the basis function is like
:math:`\phi\left( \mathbf{x} \right) = \begin{pmatrix}
1 \\
\mathbf{x} \\
\end{pmatrix}`, then the linear model equals the linear regression. Of
course, (2‑2) could be non-linear in terms of :math:`\mathbf{x}`, but is
linear in terms of :math:`\phi\left( \mathbf{x} \right)`. For the entire
data set, we have

+-----------------------+-----------------------+-----------------------+
| .. math:: \widehat{\m |                       | (2‑3)                 |
| athbf{y}}\mathbf{=}\p |                       |                       |
| hi^{T}\left( \mathbf{ |                       |                       |
| X} \right)\mathbf{w = |                       |                       |
| }\mathbf{\Phi}^{T}\ma |                       |                       |
| thbf{w}               |                       |                       |
+-----------------------+-----------------------+-----------------------+

where we use :math:`\mathbf{\Phi}` to denote
:math:`\phi^{T}\left( \mathbf{X} \right)` for simplicity. The purpose of
using basis functions is to mathematically ensure (2‑2) and (2‑3) cannot
be written as something “of smaller size”; if :math:`\phi` is not basis
functions and contain linear dependent functions, the some columns of
:math:`\phi\left( \mathbf{x} \right)` or :math:`\mathbf{\Phi}^{T}` can
be trivially collapsed, and the number of weights in :math:`\mathbf{w}`
can be reduced. The basis functions are also widely referred to as
**representations** in the context of machine learning.

The above definitions can of course be generalized to higher-dimensional
labels
:math:`\mathbf{Y =}\left( \mathbf{y}_{1},\ldots,\mathbf{y}_{N} \right) \in \mathbb{R}^{K \times N}`,
and then we have a weight matrix
:math:`\mathbf{W}^{T}\mathbf{=}\begin{pmatrix}
\mathbf{w}_{1} \\
\mathbf{\vdots} \\
\mathbf{w}_{K} \\
\end{pmatrix}\mathbf{\in}\mathbb{R}^{K \times M}` and (2‑2) becomes

.. math:: \widehat{\mathbf{y}} = \mathbf{W}^{T}\phi\left( \mathbf{x} \right)

where we exchange the positions of the weights and :math:`\phi` to keep
:math:`\widetilde{\mathbf{y}}` a column vector. For the entire data
matrix, we have the **response matrix**

+-----------------------+-----------------------+-----------------------+
| .. math:: \widehat{\m |                       | (2‑4)                 |
| athbf{Y}} = \mathbf{W |                       |                       |
| }^{T}\phi\left( \math |                       |                       |
| bf{X} \right)\mathbf{ |                       |                       |
| =}\mathbf{W}^{T}\math |                       |                       |
| bf{\Phi}              |                       |                       |
+-----------------------+-----------------------+-----------------------+

and the difference between the responses and the observations
:math:`\mathbf{Ε} = \mathbf{Y} - \widehat{\mathbf{Y}}` is an **error**
**matrix**, and some distance by
:math:`d\left( \mathbf{Y},\widetilde{\mathbf{Y}} \right)`, is the
**error**, where a typical error is the **squared error**
:math:`d\left( \mathbf{Y},\widetilde{\mathbf{Y}} \right) = \left\| \mathbf{Y} - \widetilde{\mathbf{Y}} \right\|_{F}^{2}`,
i.e. the Frobenius norm of the error matrix. *In practice*, very often
every :math:`\mathbf{w}_{i}` is independently inferred; in this case, we
can solve for
:math:`{\widetilde{\mathbf{y}}}_{i}\mathbf{=}\phi^{T}\left( \mathbf{x}_{i} \right)\mathbf{w}_{i}`
for each :math:`i = 1,\ldots,l`. This section is therefore focuses on
(2‑2) and (2‑3).

LSE for Regression
''''''''''''''''''

-  The **least squared error** (LSE) approach minimizes the squared
   error :math:`\left\| \mathbf{\epsilon} \right\|_{2}^{2}`. This is the
   most basic and intuitive way to infer :math:`\mathbf{w}`. It takes
   advantage of facts from linear algebra.

   **Fact** **2‑1** The **Moore Penrose inverse** of a real-valued
   matrix :math:`\mathbf{A}` is defined as
   :math:`\mathbf{A}^{\dagger} = \mathbf{V}\mathbf{\Sigma}^{- 1}\mathbf{U}^{T}`
   where :math:`\mathbf{A = U\Sigma}\mathbf{V}^{T}` is the **singular
   value decomposition** (SVD) of :math:`\mathbf{A}`. We have
   :math:`\mathbf{A}^{\dagger} = \left( \mathbf{A}^{T}\mathbf{A} \right)^{\dagger}\mathbf{A}`.

   **Fact** **2‑2** Given a linear system
   :math:`\mathbf{\text{Ax}} = \mathbf{y}`, the best solution
   :math:`\mathbf{x}^{\mathbf{*}}` in terms of minimum squared error
   :math:`\left\| \mathbf{y} - \mathbf{A}\widetilde{\mathbf{x}} \right\|_{2}^{2}`
   is :math:`\mathbf{x}^{\mathbf{*}} = \mathbf{A}^{\dagger}\mathbf{y}`.

   **Fact** **2‑3** :math:`\mathbf{\text{AA}}^{\dagger}\mathbf{y}` is
   projecting :math:`\mathbf{y}` onto the column space of
   :math:`\mathbf{A}`, denoted by :math:`C\left( \mathbf{A} \right)`,
   where we note the projection is in the standard “common-sense” way
   w.r.t. the standard basis and Euclidean distance. Given a linear
   system :math:`\mathbf{\text{Ax}} = \mathbf{y}` where
   :math:`\mathbf{y} \in C\left( \mathbf{A} \right)`, the solution is
   exactly :math:`\mathbf{x} = \mathbf{A}^{\dagger}\mathbf{y}`. This is
   because :math:`\mathbf{A}\mathbf{A}^{\dagger}\mathbf{y}` projects
   :math:`\mathbf{y}` to the space where it already resides, and thus
   :math:`\mathbf{A}\mathbf{A}^{\dagger}\mathbf{y = y}`, and then
   :math:`\mathbf{\text{Ax}} = \mathbf{A}\mathbf{A}^{\dagger}\mathbf{y = y}`
   holds exactly.

   **Fact** **2‑4**
   :math:`C\left( \mathbf{A} \right) = C\left( \mathbf{A}\mathbf{A}^{T} \right)`
   for any real-valued matrix :math:`\mathbf{A}`. This can be easily
   proved by SVD of :math:`\mathbf{A}`, or can also be proved by showing
   the null space of :math:`\mathbf{A}` is identical to the null space
   of :math:`\mathbf{A}\mathbf{A}^{T}`.

   **Theorem** **2‑1**\ **LSE for Regression**. The best regression for
   :math:`\mathbf{y} = \mathbf{\Phi}^{T}\mathbf{w}` in terms of LSE is
   defined by
   :math:`\mathbf{w}^{\mathbf{*}} = \left( \mathbf{\Phi}^{T} \right)^{\dagger}\mathbf{y}`,
   which immediately follows from *Fact 2‑2*. The other method to derive
   the same result is by taking derivative.

+-----------------------+-----------------------+-----------------------+
| .. math:: \left\| \ma |                       | (2‑5)                 |
| thbf{\epsilon} \right |                       |                       |
| \|_{2}^{2} = \sum_{i  |                       |                       |
| = 1}^{N}\left( y_{i}  |                       |                       |
| - {\widetilde{y}}_{i} |                       |                       |
|  \right)^{2} = \sum_{ |                       |                       |
| i = 1}^{N}\left( y_{i |                       |                       |
| } - \phi^{T}\left( \m |                       |                       |
| athbf{x}_{i} \right)\ |                       |                       |
| mathbf{w} \right)^{2} |                       |                       |
+-----------------------+-----------------------+-----------------------+

Then by **chain rule** and *Fact 1‑1*,

+-----------------------------------+-----------------------------------+
| .. math:: \nabla_{\mathbf{w}}\lef | (2‑6)                             |
| t\| \mathbf{\epsilon} \right\|_{2 |                                   |
| }^{2} = \sum_{i = 1}^{N}{2\left(  |                                   |
| y_{i} - \phi^{T}\left( \mathbf{x} |                                   |
| _{i} \right)\mathbf{w} \right)\le |                                   |
| ft( - \phi\left( \mathbf{x}_{i} \ |                                   |
| right) \right)} = 2\left( \sum_{i |                                   |
|  = 1}^{N}{\phi^{T}\left( \mathbf{ |                                   |
| x}_{i} \right)\mathbf{w}\phi_{j}\ |                                   |
| left( \mathbf{x}_{i} \right)} - \ |                                   |
| sum_{i = 1}^{N}{y_{i}\phi\left( \ |                                   |
| mathbf{x}_{i} \right)} \right)\ma |                                   |
| thbf{\Rightarrow}\nabla_{\mathbf{ |                                   |
| w}}\left\| \mathbf{\epsilon} \rig |                                   |
| ht\|_{2}^{2} = 2\left( \phi\left( |                                   |
|  \mathbf{X} \right)\phi^{T}\left( |                                   |
|  \mathbf{X} \right)\mathbf{w -}\p |                                   |
| hi\left( \mathbf{X} \right)\mathb |                                   |
| f{y} \right) = 0 \Rightarrow \nab |                                   |
| la_{\mathbf{w}}\left\| \mathbf{\e |                                   |
| psilon} \right\|_{2}^{2} = 2\left |                                   |
| ( \mathbf{\Phi}\mathbf{\Phi}^{T}\ |                                   |
| mathbf{w - \Phi y} \right) = 0 \R |                                   |
| ightarrow \mathbf{\text{Φy}} = \m |                                   |
| athbf{\Phi}\mathbf{\Phi}^{T}\math |                                   |
| bf{w}                             |                                   |
+-----------------------------------+-----------------------------------+

..

   We can also directly use

.. math:: \left\| \mathbf{\epsilon} \right\|_{2}^{2} = \mathbf{\epsilon}^{T}\mathbf{\epsilon =}\left( \mathbf{y}\mathbf{-}\mathbf{\Phi}^{T}\mathbf{w} \right)^{T}\left( \mathbf{y}\mathbf{-}\mathbf{\Phi}^{T}\mathbf{w} \right)\mathbf{\propto}\mathbf{w}^{T}\mathbf{\Phi}\mathbf{\Phi}^{T}\mathbf{w -}\mathbf{y}^{T}\mathbf{\Phi}^{T}\mathbf{w -}\mathbf{w}^{T}\mathbf{\text{Φy}}

   Then apply *Fact 1‑1* and *Fact 1‑4*, and have the following which is
   the same as (2‑6)

.. math:: \nabla_{\mathbf{w}}\left\| \mathbf{\epsilon} \right\|_{2}^{2} = 2\mathbf{\Phi}\mathbf{\Phi}^{T}\mathbf{w}\mathbf{- 2}\mathbf{\Phi y = 0}\mathbf{\Rightarrow}\mathbf{\text{Φy}} = \mathbf{\Phi}\mathbf{\Phi}^{T}\mathbf{w}

   Note :math:`\mathbf{\text{Φy}} \in C\left( \mathbf{\Phi} \right)`,
   and
   :math:`C\left( \mathbf{\Phi} \right) = C\left( \mathbf{\Phi}\mathbf{\Phi}^{T} \right) \Rightarrow \mathbf{\Phi y \in}C\left( \mathbf{\Phi}\mathbf{\Phi}^{T} \right)`
   by *Fact 2‑4*, which gives the following by first applying *Fact 2‑3*
   and then *Fact 2‑1*,

+-----------------------+-----------------------+-----------------------+
| .. math:: \mathbf{w}^ |                       | (2‑7)                 |
| {\mathbf{*}}\mathbf{= |                       |                       |
| }\left( \mathbf{\Phi} |                       |                       |
| \mathbf{\Phi}^{T} \ri |                       |                       |
| ght)^{\dagger}\mathbf |                       |                       |
| {\text{Φy}} = \left(  |                       |                       |
| \mathbf{\Phi}^{T} \ri |                       |                       |
| ght)^{\dagger}\mathbf |                       |                       |
| {y =}\left( \mathbf{\ |                       |                       |
| Phi}^{\dagger} \right |                       |                       |
| )^{T}\mathbf{y}       |                       |                       |
+-----------------------+-----------------------+-----------------------+

..

   And the regression function is thus

.. math:: f\left( \mathbf{x} \right) = \phi^{T}\left( \mathbf{x} \right)\mathbf{w}^{*}

   **The special case** when
   :math:`\phi\left( \mathbf{x} \right) = \begin{pmatrix}
   1 \\
   \mathbf{x} \\
   \end{pmatrix}`, :math:`\mathbf{\Phi} ≔ \begin{pmatrix}
   \mathbf{1}^{T} \\
   \mathbf{X} \\
   \end{pmatrix}` and :math:`\mathbf{w} ≔ \begin{pmatrix}
   w_{0} \\
   \mathbf{w} \\
   \end{pmatrix}`, we have

+-----------------------+-----------------------+-----------------------+
| .. math::             |                       | (2‑8)                 |
|                       |                       |                       |
|    \left\| \mathbf{\e |                       |                       |
| psilon} \right\|_{2}^ |                       |                       |
| {2} = \sum_{i = 1}^{N |                       |                       |
| }\left( y_{i} - {\wid |                       |                       |
| etilde{y}}_{i} \right |                       |                       |
| )^{2} = \sum_{i = 1}^ |                       |                       |
| {N}\left( y_{i} - \le |                       |                       |
| ft( 1\mathbf{,}\mathb |                       |                       |
| f{x}_{i}^{T} \right)\ |                       |                       |
| begin{pmatrix}        |                       |                       |
|    w_{0} \\           |                       |                       |
|    \mathbf{w} \\      |                       |                       |
|    \end{pmatrix} \rig |                       |                       |
| ht)^{2} = \sum_{i = 1 |                       |                       |
| }^{N}\left( y_{i} - w |                       |                       |
| _{0}\mathbf{-}\mathbf |                       |                       |
| {x}_{i}^{T}\mathbf{w} |                       |                       |
|  \right)^{2} \Rightar |                       |                       |
| row \nabla_{w_{0}}\le |                       |                       |
| ft\| \mathbf{\epsilon |                       |                       |
| } \right\|_{2}^{2} =  |                       |                       |
| - 2\sum_{i = 1}^{N}\l |                       |                       |
| eft( y_{i} - w_{0}\ma |                       |                       |
| thbf{-}\mathbf{x}_{i} |                       |                       |
| ^{T}\mathbf{w} \right |                       |                       |
| ) = 0 \Rightarrow \su |                       |                       |
| m_{i = 1}^{N}y_{i} =  |                       |                       |
| \sum_{i = 1}^{N}w_{0} |                       |                       |
|  + \sum_{i = 1}^{N}{\ |                       |                       |
| mathbf{x}_{i}^{T}\mat |                       |                       |
| hbf{w}} \Rightarrow \ |                       |                       |
| mathbf{1}^{T}\mathbf{ |                       |                       |
| y}\mathbf{=}Nw_{0} +  |                       |                       |
| \mathbf{1}^{T}\mathbf |                       |                       |
| {X}^{T}\mathbf{w}     |                       |                       |
+-----------------------+-----------------------+-----------------------+

..

   Or we can also achieve this by

.. math::

   \begin{pmatrix}
   \mathbf{1}^{T} \\
   \mathbf{X} \\
   \end{pmatrix}\mathbf{y} = \begin{pmatrix}
   \mathbf{1}^{T} \\
   \mathbf{X} \\
   \end{pmatrix}\left( \mathbf{1,}\mathbf{X}^{T} \right)\begin{pmatrix}
   w_{0} \\
   \mathbf{w} \\
   \end{pmatrix} \Rightarrow \begin{pmatrix}
   \mathbf{1}^{T}\mathbf{y} \\
   \mathbf{\text{Xy}} \\
   \end{pmatrix} = \begin{pmatrix}
   \mathbf{1}^{T}\mathbf{1} & \mathbf{1}^{T}\mathbf{X}^{T} \\
   \mathbf{X}\mathbf{1} & \mathbf{X}\mathbf{X}^{T} \\
   \end{pmatrix}\begin{pmatrix}
   w_{0} \\
   \mathbf{w} \\
   \end{pmatrix} = \begin{pmatrix}
   Nw_{0} + \mathbf{1}^{T}\mathbf{X}^{T}\mathbf{w} \\
   w_{0}\mathbf{X}\mathbf{1 +}\mathbf{X}\mathbf{X}^{T}\mathbf{w} \\
   \end{pmatrix} \Rightarrow \left\{ \begin{matrix}
   \mathbf{1}^{T}\mathbf{y}\mathbf{=}Nw_{0} + \mathbf{1}^{T}\mathbf{X}^{T}\mathbf{w} \\
   \mathbf{Xy =}w_{0}\mathbf{X}\mathbf{1 +}\mathbf{X}\mathbf{X}^{T}\mathbf{w} \\
   \end{matrix} \right.\

   Let
   :math:`\overline{y} = \frac{\mathbf{1}^{T}\mathbf{y}}{N},\overline{\mathbf{x}} = \frac{\mathbf{X}\mathbf{1}}{N}`,
   then we have

+-----------------------+-----------------------+-----------------------+
| .. math:: \overline{y |                       | (2‑9)                 |
| }\mathbf{=}w_{0} + {\ |                       |                       |
| overline{\mathbf{x}}} |                       |                       |
| ^{T}\mathbf{w \Righta |                       |                       |
| rrow}w_{0}^{*} = \ove |                       |                       |
| rline{y} - {\overline |                       |                       |
| {\mathbf{x}}}^{T}\mat |                       |                       |
| hbf{w}                |                       |                       |
+-----------------------+-----------------------+-----------------------+

..

   For :math:`\mathbf{w}^{\mathbf{*}}`, instead of plugging
   :math:`w_{0}^{*}` of (2‑9) into the second equation
   “\ :math:`\mathbf{Xy =}w_{0}\mathbf{X}\mathbf{1 +}\mathbf{X}\mathbf{X}^{T}\mathbf{w}`\ ”,
   a trick is used to simplify the calculation. Let
   :math:`\overline{\mathbf{y}} = \overline{y}\mathbf{1}` and
   :math:`\mathbf{1}{\overline{\mathbf{x}}}^{T}\mathbf{=}{\overline{\mathbf{X}}}^{T}`
   where every column of :math:`\overline{\mathbf{X}}` is the mean
   :math:`\overline{\mathbf{x}}`, then

+-----------------------+-----------------------+-----------------------+
| .. math::             |                       | (2‑10)                |
|                       |                       |                       |
|    \mathbf{y}\mathbf{ |                       |                       |
| -}\mathbf{\Phi}^{T}\m |                       |                       |
| athbf{w = y}\mathbf{- |                       |                       |
| }\left( \mathbf{1,}\m |                       |                       |
| athbf{X}^{T} \right)\ |                       |                       |
| begin{pmatrix}        |                       |                       |
|    w_{0} \\           |                       |                       |
|    \mathbf{w} \\      |                       |                       |
|    \end{pmatrix}\math |                       |                       |
| bf{=}\mathbf{y} - w_{ |                       |                       |
| 0}\mathbf{1} - \mathb |                       |                       |
| f{X}^{T}\mathbf{w}\ma |                       |                       |
| thbf{=}\mathbf{y} - \ |                       |                       |
| left( \overline{y} -  |                       |                       |
| {\overline{\mathbf{x} |                       |                       |
| }}^{T}\mathbf{w} \rig |                       |                       |
| ht)\mathbf{1} - \math |                       |                       |
| bf{X}^{T}\mathbf{w}\m |                       |                       |
| athbf{= y} - \overlin |                       |                       |
| e{\mathbf{y}} + \math |                       |                       |
| bf{1}{\overline{\math |                       |                       |
| bf{x}}}^{T}\mathbf{w} |                       |                       |
|  - \mathbf{X}^{T}\mat |                       |                       |
| hbf{w}\mathbf{=}\left |                       |                       |
| ( \mathbf{y} - \overl |                       |                       |
| ine{\mathbf{y}} \righ |                       |                       |
| t)\mathbf{-}\left( \m |                       |                       |
| athbf{X}\mathbf{-}\ov |                       |                       |
| erline{\mathbf{X}} \r |                       |                       |
| ight)^{T}\mathbf{w}   |                       |                       |
+-----------------------+-----------------------+-----------------------+

..

   Let
   :math:`\widetilde{\mathbf{X}} = \mathbf{X}\mathbf{-}\overline{\mathbf{X}}`
   and
   :math:`\widetilde{\mathbf{y}} = \mathbf{y} - \overline{\mathbf{y}}`,
   then
   :math:`\left( \widetilde{\mathbf{X}},\widetilde{\mathbf{y}} \right)`
   is called **centralized data**, and by above (2‑10) and then (2‑6) we
   have

+-----------------------+-----------------------+-----------------------+
| .. math:: \mathbf{y}\ |                       | (2‑11)                |
| mathbf{-}\mathbf{\Phi |                       |                       |
| }^{T}\mathbf{w =}\wid |                       |                       |
| etilde{\mathbf{y}}\ma |                       |                       |
| thbf{-}{\widetilde{\m |                       |                       |
| athbf{X}}}^{T}\mathbf |                       |                       |
| {w}\mathbf{\Rightarro |                       |                       |
| w}\left\| \mathbf{\ep |                       |                       |
| silon} \right\|_{2}^{ |                       |                       |
| 2} = \left( \widetild |                       |                       |
| e{\mathbf{y}}\mathbf{ |                       |                       |
| -}{\widetilde{\mathbf |                       |                       |
| {X}}}^{T}\mathbf{w} \ |                       |                       |
| right)^{T}\left( \wid |                       |                       |
| etilde{\mathbf{y}}\ma |                       |                       |
| thbf{-}{\widetilde{\m |                       |                       |
| athbf{X}}}^{T}\mathbf |                       |                       |
| {w} \right)\mathbf{\R |                       |                       |
| ightarrow}\nabla_{\ma |                       |                       |
| thbf{w}}\left\| \math |                       |                       |
| bf{\epsilon} \right\| |                       |                       |
| _{2}^{2} = 2\left( \w |                       |                       |
| idetilde{\mathbf{X}}{ |                       |                       |
| \widetilde{\mathbf{X} |                       |                       |
| }}^{T}\mathbf{w -}\wi |                       |                       |
| detilde{\mathbf{X}}\w |                       |                       |
| idetilde{\mathbf{y}}  |                       |                       |
| \right) = 0\mathbf{\R |                       |                       |
| ightarrow}\mathbf{w}^ |                       |                       |
| {\mathbf{*}}\mathbf{= |                       |                       |
| }\left( \widetilde{\m |                       |                       |
| athbf{X}}{\widetilde{ |                       |                       |
| \mathbf{X}}}^{T} \rig |                       |                       |
| ht)^{\dagger}\widetil |                       |                       |
| de{\mathbf{X}}\mathbf |                       |                       |
| {y =}\left. （\operato |                      |                       |
| rname{cov}\mathbf{X}  |                       |                       |
| \right.）^{\dagger}\wi |                      |                       |
| detilde{\mathbf{X}}\m |                       |                       |
| athbf{y}\mathbf{=}\le |                       |                       |
| ft( {\widetilde{\math |                       |                       |
| bf{X}}}^{T} \right)^{ |                       |                       |
| \dagger}\widetilde{\m |                       |                       |
| athbf{y}}\mathbf{=}\l |                       |                       |
| eft( {\widetilde{\mat |                       |                       |
| hbf{X}}}^{\dagger} \r |                       |                       |
| ight)^{T}\widetilde{\ |                       |                       |
| mathbf{y}}\mathbf{\Ri |                       |                       |
| ghtarrow}w_{0}^{*} =  |                       |                       |
| \overline{y} - {\over |                       |                       |
| line{\mathbf{x}}}^{T} |                       |                       |
| \mathbf{w}^{\mathbf{* |                       |                       |
| }}                    |                       |                       |
+-----------------------+-----------------------+-----------------------+

..

   where we *note*
   :math:`\widetilde{\mathbf{X}}{\widetilde{\mathbf{X}}}^{T}\mathbf{=}\operatorname{cov}\mathbf{X}`
   is the covariance matrix of :math:`\mathbf{X}`. We then have the
   regression function as

.. math:: g\left( \mathbf{x} \right) = \mathbf{x}^{T}\mathbf{w}^{*} + w_{0}^{*} = {\widetilde{\mathbf{x}}}^{T}\mathbf{w}^{*} + \overline{y}

   We should note two key *limitations* of LSE. 1) it is not robust and
   high sensitive to outliers, which can be seem from its objective: the
   error is squared, and if there are outliers that introduce large
   squares, then :math:`\mathbf{w}^{*}` will be skewed to reduce the
   impact of the outlier; 2) overfitting when data is insufficient
   represent all possible case, then the inferred :math:`\mathbf{w}^{*}`
   could have small error for known data, but poor for prediction. Both
   problems are mediated by regularization or assign priors that we show
   later.

Gaussian ML
'''''''''''

-  **Theorem** **2‑2**\ **Gaussian ML**. Under Gaussian maximum
   likelihood, the variance is the mean LSE, and optimal weights is the
   same as that inferred from LSE.

   This approach is a simple generalization of least square, it keeps
   the result :math:`\mathbf{w}^{\mathbf{*}}` as in (2‑7) (so it keeps
   the same limitations of LSE), and in addition finds “error variance”
   under Gaussian assumption. It *assumes* each observation
   :math:`y_{i},i = 1,\ldots,N` is the
   response \ :math:`{\widetilde{y}}_{i}` plus i.i.d. independent
   Gaussian random error :math:`\epsilon_{i}` with zero mean and some
   variance :math:`\sigma^{2}`, i.e.
   :math:`y_{i}\sim\text{Gaussian}\left( {\widetilde{y}}_{i},\sigma^{2} \right)`
   or
   :math:`\epsilon_{i} = y_{i} - {\widetilde{y}}_{i}\sim\text{Gaussian}\left( 0,\sigma^{2} \right)`.
   Note two assumptions are made here, 1) one point of this assumption
   is the error is unimodal; 2) errors for each observation are i.i.d.
   The probability of observing :math:`\mathbf{y}` is therefore

.. math:: p\left( \mathbf{y} \right) = \prod_{i = 1}^{N}{\frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\left\{ - \frac{\left( y_{i} - {\widetilde{y}}_{i} \right)^{2}}{2\sigma^{2}} \right\}}

And the goal is to maximize :math:`p`. Taking logarithm, we have

.. math:: \ln p = \sum_{i = 1}^{N}{\ln\frac{1}{\sqrt{2\pi\sigma^{2}}} - \frac{\left( y_{i} - {\widetilde{y}}_{i} \right)^{2}}{2\sigma^{2}}} = - \frac{N}{2}\ln{2\pi\sigma^{2}} - \frac{1}{2\sigma^{2}}\sum_{i = 1}^{N}\left( y_{i} - {\widetilde{y}}_{i} \right)^{2} \propto \frac{N}{2}\ln\frac{1}{\sigma^{2}} - \frac{1}{2\sigma^{2}}\left\| \mathbf{\epsilon} \right\|_{2}^{2}

For convenience, denote :math:`\beta = \sigma^{- 2}` where :math:`\beta`
is the precision, and
:math:`\ln p \propto N\ln\beta - \beta\left\| \mathbf{\epsilon} \right\|_{2}^{2}`.
Note only :math:`\mathbf{\epsilon}` is dependent on :math:`\mathbf{w}`,
thus finding optimal :math:`\mathbf{w}^{*}` is exactly the same as
solving least squared error; from the proof of *Theorem 2‑1*, we have
:math:`\mathbf{w}^{\mathbf{*}}\mathbf{=}\left( \mathbf{\Phi}^{T} \right)^{\dagger}\mathbf{y}`.
Denote :math:`\mathbf{\epsilon}^{*}` as the optimal error vector, then

+-----------------------+-----------------------+-----------------------+
| .. math:: \frac{\part |                       | (2‑12)                |
| ial\ln p}{\partial\be |                       |                       |
| ta} = \frac{N}{\beta} |                       |                       |
|  - \left\| \mathbf{\e |                       |                       |
| psilon}^{\mathbf{*}}  |                       |                       |
| \right\|_{2}^{2} = 0  |                       |                       |
| \Rightarrow \beta^{*} |                       |                       |
|  = \frac{N}{\left\| \ |                       |                       |
| mathbf{\epsilon}^{\ma |                       |                       |
| thbf{*}} \right\|_{2} |                       |                       |
| ^{2}} = \frac{N}{\lef |                       |                       |
| t\| \mathbf{y -}\math |                       |                       |
| bf{\Phi}^{T}\left( \m |                       |                       |
| athbf{\Phi}^{T} \righ |                       |                       |
| t)^{\dagger}\mathbf{y |                       |                       |
| } \right\|_{2}^{2}} \ |                       |                       |
| Rightarrow {{(\sigma} |                       |                       |
| ^{2})}^{*} = \frac{\l |                       |                       |
| eft\| \mathbf{\epsilo |                       |                       |
| n}^{\mathbf{*}} \righ |                       |                       |
| t\|_{2}^{2}}{N} = \fr |                       |                       |
| ac{\left\| \mathbf{y  |                       |                       |
| -}\mathbf{\Phi}^{T}\l |                       |                       |
| eft( \mathbf{\Phi}^{T |                       |                       |
| } \right)^{\dagger}\m |                       |                       |
| athbf{y} \right\|_{2} |                       |                       |
| ^{2}}{N}              |                       |                       |
+-----------------------+-----------------------+-----------------------+

That is, the optimal variance is the **mean squared error** (MSE).

Regularization
''''''''''''''

-  **Regularization**. To prevent the well-known **overfitting** problem
   that occurs when the data size is not sufficient to well support the
   regression model, one classic approach is to add **regularization
   function** :math:`r\left( \mathbf{w} \right)` to the regression
   objective like (2‑5). A typical regularization is
   :math:`r\left( \mathbf{w} \right) = \gamma\left\| \mathbf{w} \right\|_{p}^{p}`,
   where :math:`\gamma` is a scalar controlling the strength of
   regularization, and :math:`\left\| \cdot \right\|` is the
   :math:`p`-**norm** of :math:`\mathbf{w}`, e.g. in the case of
   :math:`r\left( \mathbf{w} \right) = \gamma\left\| \mathbf{w} \right\|_{2}^{2} = \gamma\mathbf{w}^{T}\mathbf{w}`,
   (2‑5) becomes the following objective,

+-----------------------+-----------------------+-----------------------+
| .. math:: \operatorna |                       | (2‑13)                |
| me{}L = \operatorname |                       |                       |
| {}\left( \frac{1}{2}\ |                       |                       |
| left\| \mathbf{\epsil |                       |                       |
| on} \right\|_{2}^{2}  |                       |                       |
| + \gamma\mathbf{w}^{T |                       |                       |
| }\mathbf{w} \right)   |                       |                       |
+-----------------------+-----------------------+-----------------------+

Applying (*2*\ ‑\ *6*) to the squared error, and gradient rule *Fact
1‑4* to the regularization, and note
:math:`\mathbf{\Phi}\mathbf{\Phi}^{T}\mathbf{+}\gamma\mathbf{I}` must be
invertible because it is positive definite (a sum of a positive
semidefinite matrix :math:`\mathbf{\Phi}\mathbf{\Phi}^{T}` and a
positive definite matrix :math:`\mathbf{I}`), then we have

.. math:: \nabla_{\mathbf{w}}L \propto \mathbf{\Phi}\mathbf{\Phi}^{T}\mathbf{w - \Phi y} + \gamma\mathbf{w} = \mathbf{0}\mathbf{\Rightarrow \Phi y} = \left( \mathbf{\Phi}\mathbf{\Phi}^{T}\mathbf{+}\gamma\mathbf{I} \right)\mathbf{w}\mathbf{\Rightarrow}\mathbf{w}^{*}\mathbf{=}\left( \mathbf{\Phi}\mathbf{\Phi}^{T}\mathbf{+}\gamma\mathbf{I} \right)^{- 1}\mathbf{\text{Φy}}

*Note* :math:`\gamma` has to be pre-defined, and there is no way to
optimize it in terms of (*2*\ ‑\ *13*) or other :math:`p`-norm, simply
because
:math:`\frac{\partial L}{\partial\gamma} = \mathbf{w}^{T}\mathbf{w} \neq 0`
unless :math:`\mathbf{w = 0}`.

Gaussian MAP
''''''''''''

-  **Theorem** **2‑3**\ **Gaussian MAP**. Based on above, we can further
   assume a prior distribution over :math:`\mathbf{w}` and use maximum a
   posteriori technique. Based on the Gaussian ML in *Theorem 2‑2*,
   besides
   :math:`y_{i}|\mathbf{w}\sim\text{Gaussian}\left( \phi^{T}\left( \mathbf{x}_{i} \right)\mathbf{w},\sigma^{2} \right)`
   and :math:`\beta = \sigma^{- 2}` is the precision, we assume a
   Gaussian prior over the weight
   :math:`\mathbf{w}\sim\operatorname{Gaussian}\left( \mathbf{w}|\mathbf{\mu}_{\mathbf{w}},\mathbf{\Sigma}_{\mathbf{w}} \right)`
   with pre-defined parameters
   :math:`\mathbf{\mu}_{\mathbf{w}},\mathbf{\Sigma}_{\mathbf{w}}`, then

.. math:: L_{\mathbf{w}|\mathbf{y}} \propto \ln{p\left( \mathbf{y}|\mathbf{w} \right)} + \ln{p\left( \mathbf{w} \right)} = \sum_{i = 1}^{N}{\ln{p\left( y_{i}|\mathbf{w} \right)}} + \ln{p\left( \mathbf{w} \right)} \propto \frac{N}{2}\ln\frac{1}{\sigma^{2}} - \frac{1}{2\sigma^{2}}\left( \sum_{i = 1}^{N}\left( y_{i} - \phi^{T}\left( \mathbf{x}_{i} \right)\mathbf{w} \right)^{2} \right) - \frac{1}{2}\left( \mathbf{w -}\mathbf{\mu}_{\mathbf{w}} \right)^{T}\mathbf{\Sigma}_{\mathbf{w}}^{- 1}\left( \mathbf{w -}\mathbf{\mu}_{\mathbf{w}} \right)\mathbf{=}\frac{N}{2}\ln\frac{1}{\sigma^{2}} - \frac{1}{2\sigma^{2}}\left\| \mathbf{\epsilon} \right\|_{2}^{2} - \frac{1}{2}\left( \mathbf{w -}\mathbf{\mu}_{\mathbf{w}} \right)^{T}\mathbf{\Sigma}_{\mathbf{w}}^{- 1}\left( \mathbf{w -}\mathbf{\mu}_{\mathbf{w}} \right)

where the choice of best :math:`\sigma^{2}` is still the MSE based on
(2‑12). The choice of :math:`\mathbf{w}` uses objective

.. math:: \operatorname{}L = \operatorname{}\left( \frac{1}{2\sigma^{2}}\left\| \mathbf{\epsilon} \right\|_{2}^{2} + \frac{1}{2}\left( \mathbf{w}^{T}\mathbf{\Sigma}_{\mathbf{w}}^{- 1}\mathbf{w -}2\mathbf{\mu}_{\mathbf{w}}^{T}\mathbf{\Sigma}_{\mathbf{w}}^{- 1}\mathbf{w} \right) \right)

where we note :math:`\mathbf{\Sigma}_{\mathbf{w}}^{- 1}` is symmetric,
and apply (2‑6), gradient rule *Fact 1‑1*, *Fact 1‑4*, we have

+-----------------------------------+-----------------------------------+
| .. math:: \nabla_{\mathbf{w}}L \p | (2‑14)                            |
| ropto \frac{1}{\sigma^{2}}\left(  |                                   |
| \mathbf{\Phi}\mathbf{\Phi}^{T}\ma |                                   |
| thbf{w - \Phi y} \right) + \frac{ |                                   |
| 1}{2}\left( \mathbf{\Sigma}_{\mat |                                   |
| hbf{w}}^{- 1} + \mathbf{\Sigma}_{ |                                   |
| \mathbf{w}}^{- T} \right)\mathbf{ |                                   |
| w} - \mathbf{\Sigma}_{\mathbf{w}} |                                   |
| ^{T}\mathbf{\mu}_{\mathbf{w}} = \ |                                   |
| frac{1}{\sigma^{2}}\left( \mathbf |                                   |
| {\Phi}\mathbf{\Phi}^{T}\mathbf{w  |                                   |
| - \Phi y} \right) + \mathbf{\Sigm |                                   |
| a}_{\mathbf{w}}^{- 1}\mathbf{w} - |                                   |
|  \mathbf{\Sigma}_{\mathbf{w}}^{-  |                                   |
| 1}\mathbf{\mu}_{\mathbf{w}}\mathb |                                   |
| f{= 0}\mathbf{\Rightarrow}\frac{1 |                                   |
| }{\sigma^{2}}\mathbf{\Phi y +}\ma |                                   |
| thbf{\Sigma}_{\mathbf{w}}^{- 1}\m |                                   |
| athbf{\mu}_{\mathbf{w}}\mathbf{=} |                                   |
| \frac{1}{\sigma^{2}}\mathbf{\Phi} |                                   |
| \mathbf{\Phi}^{T}\mathbf{w +}\mat |                                   |
| hbf{\Sigma}_{\mathbf{w}}^{- 1}\ma |                                   |
| thbf{w}\mathbf{\Rightarrow}\mathb |                                   |
| f{w}^{\mathbf{*}}\mathbf{=}\left( |                                   |
|  \mathbf{\Phi}\mathbf{\Phi}^{T}\m |                                   |
| athbf{+}\sigma^{2}\mathbf{\Sigma} |                                   |
| _{\mathbf{w}}^{- 1} \right)^{- 1} |                                   |
| \left( \mathbf{\Phi y +}\sigma^{2 |                                   |
| }\mathbf{\Sigma}_{\mathbf{w}}^{-  |                                   |
| 1}\mathbf{\mu}_{\mathbf{w}} \righ |                                   |
| t)\mathbf{=}\left( \beta\mathbf{\ |                                   |
| Phi}\mathbf{\Phi}^{T}\mathbf{+}\m |                                   |
| athbf{\Sigma}_{\mathbf{w}}^{- 1}  |                                   |
| \right)^{- 1}\left( \beta\mathbf{ |                                   |
| \Phi y +}\mathbf{\Sigma}_{\mathbf |                                   |
| {w}}^{- 1}\mathbf{\mu}_{\mathbf{w |                                   |
| }} \right)                        |                                   |
+-----------------------------------+-----------------------------------+

MAP (and the Bayesian inference to be discussed) is smoothing the
parameter inference and hence making up for overfitting and outliers, as
we can see from above that it supplements the distribution of
:math:`\mathbf{w}`, which provides predefined information for
:math:`\mathbf{w}` especially for the case when the data is
insufficient.

   **Example** **2‑1**. Assume
   :math:`\mathbf{w}\sim\operatorname{Gaussian}\left( \mathbf{w}|\mathbf{0},\varsigma^{2}\mathbf{I} \right)`
   and
   :math:`y_{i}|\mathbf{w}\sim Gaussian\left( \phi^{T}\left( \mathbf{x}_{i} \right)\mathbf{w},\sigma^{2} \right)`,
   and let :math:`\alpha = \varsigma^{- 2},\beta = \sigma^{- 2}` be the
   precisions, then the MAP objective for :math:`\mathbf{y}|\mathbf{w}`
   is :math:`\operatorname{}L_{\mathbf{w}|\mathbf{y}}` where

+-----------------------+-----------------------+-----------------------+
| .. math:: L_{\mathbf{ |                       | (2‑15)                |
| w}|\mathbf{y}} \propt |                       |                       |
| o \ln{p\left( \mathbf |                       |                       |
| {y}|\mathbf{w} \right |                       |                       |
| )} + \ln{p\left( \mat |                       |                       |
| hbf{w} \right)} \prop |                       |                       |
| to - \frac{1}{2\sigma |                       |                       |
| ^{2}}\left\| \mathbf{ |                       |                       |
| \epsilon} \right\|_{2 |                       |                       |
| }^{2} - \frac{1}{2\va |                       |                       |
| rsigma^{2}}\mathbf{w} |                       |                       |
| ^{T}\mathbf{w}\mathbf |                       |                       |
| {\Rightarrow}\nabla_{ |                       |                       |
| \mathbf{w}}L \propto  |                       |                       |
| \frac{1}{\sigma^{2}}\ |                       |                       |
| left( \mathbf{\Phi}\m |                       |                       |
| athbf{\Phi}^{T}\mathb |                       |                       |
| f{w - \Phi y} \right) |                       |                       |
|  + \frac{1}{\varsigma |                       |                       |
| ^{2}}\mathbf{w}\mathb |                       |                       |
| f{=}0\mathbf{\Rightar |                       |                       |
| row}\mathbf{w}^{\math |                       |                       |
| bf{*}}\mathbf{=}\left |                       |                       |
| ( \mathbf{\Phi}\mathb |                       |                       |
| f{\Phi}^{T}\mathbf{+} |                       |                       |
| \frac{\sigma^{2}}{\va |                       |                       |
| rsigma^{2}}\mathbf{I} |                       |                       |
|  \right)^{- 1}\mathbf |                       |                       |
| {\Phi y =}\left( \mat |                       |                       |
| hbf{\Phi}\mathbf{\Phi |                       |                       |
| }^{T}\mathbf{+}\frac{ |                       |                       |
| \alpha}{\beta}\mathbf |                       |                       |
| {I} \right)^{- 1}\mat |                       |                       |
| hbf{\text{Φy}}        |                       |                       |
+-----------------------+-----------------------+-----------------------+

which is equivalent to minimizing the regularized least square in (2‑13)
with :math:`\gamma = \frac{\sigma^{2}}{\varsigma^{2}}`. This illustrates
the idea that MAP is a generalized regularization.

In contrast to deterministic regression which finds a determined
regression function :math:`f`, a **probabilistic regression model**
constructs a conditional probability distribution
:math:`p_{\mathbf{X},\mathbf{y}}\left( \mathcal{y}\mathcal{|x} \right)`
for the property :math:`\mathcal{y}` of a future variable
:math:`\mathcal{x}`, where :math:`p_{\mathbf{X},\mathbf{y}}` is inferred
based on data :math:`\left( \mathbf{X},\mathbf{y} \right)`. Here,
:math:`p_{\mathbf{X},\mathbf{y}}\left( \mathcal{y}\mathcal{|x} \right)`
is also named the **predictive distribution**, and we use another
notation
:math:`p\left( \mathcal{y|x,}\mathbf{y} \right) ≔ p_{\mathbf{X},\mathbf{y}}\left( \mathcal{y|x} \right)`
(omits :math:`\mathbf{X}`) in future discussion for better arithmetic
consistency. At this moment, our probabilistic regression is built upon
the Bayesian inference, which assumes prior distributions for
parameters, e.g. prior distributions over :math:`\mathbf{w}` for
Gaussian MAP like in *Theorem 2‑3*.

Gaussian Bayesian regression
''''''''''''''''''''''''''''

-  **Theorem** **2‑4**\ **Gaussian Bayesian Regression**. This is the
   most basic Bayesian inference for Gaussian regression. Assume

.. math:: \mathbf{w}\sim\operatorname{Gaussian}\left( \mathbf{w}|\mathbf{\mu}_{\mathbf{w}},\mathbf{\Sigma}_{\mathbf{w}} \right)

.. math:: y_{i}|\mathbf{w}\sim\text{Gaussian}\left( \phi^{T}\left( \mathbf{x}_{i} \right)\mathbf{w},\sigma^{2} \right),\beta = \sigma^{- 2}

   where however :math:`\sigma^{2}` has to predefined. Then by *Theorem
   1‑8*, especially (1‑8) and (1‑9), we have
   :math:`\mathbf{w}|\mathbf{y}\sim\operatorname{Gaussian}\left( \mathbf{\mu}_{\mathbf{w}|\mathbf{y}}\mathbf{,}\mathbf{\Sigma}_{\mathbf{w}|\mathbf{y}} \right)`
   where

+-----------------------+-----------------------+-----------------------+
| .. math::             |                       | (2‑16)                |
|                       |                       |                       |
|    \left\{ \begin{mat |                       |                       |
| rix}                  |                       |                       |
|    \mathbf{\mu}_{\mat |                       |                       |
| hbf{w}|\mathbf{y}}\ma |                       |                       |
| thbf{=}\mathbf{\mu}_{ |                       |                       |
| \mathbf{w}}\mathbf{+} |                       |                       |
| \beta\mathbf{\Sigma}_ |                       |                       |
| {\mathbf{w}|\mathbf{y |                       |                       |
| }}\mathbf{\Phi}\left( |                       |                       |
|  \mathbf{y}\mathbf{-} |                       |                       |
| \mathbf{\Phi}^{T}\mat |                       |                       |
| hbf{\mu}_{\mathbf{w}} |                       |                       |
|  \right)\mathbf{=}\ma |                       |                       |
| thbf{\Sigma}_{\mathbf |                       |                       |
| {w}|\mathbf{y}}\left( |                       |                       |
|  \mathbf{\Sigma}_{\ma |                       |                       |
| thbf{w}}^{- 1}\mathbf |                       |                       |
| {\mu}_{\mathbf{w}}\ma |                       |                       |
| thbf{+}\beta\mathbf{\ |                       |                       |
| Phi}\mathbf{y} \right |                       |                       |
| ) \\                  |                       |                       |
|    \mathbf{\Sigma}_{\ |                       |                       |
| mathbf{w}|\mathbf{y}} |                       |                       |
| \mathbf{=}\left( \mat |                       |                       |
| hbf{\Sigma}_{\mathbf{ |                       |                       |
| w}}^{- 1}\mathbf{+}\b |                       |                       |
| eta\mathbf{\Phi}\math |                       |                       |
| bf{\Phi}^{T} \right)^ |                       |                       |
| {- 1} \\              |                       |                       |
|    \end{matrix} \righ |                       |                       |
| t.\                   |                       |                       |
+-----------------------+-----------------------+-----------------------+

..

   Note plug :math:`\mathbf{\Sigma}_{\mathbf{w}|\mathbf{y}}` into
   :math:`\mathbf{\mu}_{\mathbf{w}|\mathbf{y}}` and we have

.. math:: \mathbf{\mu}_{\mathbf{w}|\mathbf{y}}\mathbf{=}\left( \mathbf{\Sigma}_{\mathbf{w}}^{- 1}\mathbf{+}\beta\mathbf{\Phi}\mathbf{\Phi}^{T} \right)^{- 1}\left( \mathbf{\Sigma}_{\mathbf{w}}^{- 1}\mathbf{\mu}_{\mathbf{w}}\mathbf{+}\beta\mathbf{\Phi}\mathbf{y} \right)

   which is identical to :math:`\mathbf{w}^{*}` in (2‑14). The solution
   of Gaussian MAP now becomes the mean of the posterior, and from this
   point of view Gaussian Bayesian can be viewed as extension of
   Gaussian MAP.

   Then one approach to derive the predictive distribution is to first
   find the joint distribution
   :math:`p\left( \mathcal{y,}\mathbf{w|}\mathcal{x,}\mathbf{y} \right)`,
   and then find the marginal

+-----------------------+-----------------------+-----------------------+
| .. math:: p\left( \ma |                       | (2‑17)                |
| thcal{y|}\mathcal{x,} |                       |                       |
| \mathbf{y} \right) =  |                       |                       |
| \int_{}^{}{p\left( \m |                       |                       |
| athcal{y,}\mathbf{w|} |                       |                       |
| \mathcal{x,}\mathbf{y |                       |                       |
| } \right)d\mathbf{w}} |                       |                       |
|  = \int_{}^{}{p\left( |                       |                       |
|  \mathcal{y|}\mathbf{ |                       |                       |
| w,}\mathcal{x,}\mathb |                       |                       |
| f{y} \right)p\left( \ |                       |                       |
| mathbf{w|}\mathcal{x, |                       |                       |
| }\mathbf{y} \right)d\ |                       |                       |
| mathbf{w}}            |                       |                       |
+-----------------------+-----------------------+-----------------------+

..

   For the Gaussian Bayesian regression, (2‑17) can be solved using
   *Theorem 1‑8* without actual integration. We have
   :math:`\mathbf{w|y\sim}\operatorname{Gaussian}\left( \mathbf{\mu}_{w|y}\mathbf{,}\Sigma_{\mathbf{w}|\mathbf{y}} \right)`
   and
   :math:`\mathcal{y|}\mathbf{w,}\mathcal{x,}\mathbf{y\sim}\text{Gaussian}\left( \phi^{T}\left( \mathcal{x} \right)\mathbf{w},\sigma^{2} \right)`.
   By *Theorem 1‑8*, we have the joint distribution is
   :math:`\mathcal{y}\mathcal{|x,}\mathbf{y}\mathbf{\sim}\operatorname{Gaussian}\left( \mu_{\mathcal{y}},\sigma_{\mathcal{y}}^{2} \right)`
   where

+-----------------------+-----------------------+-----------------------+
| .. math:: \mu_{\mathc |                       | (2‑18)                |
| al{y}} = \phi^{T}\lef |                       |                       |
| t( \mathcal{x} \right |                       |                       |
| )\mathbf{\mu}_{\mathb |                       |                       |
| f{w}|\mathbf{y}},\sig |                       |                       |
| ma_{\mathcal{y}}^{2}  |                       |                       |
| = \sigma^{2}\mathbf{+ |                       |                       |
| }\phi^{T}\left( \math |                       |                       |
| cal{x} \right)\mathbf |                       |                       |
| {\Sigma}_{\mathbf{w}| |                       |                       |
| \mathbf{y}}\phi\left( |                       |                       |
|  \mathcal{x} \right)  |                       |                       |
+-----------------------+-----------------------+-----------------------+

..

   One known *issue* of the predictive distribution define by (2‑18) is
   it tends to have large variance for those :math:`\mathcal{x}` not
   near any data entries, because of no data in the neighborhood of
   :math:`\mathcal{\text{x\ }}` to provide sufficient information for
   the prediction, and the issue is especially serious when the data is
   sparse.

   **Example** **2‑2** Assume
   :math:`\mathbf{w}\sim\operatorname{Gaussian}\left( \mathbf{w}|\mathbf{0},\varsigma^{2}\mathbf{I} \right),\alpha = \varsigma^{- 2}`
   and
   :math:`y_{i}|\mathbf{w}\sim\text{Gaussian}\left( \phi^{T}\left( \mathbf{x}_{i} \right)\mathbf{w},\sigma^{2} \right),\beta = \sigma^{- 2}`
   as in Example 2‑1, we have

+-----------------------+-----------------------+-----------------------+
| .. math::             |                       | (2‑19)                |
|                       |                       |                       |
|    \left\{ \begin{mat |                       |                       |
| rix}                  |                       |                       |
|    \mathbf{\mu}_{\mat |                       |                       |
| hbf{w}|\mathbf{y}}\ma |                       |                       |
| thbf{=}\beta\mathbf{\ |                       |                       |
| Sigma}_{\mathbf{w}|\m |                       |                       |
| athbf{y}}\mathbf{\Phi |                       |                       |
| }\mathbf{y} \\        |                       |                       |
|    \mathbf{\Sigma}_{\ |                       |                       |
| mathbf{w}|\mathbf{y}} |                       |                       |
| \mathbf{=}\left( \alp |                       |                       |
| ha\mathbf{I}\mathbf{+ |                       |                       |
| }\beta\mathbf{\Phi}\m |                       |                       |
| athbf{\Phi}^{T}\left( |                       |                       |
|  \mathbf{X} \right) \ |                       |                       |
| right)^{- 1} \\       |                       |                       |
|    \end{matrix} \righ |                       |                       |
| t.\                   |                       |                       |
+-----------------------+-----------------------+-----------------------+

..

   Note

+-----------------------+-----------------------+-----------------------+
| .. math:: \mathbf{\mu |                       | (2‑20)                |
| }_{\mathbf{w}|\mathbf |                       |                       |
| {y}}\mathbf{=}\beta\m |                       |                       |
| athbf{\Sigma}_{\mathb |                       |                       |
| f{w}|\mathbf{y}}\math |                       |                       |
| bf{\Phi}\mathbf{y =}\ |                       |                       |
| beta\left( \alpha\mat |                       |                       |
| hbf{I}\mathbf{+}\beta |                       |                       |
| \mathbf{\Phi}\mathbf{ |                       |                       |
| \Phi}^{T} \right)^{-  |                       |                       |
| 1}\mathbf{\Phi}\mathb |                       |                       |
| f{y}\mathbf{=}\left(  |                       |                       |
| \mathbf{\Phi}\mathbf{ |                       |                       |
| \Phi}^{T}\mathbf{+}\f |                       |                       |
| rac{\alpha}{\beta}\ma |                       |                       |
| thbf{I} \right)^{- 1} |                       |                       |
| \mathbf{\text{Φy}}    |                       |                       |
+-----------------------+-----------------------+-----------------------+

..

   which is identical to :math:`\mathbf{w}^{*}` in (2‑15).

Model selection
'''''''''''''''

-  **Model Selection**. Given a probabilistic model
   :math:`p\left( \mathbf{y}|\mathbf{\omega} \right)` where
   :math:`\mathbf{\omega}` are parameter, then the choice of
   :math:`\mathbf{\omega}` can be done by maximizing
   :math:`p\left( \mathbf{\omega}|\mathbf{y} \right)`, called the
   **model selection**. For simplicity, we study the case of Example 2‑1
   (Gaussian MAP) and Example 2‑2 (Gaussian Bayesian), i.e. assuming the
   independence of components of :math:`\mathbf{w}`,

.. math:: \left( \alpha,\beta \right)\sim p\left( \alpha,\beta|\mathbf{y} \right)

.. math:: \mathbf{w}\sim\operatorname{Gaussian}\left( \mathbf{w}|\mathbf{0},\alpha^{- 1}\mathbf{I} \right),\alpha = \varsigma^{- 2}

.. math:: y_{i}|\mathbf{w}\sim\text{Gaussian}\left( \phi^{T}\left( \mathbf{x}_{i} \right)\mathbf{w},\beta^{- 1} \right),\beta = \sigma^{- 2}

where we use precisions :math:`\alpha,\beta` for convenience. Note

.. math:: p\left( \alpha,\beta|\mathbf{y} \right)\mathbf{\propto}p\left( \mathbf{y}|\alpha,\beta \right)p\left( \alpha,\beta \right)

Further assume :math:`p\left( \alpha,\beta \right)` is nearly flat and
can be treated as constant. This assumption can be justified by thinking
about there is no particular favorable choice of :math:`\alpha,\beta`
without the data. Therefore, the model selection is equivalent to
maximizing to maximizing the evidence

.. math:: p\left( \alpha,\beta|\mathbf{y} \right)\mathbf{\propto}p\left( \mathbf{y}|\alpha,\beta \right)

+-----------------------------------------------------------------------+
| If we use the familiar *Theorem 1‑8* again,                           |
| :math:`p\left( \mathbf{y}|\alpha,\beta \right)\sim\operatorname{Gauss |
| ian}\left( \mathbf{0},\mathbf{\Sigma}_{\mathbf{y}} \right)`,          |
| and by (1‑10) we have                                                 |
|                                                                       |
| .. math:: \mathbf{\Sigma}_{\mathbf{y}} = \sigma^{2}\mathbf{I}\mathbf{ |
| +}\varsigma^{2}\phi^{T}\left( \mathbf{X} \right)\mathbf{\phi}\left( \ |
| mathbf{X} \right)                                                     |
|                                                                       |
| Then                                                                  |
| :math:`\ln p \propto \log\left| \mathbf{\Sigma}_{\mathbf{y}}^{- 1} \r |
| ight| - \mathbf{y}^{T}\mathbf{\Sigma}_{\mathbf{y}}^{- 1}\mathbf{y}`,  |
| which involves matrix, making it difficult to calculate gradients     |
| :math:`\frac{\partial\ln p}{\partial\sigma}` and                      |
| :math:`\frac{\partial\ln p}{\partial\varsigma}`. Thus, we turn to     |
| another approach that integrates joint distribution                   |
| :math:`p\left( \mathbf{y,w}|\alpha,\beta \right)`.                    |
+-----------------------------------------------------------------------+

It turns out direct use of (1‑10) of *Theorem 1‑8* does not yield an
easy solution, see the remark above. We instead look at

.. math:: p\left( \mathbf{y}|\alpha,\beta \right) = \int_{}^{}{p\left( \mathbf{y,w}|\alpha,\beta \right)d\mathbf{w}} = \int_{}^{}{p\left( \mathbf{y}|\mathbf{w},\beta \right)p\left( \mathbf{w}|\alpha \right)d\mathbf{w}}

Since the components of :math:`\mathbf{y,w}` are independent, let
:math:`M` be the size of :math:`\mathbf{w}` as well as the number of
basis functions, then

.. math:: p\left( \mathbf{y}|\alpha,\beta \right) = \int_{}^{}{\prod_{i = 1}^{N}{p\left( y_{i}\mathbf{|w},\beta \right)}\prod_{j = 1}^{M}{p\left( w_{j}|\alpha \right)}d\mathbf{w}} = \left( \frac{\beta}{2\pi} \right)^{\frac{N}{2}}\left( \frac{\alpha}{2\pi} \right)^{\frac{M}{2}}\int_{}^{}{\exp\left\{ - \frac{1}{2}L\left( \mathbf{w} \right) \right\} d\mathbf{w}}

where by (2‑15) we have

+-----------------------+-----------------------+-----------------------+
| .. math:: L\left( \ma |                       | (2‑21)                |
| thbf{w} \right) = \be |                       |                       |
| ta\left( \mathbf{y} - |                       |                       |
|  \phi^{T}\left( \math |                       |                       |
| bf{X} \right)\mathbf{ |                       |                       |
| w} \right)^{T}\left(  |                       |                       |
| \mathbf{y} - \phi^{T} |                       |                       |
| \left( \mathbf{X} \ri |                       |                       |
| ght)\mathbf{w} \right |                       |                       |
| ) + \alpha\mathbf{w}^ |                       |                       |
| {T}\mathbf{w}         |                       |                       |
+-----------------------+-----------------------+-----------------------+

**Step 1 - Simplify the integration**. The integration result does not
easily follow from the format of (2‑21). The integration is only
dependent on :math:`\mathbf{Ⲗ}_{\mathbf{y},\mathbf{w}}` based on the
Gaussian density, and by (1‑7) we have

.. math::

   \mathbf{Ⲗ}_{\mathbf{w,y}}\mathbf{=}\begin{pmatrix}
   \alpha\mathbf{I}\mathbf{+}\beta\phi\left( \mathbf{X} \right)\phi^{T}\left( \mathbf{X} \right) & - \beta\phi\left( \mathbf{X} \right) \\
    - \beta\phi^{T}\left( \mathbf{X} \right) & \beta\mathbf{I} \\
   \end{pmatrix},\mathbf{\mu}_{\mathbf{w,y}}\mathbf{=}\mathbf{0}

Unfortunately, this would be too complicated since the integration
result contains
:math:`\left| \mathbf{Ⲗ}_{\mathbf{y},\mathbf{w}} \right|`; but notice
:math:`\mathbf{Ⲗ}_{\mathbf{w}|\mathbf{y}}\mathbf{=}\alpha\mathbf{I}\mathbf{+}\beta\phi\left( \mathbf{X} \right)\phi^{T}\left( \mathbf{X} \right)`
by (2‑19) of Example 2‑1 and (*1*\ ‑\ *5*) of *Theorem 1‑6*, we can
write

+-----------------------+-----------------------+-----------------------+
| .. math:: L\left( \ma |                       | (2‑22)                |
| thbf{w} \right) = \be |                       |                       |
| ta\mathbf{y}^{T}\math |                       |                       |
| bf{y}\mathbf{-}2\beta |                       |                       |
| \mathbf{y}^{T}\phi^{T |                       |                       |
| }\left( \mathbf{X} \r |                       |                       |
| ight)\mathbf{w +}\bet |                       |                       |
| a\mathbf{w}^{T}\phi\l |                       |                       |
| eft( \mathbf{X} \righ |                       |                       |
| t)\phi^{T}\left( \mat |                       |                       |
| hbf{X} \right)\mathbf |                       |                       |
| {w} + \alpha\mathbf{w |                       |                       |
| }^{T}\mathbf{w}\mathb |                       |                       |
| f{=}\beta\mathbf{y}^{ |                       |                       |
| T}\mathbf{y}\mathbf{- |                       |                       |
| }2\beta\mathbf{w}^{T} |                       |                       |
| \phi\left( \mathbf{X} |                       |                       |
|  \right)\mathbf{y +}\ |                       |                       |
| mathbf{w}^{T}\left( \ |                       |                       |
| text{βϕ}\left( \mathb |                       |                       |
| f{X} \right)\phi^{T}\ |                       |                       |
| left( \mathbf{X} \rig |                       |                       |
| ht) + \alpha\mathbf{I |                       |                       |
| } \right)\mathbf{w}\m |                       |                       |
| athbf{=}\beta\mathbf{ |                       |                       |
| y}^{T}\mathbf{y}\math |                       |                       |
| bf{-}2\beta\mathbf{w} |                       |                       |
| ^{T}\phi\left( \mathb |                       |                       |
| f{X} \right)\mathbf{y |                       |                       |
| }\mathbf{+}\mathbf{w} |                       |                       |
| ^{T}\mathbf{Ⲗ}_{\math |                       |                       |
| bf{w}|\mathbf{y}}\mat |                       |                       |
| hbf{w}                |                       |                       |
+-----------------------+-----------------------+-----------------------+

Recall
:math:`\mathbf{\mu}_{\mathbf{w}|\mathbf{y}}\mathbf{=}\beta\mathbf{Ⲗ}_{\mathbf{w}|\mathbf{y}}^{- 1}\phi\left( \mathbf{X} \right)\mathbf{y}`
from (*2*\ ‑\ *19*), then we further manipulate above as,

+-----------------------+-----------------------+-----------------------+
| .. math:: L\left( \ma |                       | (2‑23)                |
| thbf{w} \right) = \le |                       |                       |
| ft( \mathbf{w -}\math |                       |                       |
| bf{\mu}_{\mathbf{w}|\ |                       |                       |
| mathbf{y}} \right)^{T |                       |                       |
| }\mathbf{Ⲗ}_{\mathbf{ |                       |                       |
| w}|\mathbf{y}}\left(  |                       |                       |
| \mathbf{w -}\mathbf{\ |                       |                       |
| mu}_{\mathbf{w}|\math |                       |                       |
| bf{y}} \right)\mathbf |                       |                       |
| {+}2\mathbf{\mu}_{\ma |                       |                       |
| thbf{w}|\mathbf{y}}^{ |                       |                       |
| T}\mathbf{Ⲗ}_{\mathbf |                       |                       |
| {w}|\mathbf{y}}\mathb |                       |                       |
| f{w} - \mathbf{\mu}_{ |                       |                       |
| \mathbf{w}|\mathbf{y} |                       |                       |
| }^{T}\mathbf{Ⲗ}_{\mat |                       |                       |
| hbf{w}|\mathbf{y}}\ma |                       |                       |
| thbf{\mu}_{\mathbf{w} |                       |                       |
| |\mathbf{y}}\mathbf{+ |                       |                       |
| }\mathbf{y}^{T}\mathb |                       |                       |
| f{y}\mathbf{-}2\beta\ |                       |                       |
| mathbf{w}^{T}\phi\lef |                       |                       |
| t( \mathbf{X} \right) |                       |                       |
| \mathbf{y}            |                       |                       |
+-----------------------+-----------------------+-----------------------+

At first glance it looks like the simplification cannot continue, but
note the similarity between :math:`\mathbf{\mu}_{\mathbf{w}|\mathbf{y}}`
and :math:`\beta\mathbf{w}^{T}\phi\left( \mathbf{X} \right)\mathbf{y}`,
and we actually have

.. math:: \beta\mathbf{w}^{T}\phi\left( \mathbf{X} \right)\mathbf{y =}\beta\mathbf{w}^{T}\mathbf{Ⲗ}_{\mathbf{w}|\mathbf{y}}\mathbf{Ⲗ}_{\mathbf{w}|\mathbf{y}}^{- 1}\phi\left( \mathbf{X} \right)\mathbf{y}\mathbf{=}\mathbf{w}^{T}\mathbf{Ⲗ}_{\mathbf{w}|\mathbf{y}}\mathbf{\mu}_{\mathbf{w}|\mathbf{y}}

Then respectively for (*2*\ ‑\ *22*) and (*2*\ ‑\ *23*) we have

+-----------------------+-----------------------+-----------------------+
| .. math:: L\left( \ma |                       | (2‑24)                |
| thbf{w} \right) = \be |                       |                       |
| ta\mathbf{y}^{T}\math |                       |                       |
| bf{y}\mathbf{-}2\math |                       |                       |
| bf{w}^{T}\mathbf{Ⲗ}_{ |                       |                       |
| \mathbf{w}|\mathbf{y} |                       |                       |
| }\mathbf{\mu}_{\mathb |                       |                       |
| f{w}|\mathbf{y}}\math |                       |                       |
| bf{+}\mathbf{w}^{T}\m |                       |                       |
| athbf{Ⲗ}_{\mathbf{w}| |                       |                       |
| \mathbf{y}}\mathbf{w} |                       |                       |
+-----------------------+-----------------------+-----------------------+

and

.. math:: L\left( \mathbf{w} \right) = \left( \mathbf{w -}\mathbf{\mu}_{\mathbf{w}|\mathbf{y}} \right)^{T}\mathbf{Ⲗ}_{\mathbf{w}|\mathbf{y}}\left( \mathbf{w -}\mathbf{\mu}_{\mathbf{w}|\mathbf{y}} \right)\mathbf{+}\boxed{2\mathbf{\mu}_{\mathbf{w}|\mathbf{y}}^{T}\mathbf{Ⲗ}_{\mathbf{w}|\mathbf{y}}\mathbf{w}} - \mathbf{\mu}_{\mathbf{w}|\mathbf{y}}^{T}\mathbf{Ⲗ}_{\mathbf{w}|\mathbf{y}}\mathbf{\mu}_{\mathbf{w}|\mathbf{y}}\mathbf{+}\beta\mathbf{y}^{T}\mathbf{y}\mathbf{-}\boxed{2\mathbf{w}^{T}\mathbf{Ⲗ}_{\mathbf{w}|\mathbf{y}}\mathbf{\mu}_{\mathbf{w}|\mathbf{y}}} = \left( \mathbf{w -}\mathbf{\mu}_{\mathbf{w}|\mathbf{y}} \right)^{T}\mathbf{Ⲗ}_{\mathbf{w}|\mathbf{y}}\left( \mathbf{w -}\mathbf{\mu}_{\mathbf{w}|\mathbf{y}} \right)\mathbf{+}\beta\mathbf{y}^{T}\mathbf{y} - \mathbf{\mu}_{\mathbf{w}|\mathbf{y}}^{T}\mathbf{Ⲗ}_{\mathbf{w}|\mathbf{y}}\mathbf{\mu}_{\mathbf{w}|\mathbf{y}}

where the remaining terms
“\ :math:`\beta\mathbf{y}^{T}\mathbf{y} - \mathbf{\mu}_{\mathbf{w}|\mathbf{y}}^{T}\mathbf{Ⲗ}_{\mathbf{w}|\mathbf{y}}\mathbf{\mu}_{\mathbf{w}|\mathbf{y}}`\ ”
are independent of :math:`\mathbf{w}`, and consequently,

+-----------------------+-----------------------+-----------------------+
| .. math:: \int_{}^{}{ |                       | (2‑25)                |
| \exp\left\{ - \frac{1 |                       |                       |
| }{2}L\left( \mathbf{w |                       |                       |
| } \right) \right\} d\ |                       |                       |
| mathbf{w}} = \exp\lef |                       |                       |
| t\{ - \frac{1}{2}\lef |                       |                       |
| t( \beta\mathbf{y}^{T |                       |                       |
| }\mathbf{y} - \mathbf |                       |                       |
| {\mu}_{\mathbf{w}|\ma |                       |                       |
| thbf{y}}^{T}\mathbf{Ⲗ |                       |                       |
| }_{\mathbf{w}|\mathbf |                       |                       |
| {y}}\mathbf{\mu}_{\ma |                       |                       |
| thbf{w}|\mathbf{y}} \ |                       |                       |
| right) \right\}\int_{ |                       |                       |
| }^{}{\exp\left\{ - \f |                       |                       |
| rac{1}{2}\left( \math |                       |                       |
| bf{w -}\mathbf{\mu}_{ |                       |                       |
| \mathbf{w}|\mathbf{y} |                       |                       |
| } \right)^{T}\mathbf{ |                       |                       |
| Ⲗ}_{\mathbf{w}|\mathb |                       |                       |
| f{y}}\left( \mathbf{w |                       |                       |
|  -}\mathbf{\mu}_{\mat |                       |                       |
| hbf{w}|\mathbf{y}} \r |                       |                       |
| ight) \right\} d\math |                       |                       |
| bf{w}} = \left( 2\pi  |                       |                       |
| \right)^{\frac{M}{2}} |                       |                       |
| \left| \mathbf{Ⲗ}_{\m |                       |                       |
| athbf{w}|\mathbf{y}}  |                       |                       |
| \right|^{- \frac{1}{2 |                       |                       |
| }}\exp\left\{ - \frac |                       |                       |
| {1}{2}\left( \beta\ma |                       |                       |
| thbf{y}^{T}\mathbf{y} |                       |                       |
|  - \mathbf{\mu}_{\mat |                       |                       |
| hbf{w}|\mathbf{y}}^{T |                       |                       |
| }\mathbf{Ⲗ}_{\mathbf{ |                       |                       |
| w}|\mathbf{y}}\mathbf |                       |                       |
| {\mu}_{\mathbf{w}|\ma |                       |                       |
| thbf{y}} \right) \rig |                       |                       |
| ht\}                  |                       |                       |
+-----------------------+-----------------------+-----------------------+

We can further find from (2‑24) that

.. math:: L\left( \mathbf{\mu}_{\mathbf{w}|\mathbf{y}} \right) = \beta\mathbf{y}^{T}\mathbf{y}\mathbf{-}2\mathbf{\mu}_{\mathbf{w}|\mathbf{y}}^{T}\mathbf{Ⲗ}_{\mathbf{w}|\mathbf{y}}\mathbf{\mu}_{\mathbf{w}|\mathbf{y}}\mathbf{+}\mathbf{\mu}_{\mathbf{w}|\mathbf{y}}^{T}\mathbf{Ⲗ}_{\mathbf{w}|\mathbf{y}}\mathbf{\mu}_{\mathbf{w}|\mathbf{y}}\mathbf{=}\beta\mathbf{y}^{T}\mathbf{y}\mathbf{-}\mathbf{\mu}_{\mathbf{w}|\mathbf{y}}^{T}\mathbf{Ⲗ}_{\mathbf{w}|\mathbf{y}}\mathbf{\mu}_{\mathbf{w}|\mathbf{y}}

Plug this into (2‑25) and we have

.. math:: \int_{}^{}{\exp\left\{ - \frac{1}{2}L\left( \mathbf{w} \right) \right\} d\mathbf{w}} = \left( 2\pi \right)^{\frac{M}{2}}\left| \mathbf{Ⲗ}_{\mathbf{w}|\mathbf{y}} \right|^{- \frac{1}{2}}\exp\left\{ - \frac{1}{2}L\left( \mathbf{\mu}_{\mathbf{w}|\mathbf{y}} \right) \right\} \Rightarrow p\left( \mathbf{y}|\alpha,\beta \right) = \left( \frac{\beta}{2\pi} \right)^{\frac{N}{2}}\left( \alpha \right)^{\frac{M}{2}}\left| \mathbf{Ⲗ}_{\mathbf{w}|\mathbf{y}} \right|^{- \frac{1}{2}}\exp\left\{ - \frac{1}{2}L\left( \mathbf{\mu}_{\mathbf{w}|\mathbf{y}} \right) \right\} \Rightarrow \ln p \propto \frac{M}{2}\ln\alpha + \frac{N}{2}\ln\beta - \frac{1}{2}\ln\left| \mathbf{Ⲗ}_{\mathbf{w}|\mathbf{y}} \right| - \frac{1}{2}L\left( \mathbf{\mu}_{\mathbf{w}|\mathbf{y}} \right)

**Step 2 - Find optimal parameters**. Treating :math:`\beta` as
constant, we have

.. math:: \frac{\partial\ln p}{\partial\alpha} = \frac{M}{2\alpha} - \frac{1}{2}\frac{\partial\ln\left| \mathbf{Ⲗ}_{\mathbf{w}|\mathbf{y}} \right|}{\partial\alpha} - \frac{1}{2}\frac{\partial L\left( \mathbf{\mu}_{\mathbf{w}|\mathbf{y}} \right)}{\partial\alpha} = 0

From (*2*\ ‑\ *21*), we have

.. math:: \frac{\partial L\left( \mathbf{\mu}_{\mathbf{w}|\mathbf{y}} \right)}{\partial\alpha} = \mathbf{\mu}_{\mathbf{w}|\mathbf{y}}^{T}\mathbf{\mu}_{\mathbf{w}|\mathbf{y}}

Recall the fact that the determinant of a matrix is the product of its
eigenvalues. Since
:math:`\mathbf{Ⲗ}_{\mathbf{w}|\mathbf{y}}\mathbf{=}\alpha\mathbf{I}\mathbf{+}\beta\phi\left( \mathbf{X} \right)\phi^{T}\left( \mathbf{X} \right)`,
then
:math:`\left| \mathbf{Ⲗ}_{\mathbf{w}|\mathbf{y}} \right| = \prod_{i = 1}^{M}{\beta\lambda_{i} + \alpha}`
where :math:`\lambda_{i}` are the :math:`m` eigenvalues of
:math:`\phi\left( \mathbf{X} \right)\phi^{T}\left( \mathbf{X} \right)`,
then

.. math:: \ln\left| \mathbf{Ⲗ}_{\mathbf{w}|\mathbf{y}} \right| = \sum_{i = 1}^{M}{\ln\left( \beta\lambda_{i} + \alpha \right)} \Rightarrow \frac{\partial\ln\left| \mathbf{Ⲗ}_{\mathbf{w}|\mathbf{y}} \right|}{\partial\alpha} = \sum_{i = 1}^{M}\frac{1}{\beta\lambda_{i} + \alpha} \Rightarrow \frac{M}{\alpha} - \sum_{i = 1}^{M}\frac{1}{\beta\lambda_{i} + \alpha} - \mathbf{\mu}_{\mathbf{w}|\mathbf{y}}^{T}\mathbf{\mu}_{\mathbf{w}|\mathbf{y}} = 0 \Rightarrow M - \sum_{i = 1}^{M}\frac{\alpha}{\beta\lambda_{i} + \alpha} = \sum_{i = 1}^{M}\frac{\beta\lambda_{i}}{\beta\lambda_{i} + \alpha} = \alpha\ \mathbf{\mu}_{\mathbf{w}|\mathbf{y}}^{T}\mathbf{\mu}_{\mathbf{w}|\mathbf{y}}\mathbf{\Rightarrow}\alpha = \frac{\sum_{i = 1}^{M}\frac{\beta\lambda_{i}}{\beta\lambda_{i} + \alpha}}{\mathbf{\mu}_{\mathbf{w}|\mathbf{y}}^{T}\mathbf{\mu}_{\mathbf{w}|\mathbf{y}}}

Let
:math:`\mathcal{M =}\sum_{i = 1}^{M}\frac{\beta\lambda_{i}}{\beta\lambda_{i} + \alpha}`,
we have

.. math:: \alpha = \frac{\mathcal{M}}{\mathbf{\mu}_{\mathbf{w}|\mathbf{y}}^{T}\mathbf{\mu}_{\mathbf{w}|\mathbf{y}}}

Treating :math:`\alpha` as a constant, then we have

.. math:: \frac{\partial\ln\left| \mathbf{Ⲗ}_{\mathbf{w}|\mathbf{y}} \right|}{\partial\beta} = \sum_{i = 1}^{M}\frac{\lambda_{i}}{\beta\lambda_{i} + \alpha},\frac{\partial L\left( \mathbf{\mu}_{\mathbf{w}|\mathbf{y}} \right)}{\partial\beta} = \left( \mathbf{y} - \mathbf{\Phi}^{T}\mathbf{\mu}_{\mathbf{w}|\mathbf{y}} \right)^{T}\left( \mathbf{y} - \mathbf{\Phi}^{T}\mathbf{\mu}_{\mathbf{w}|\mathbf{y}} \right)

Then

.. math:: \frac{\partial\ln p}{\partial\beta} = \frac{N}{2\beta} - \frac{1}{2}\sum_{i = 1}^{M}\frac{\lambda_{i}}{\beta\lambda_{i} + \alpha} - \frac{1}{2}\left( \mathbf{y} - \mathbf{\Phi}^{T}\mathbf{\mu}_{\mathbf{w}|\mathbf{y}} \right)^{T}\left( \mathbf{y} - \mathbf{\Phi}^{T}\mathbf{\mu}_{\mathbf{w}|\mathbf{y}} \right) = 0 \Rightarrow \frac{N}{\beta} - \frac{1}{\beta}\mathcal{M -}\left\| \mathbf{\epsilon} \right\|_{2}^{2} = 0 \Rightarrow \beta = \frac{N\mathcal{- M}}{\left\| \mathbf{\epsilon} \right\|_{2}^{2}}

Now we have iterative formulas to estimate a fixed point for both
:math:`\alpha,\beta`.

+-----------------------+-----------------------+-----------------------+
| .. math::             |                       | (2‑26)                |
|                       |                       |                       |
|    \left\{ \begin{mat |                       |                       |
| rix}                  |                       |                       |
|    \alpha = \frac{\ma |                       |                       |
| thcal{M}}{\mathbf{\mu |                       |                       |
| }_{\mathbf{w}|\mathbf |                       |                       |
| {y}}^{T}\mathbf{\mu}_ |                       |                       |
| {\mathbf{w}|\mathbf{y |                       |                       |
| }}} \\                |                       |                       |
|    \beta = \frac{N\ma |                       |                       |
| thcal{- M}}{\left\| \ |                       |                       |
| mathbf{\epsilon} \rig |                       |                       |
| ht\|_{2}^{2}} \\      |                       |                       |
|    \end{matrix} \righ |                       |                       |
| t.\                   |                       |                       |
+-----------------------+-----------------------+-----------------------+

**Implementation**. Note at the end of each iteration we need to update

.. math:: \mathbf{\mu}_{\mathbf{w}|\mathbf{y}}\mathbf{=}\beta\mathbf{\Sigma}_{\mathbf{w}|\mathbf{y}}^{- 1}\mathbf{\Phi}\mathbf{y}\mathbf{=}\beta\left( \alpha\mathbf{I}\mathbf{+}\beta\mathbf{\Phi}\mathbf{\Phi}^{T} \right)^{- 1}\mathbf{\Phi}\mathbf{y}

Computing matrix inverse is not recommended. A recommended algorithm is
to first eigen-decompose

.. math::

   \mathbf{\Phi}\mathbf{\Phi}^{T} = \mathbf{\text{UΛ}}\mathbf{U}^{T} \Rightarrow \mathbf{\Sigma}_{\mathbf{w}|\mathbf{y}}\mathbf{=}\mathbf{U}\left( \alpha\mathbf{I} + \beta\mathbf{\Lambda} \right)\mathbf{U}^{T}\mathbf{\Rightarrow}\mathbf{\Sigma}_{\mathbf{w}|\mathbf{y}}^{- 1}\mathbf{=}\mathbf{U}\left( \alpha\mathbf{I} + \beta\mathbf{\Lambda} \right)^{- 1}\mathbf{U}^{T}

.. math:: \mathbf{\mu}_{\mathbf{w}|\mathbf{y}}\mathbf{=}\beta\mathbf{U}\left( \alpha\mathbf{I} + \beta\mathbf{\Lambda} \right)^{- 1}\mathbf{U}^{T}\mathbf{\Phi}\mathbf{y}\mathbf{\Rightarrow}\mathbf{\mu}_{\mathbf{w}|\mathbf{y}}^{T}\mathbf{\mu}_{\mathbf{w}|\mathbf{y}}\mathbf{=}\beta^{2}\mathbf{y}^{T}\mathbf{\Phi}^{T}\mathbf{U}\left( \alpha\mathbf{I} + \beta\mathbf{\Lambda} \right)^{- 1}\mathbf{U}^{T}\mathbf{U}\left( \alpha\mathbf{I} + \beta\mathbf{\Lambda} \right)^{- 1}\mathbf{U}^{T}\mathbf{\Phi}\mathbf{y =}\beta^{2}\mathbf{y}^{T}\mathbf{\Phi}^{T}\mathbf{U}\left( \alpha\mathbf{I} + \beta\mathbf{\Lambda} \right)^{- 2}\mathbf{U}^{T}\mathbf{\Phi}\mathbf{y}

and

.. math:: \left\| \mathbf{y} - \mathbf{\Phi}^{T}\mathbf{\mu}_{\mathbf{w}|\mathbf{y}} \right\|_{2}^{2} = \mathbf{y}^{T}\mathbf{y}\mathbf{-}2\mathbf{y}^{T}\mathbf{\Phi}^{T}\mathbf{\mu}_{\mathbf{w}|\mathbf{y}}\mathbf{+}\mathbf{\mu}_{\mathbf{w}|\mathbf{y}}^{T}\mathbf{\Phi}\mathbf{\Phi}^{T}\mathbf{\mu}_{\mathbf{w}|\mathbf{y}}\mathbf{=}\mathbf{y}^{T}\mathbf{y}\mathbf{-}2\beta\mathbf{y}^{T}\mathbf{\Phi}^{T}\mathbf{U}\left( \alpha\mathbf{I} + \beta\mathbf{\Lambda} \right)^{- 1}\mathbf{U}^{T}\mathbf{\Phi}\mathbf{y}\mathbf{+}\beta^{2}\mathbf{y}^{T}\mathbf{\Phi}^{T}\mathbf{U}\left( \alpha\mathbf{I} + \beta\mathbf{\Lambda} \right)^{- 1}\mathbf{U}^{T}\mathbf{\Phi}\mathbf{\Phi}^{T}\mathbf{U}\left( \alpha\mathbf{I} + \beta\mathbf{\Lambda} \right)^{- 1}\mathbf{U}^{T}\mathbf{\Phi}\mathbf{y}

Thus, we can store
:math:`\mathbf{y}^{T}\mathbf{y,}\mathbf{U}^{T}\mathbf{\Phi}\mathbf{y}`
and :math:`\mathbf{U}^{T}\mathbf{\Phi}\mathbf{\Phi}^{T}\mathbf{U}`,
which are fixed quantities, for repeated use at the end of each
iteration.

**Interpretation of** :math:`\mathcal{M}`. Note above model selection
works for both Gaussian MAP and Gaussian Bayesian. Let
:math:`\mathbf{\Phi = U}\mathbf{\Lambda}^{\frac{1}{2}}\mathbf{V}^{T}` be
the SVD of :math:`\mathbf{\Phi}` where :math:`\mathbf{U},\mathbf{V}` are
orthonormal matrices, and recall orthonormal matrices geometrically
represent rotation, then for :math:`\mathbf{w}^{*}` in (*2*\ ‑\ *15*)
and :math:`\mathbf{\mu}_{\mathbf{w}|\mathbf{y}}` in (*2*\ ‑\ *20*), we
have

.. math:: \mathbf{w}^{*} = \mathbf{\mu}_{\mathbf{w}|\mathbf{y}}\mathbf{=}\beta\mathbf{\Sigma}_{\mathbf{w}|\mathbf{y}}^{- 1}\mathbf{\Phi}\mathbf{y}\mathbf{=}\beta\left( \alpha\mathbf{I}\mathbf{+}\beta\mathbf{\Phi}\mathbf{\Phi}^{T} \right)^{- 1}\mathbf{\Phi}\mathbf{y}

Then
:math:`\widetilde{\mathbf{y}}\mathbf{=}\mathbf{\Phi}^{T}\mathbf{w}^{*}`
in Gaussian MAP and
:math:`\mathbb{E}\left\lbrack \widetilde{\mathbf{y}} \right\rbrack\mathbf{=}\mathbf{\Phi}^{T}\mathbf{\mu}_{\mathbf{w}|\mathbf{y}}`
in Gaussian Bayesian are

.. math::

   \mathbf{\Phi}^{T}\mathbf{w}^{*}\mathbf{=}\mathbf{\Phi}^{T}\mathbf{\mu}_{\mathbf{w}|\mathbf{y}}\mathbf{=}\mathbf{V}\mathbf{\Lambda}^{\frac{1}{2}}\boxed{\mathbf{U}^{T}\mathbf{U}}{\beta\left( \alpha\mathbf{I} + \beta\mathbf{\Lambda} \right)}^{- 1}\boxed{\mathbf{U}^{T}\mathbf{U}}\mathbf{\Lambda}^{\frac{1}{2}}\mathbf{V}^{T}\mathbf{y}\mathbf{=}\mathbf{V}\begin{pmatrix}
   \frac{\beta\lambda_{1}}{\alpha + \beta\lambda_{1}} & & \\
    & \ddots & \\
    & & \frac{\beta\lambda_{M}}{\alpha + \beta\lambda_{M}} \\
   \end{pmatrix}\mathbf{V}^{T}\mathbf{y}

where
:math:`\mathbf{V =}\mathbf{\Lambda}^{- \frac{1}{2}}\mathbf{U}^{T}\mathbf{\Phi}`
is rotated and scaled data, and
:math:`\widetilde{\mathbf{\mu}_{\mathbf{w}|\mathbf{y}}} = \mathbf{\Lambda}^{\frac{1}{2}}\mathbf{U}^{T}\mathbf{w} = \begin{pmatrix}
\frac{\beta\lambda_{1}}{\alpha + \beta\lambda_{1}} & & \\
 & \ddots & \\
 & & \frac{\beta\lambda_{M}}{\alpha + \beta\lambda_{M}} \\
\end{pmatrix}\mathbf{V}^{T}\mathbf{y}` is with the same rotation
:math:`\mathbf{U}^{T}` but different scaling for interpretation
convenience. Note the model assumes
:math:`\mathbf{\mu}_{\mathbf{w}}\mathbf{= 0 \Rightarrow}\mathbf{U}^{T}\mathbf{w}\mathbf{= 0}`;
and
:math:`\frac{\beta\lambda_{1}}{\alpha + \beta\lambda_{1}} \in \left\lbrack 0,1 \right\rbrack`.

Our interpretation is w.r.t. rotation under rotation
:math:`\mathbf{U}^{T}`. When :math:`\lambda_{i} \approx 0`, then
:math:`\widetilde{\mathbf{\mu}_{\mathbf{w}|\mathbf{y}}}\left( i \right)\mathbf{\approx}0`,
and thus the data does not provide sufficient information for the
:math:`i`\ th weight. When
:math:`\widetilde{\mathbf{\mu}_{\mathbf{w}|\mathbf{y}}}` is large s.t.
:math:`\frac{\beta\lambda_{i}}{\alpha + \beta\lambda_{i}} \approx 1`,
then the :math:`i`\ th weight is almost determined only by the data
(:math:`\lambda_{i},\mathbf{V},\mathbf{y}` all come from the data) and
has little to do with the predefined :math:`\alpha`. As a result, we
*interpret*
:math:`\mathcal{M =}\sum_{i = 1}^{M}\frac{\beta\lambda_{i}}{\alpha + \beta\lambda_{i}}`
as the number of effective weights.

If we add :math:`N^{(1)}` new data
:math:`\mathbf{X}_{m \times N^{\left( 1 \right)}}^{\left( 1 \right)}` to
:math:`\mathbf{X}` and denote
:math:`\mathbf{\Phi}_{M \times N^{\left( 1 \right)}}^{\left( 1 \right)}\mathbf{= \phi}\left( \mathbf{X}^{\left( 1 \right)} \right)`,
then we will take eigenvalues
:math:`\lambda_{1}^{\left( 1 \right)},\ldots,\lambda_{n}^{\left( 1 \right)}`
of
:math:`\mathbf{\Phi}\mathbf{\Phi}^{T} + \mathbf{\Phi}_{1}^{\left( 1 \right)}\left( \mathbf{\Phi}_{1}^{\left( 1 \right)} \right)^{T}`.
In practice, it is usually the case that
:math:`\lambda_{1}^{\left( 1 \right)} > \lambda_{1},\ldots,\lambda_{n}^{\left( 1 \right)} > \lambda_{n}`
when :math:`N^{\left( 1 \right)}` is sufficiently large; or in other
words, empirically all eigenvalues will be large with sufficient many
data (:math:`N \gg M`), and in this case :math:`\mathcal{M \approx}M`,
and the choice of parameters become

.. math::

   \left\{ \begin{matrix}
   \alpha \approx \frac{M}{\mathbf{\mu}_{\mathbf{w}|\mathbf{y}}^{T}\mathbf{\mu}_{\mathbf{w}|\mathbf{y}}} \\
   \beta \approx \frac{N - M}{\left\| \mathbf{\epsilon} \right\|_{2}^{2}} \approx \frac{N}{\left\| \mathbf{\epsilon} \right\|_{2}^{2}} \\
   \end{matrix} \right.\

And we only need to iteratively compute a fixed point for
:math:`\alpha`.

EX 1. Solve for weighted LSE problem
:math:`\arg{\operatorname{}\left\| \mathbf{\Lambda}^{\frac{1}{2}}\left( \mathbf{y}\mathbf{-}\mathbf{\Phi}^{T}\mathbf{w} \right) \right\|_{2}^{2}}`
where :math:`\mathbf{\Lambda}` is a diagonal matrix consisting of
weights. This is later applied in logistic regression (2‑55).

Key. Let
:math:`\widetilde{\mathbf{y}} = \mathbf{\Lambda}^{\frac{1}{2}}\mathbf{y}`
and
:math:`{\widetilde{\mathbf{\Phi}}}^{T}\mathbf{=}\mathbf{\Lambda}^{\frac{1}{2}}\mathbf{\Phi}^{T}`,
then by (2‑7)
:math:`\mathbf{w}^{\mathbf{*}}\mathbf{=}\left( \widetilde{\mathbf{\Phi}}{\widetilde{\mathbf{\Phi}}}^{T} \right)^{\dagger}\widetilde{\mathbf{\Phi}}\widetilde{\mathbf{y}}\mathbf{=}\left( \mathbf{\Phi}\mathbf{\Lambda}\mathbf{\Phi}^{T} \right)^{\dagger}\mathbf{\Phi}\mathbf{\Lambda}\mathbf{y}`.

**Linear Discriminant Analysis**
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

**Discriminant analysis** or **classification**, can be viewed as a
special type of regression, sometimes named **categorical regression**.
Still, we are given the data set
:math:`\left( \mathbf{X},\mathbf{y} \right)` where data matrix
:math:`\mathbf{X} = \left( \mathbf{x}_{1},\ldots,\mathbf{x}_{N} \right) \in \mathbb{R}^{M \times N}`,
but
:math:`\mathbf{y =}\left( y_{1},\ldots,y_{N} \right) \in \mathbb{L}^{N},\mathbb{L \subseteq N}`,
where :math:`\mathbb{L}` is a finite set of discrete **class labels**,
and we name its size :math:`K = \left| \mathbb{L} \right|` as the
**class size**. We *note* the labels are **categorical** and for
distinction of groups of data; although labels are represented by
integers, their numerical values do not really matter. For example, the
whole data set still makes sense if the chosen labels in
:math:`\mathbb{L}` are replaced by other distinct integers. The
discriminant analysis finds a **discriminant function**
:math:`\mathcal{f:}\mathbb{R}^{M}\mathbb{\rightarrow L}` and
:math:`\widetilde{\mathbf{y}} = \left( {\widetilde{y}}_{1},\ldots,{\widetilde{y}}_{N} \right) = \mathcal{f}\left( \mathbf{X} \right)`
gives estimation of the true labels :math:`\mathbf{y}`. The most basic
type is the **binary classification** when :math:`K = 2`. For a **future
variable** :math:`\mathcal{x}`, :math:`\mathcal{f}` provides
**prediction**
:math:`\mathcal{y}\mathcal{= f}\left( \mathcal{x} \right)`.

We again emphasize the labels mainly serve the purpose of distinction
between groups of data; *however*, it is not uncommon the labels could
also serve technical or interpretive purposes. For example, 1) in
perceptron algorithm, the label set is designed as
:math:`\left\{ - 1,1 \right\}` for its error function; 2) Fisher’s
binary linear discriminant in *Theorem 2‑6* can be shown to be
equivalent to LSE if using label set
:math:`\left\{ - \frac{N}{N_{0}},\frac{N}{N_{1}} \right\}`; 3) in
**sentiment analysis**, :math:`- 1` is usually to label the class of
negative sentiment, and :math:`+ 1` is used to label the class of
positive sentiment. The design of label set is called **label**
**coding**.

As before, there can be feature function :math:`\phi` applied to each
:math:`\mathbf{x}`, and we are actually dealing with
:math:`\mathbf{\phi} = \phi\left( \mathbf{x} \right)`. The basic idea is
to utilize the regression in (2‑2), and write the discriminant as

+-----------------------+-----------------------+-----------------------+
| .. math:: \mathcal{f} |                       | (2‑27)                |
| \left( \mathbf{x} \ri |                       |                       |
| ght) = f\left( \phi^{ |                       |                       |
| T}\left( \mathbf{X} \ |                       |                       |
| right)\mathbf{w} \rig |                       |                       |
| ht) = f\left( \mathbf |                       |                       |
| {\phi}^{T}\mathbf{w}  |                       |                       |
| \right)               |                       |                       |
+-----------------------+-----------------------+-----------------------+

where :math:`f` is to convert the decimal value resulting from
:math:`\mathbf{\phi}^{T}\mathbf{w}` to a label, and it is very often
referred to as **activation function**. Discriminant analysis based on
(2‑27) is called **linear discriminant analysis**. For example, suppose
we have binary label set
:math:`\mathbb{L =}\left\{ l_{1},l_{2} \right\}`, then we can define

+-----------------------+-----------------------+-----------------------+
| .. math::             |                       | (2‑28)                |
|                       |                       |                       |
|    \widetilde{y}\math |                       |                       |
| cal{= f}\left( \mathb |                       |                       |
| f{x} \right) = \left\ |                       |                       |
| { \begin{matrix}      |                       |                       |
|    1 & \phi^{T}\left( |                       |                       |
|  \mathbf{x} \right)\m |                       |                       |
| athbf{w} \geq 0 \\    |                       |                       |
|    0 & \phi^{T}\left( |                       |                       |
|  \mathbf{x} \right)\m |                       |                       |
| athbf{w <}0 \\        |                       |                       |
|    \end{matrix} \righ |                       |                       |
| t.\                   |                       |                       |
+-----------------------+-----------------------+-----------------------+

For the special case where the bias :math:`w_{0}` is explicit, we can
write

+-----------------------+-----------------------+-----------------------+
| .. math:: \mathcal{f} |                       | (2‑29)                |
| \left( \mathbf{x} \ri |                       |                       |
| ght) = f\left( \phi^{ |                       |                       |
| T}\left( \mathbf{X} \ |                       |                       |
| right)\mathbf{w +}w_{ |                       |                       |
| 0} \right) = f\left(  |                       |                       |
| \mathbf{\phi}^{T}\mat |                       |                       |
| hbf{w +}w_{0} \right) |                       |                       |
+-----------------------+-----------------------+-----------------------+

which can be viewed as classification in :math:`\mathbb{R}^{M + 1}`
dimension consistent with (2‑27), putting all feature points at height
:math:`1` in the first dimension.

For error, there could be a variety of different definitions.
Considering the categorical nature of labels, one intuitive error
definition is to count the differences between
:math:`\widetilde{\mathbf{y}}\mathcal{= f}\left( \mathbf{X} \right)` and
:math:`\mathbf{y}`:
:math:`\mathcal{e}\left( \widetilde{\mathbf{y}} \right) = \sum_{i = 1}^{N}1_{y_{i} = {\widetilde{y}}_{i}}`,
which we refer to as **identity error**. Identity error is the most
widely used performance indicator, but it is hard to optimize (it is
highly piecewise, and its derivative is zero almost everywhere).

Perceptron Algorithm
''''''''''''''''''''

-  **Perceptron Algorithm** could be the simplest algorithm for binary
   classification and can be viewed as a simplest one-layer **neural
   network** with one output. Technically, it tries to minimize a more
   numerical error rather than the identity error. To achieve this, it
   designs the label set as :math:`\left\{ - 1,1 \right\}`. For each
   data point :math:`\mathbf{x}_{i}` and label :math:`y_{i}`, the
   algorithm checks if
   :math:`\operatorname{sgn}\left( \mathbf{\phi}_{i}^{T}\mathbf{w} \right) = y_{i}`;
   if so, the classification is correct; otherwise, the classification
   is wrong, and we calculate the **perceptron error** as the following,

.. math:: \mathcal{e}_{p} = \sum_{\operatorname{sgn}\left( \mathbf{\phi}_{i}^{T}\mathbf{w} \right) \neq y_{i}}^{}{- y_{i}\mathbf{\phi}_{i}^{T}\mathbf{w}}

*Note*
:math:`\operatorname{sgn}\left( \mathbf{\phi}_{i}^{T}\mathbf{w} \right) \neq y_{i}`
is equivalent to :math:`y_{i}\mathbf{\phi}_{i}^{T}\mathbf{w <}0`, and
thus we put a minus sign in front of it to make the error positive. Our
objective is now to simply minimize the error, i.e.
:math:`\operatorname{}\mathcal{e}_{p}`. The classic way is to use
gradient descent, with :math:`\eta` as a predefined positive step length

.. math:: \mathbf{w}^{\left( t + 1 \right)}\mathbf{=}\mathbf{w}^{\left( t \right)}\mathbf{-}\eta\nabla_{\mathbf{w}^{\left( t \right)}}\mathcal{e}_{p}

where by *Fact 1‑1*

+-----------------------+-----------------------+-----------------------+
| .. math:: \nabla_{\ma |                       | (2‑30)                |
| thbf{w}^{\left( t \ri |                       |                       |
| ght)}}\mathcal{e}_{p} |                       |                       |
|  = \sum_{\operatornam |                       |                       |
| e{sgn}\left( \mathbf{ |                       |                       |
| \phi}_{i}^{T}\mathbf{ |                       |                       |
| w}^{\left( t \right)} |                       |                       |
|  \right) \neq y_{i}}^ |                       |                       |
| {}{- y_{i}\mathbf{\ph |                       |                       |
| i}_{i}} \Rightarrow \ |                       |                       |
| mathbf{w}^{\left( t + |                       |                       |
|  1 \right)}\mathbf{=} |                       |                       |
| \mathbf{w}^{\left( t  |                       |                       |
| \right)}\mathbf{+}\et |                       |                       |
| a\sum_{\operatorname{ |                       |                       |
| sgn}\left( \mathbf{\p |                       |                       |
| hi}_{i}^{T}\mathbf{w} |                       |                       |
| ^{\left( t \right)} \ |                       |                       |
| right) \neq y_{i}}^{} |                       |                       |
| {y_{i}\mathbf{\phi}_{ |                       |                       |
| i}}                   |                       |                       |
+-----------------------+-----------------------+-----------------------+

The above can be modified as online updates whenever an error is
encountered.

+-----------------------+-----------------------+-----------------------+
| :math:`\mathbf{w}^{\l |                       | (2‑31)                |
| eft( t + 1 \right)}\m |                       |                       |
| athbf{≔}\mathbf{w}^{\ |                       |                       |
| left( t \right)}\math |                       |                       |
| bf{+}\eta y_{i_{t}}\m |                       |                       |
| athbf{\phi}_{i_{t}}`  |                       |                       |
| if                    |                       |                       |
| :math:`\operatorname{ |                       |                       |
| sgn}\left( \mathbf{\p |                       |                       |
| hi}_{i_{t}}^{T}\mathb |                       |                       |
| f{w}^{\left( t \right |                       |                       |
| )} \right) \neq y_{i_ |                       |                       |
| {t}}`,\ :math:`\ i_{t |                       |                       |
| } \in \left\{ 1,\ldot |                       |                       |
| s,N \right\}`         |                       |                       |
+-----------------------+-----------------------+-----------------------+

+-----------------------------------------------------------------------+
| |image1|                                                              |
+=======================================================================+
| *Figure* *2‑1 Perceptron algorithm is to find the best hyperplane     |
| through origin separating the points. With unit normal, the           |
| perceptron error can be interpreted as adding up the distances from   |
| misclassified points to the hyperplane.*                              |
+-----------------------------------------------------------------------+

The *geometric interpretation* is illustrated in *Figure 2‑1*. A
hyperplane :math:`H\left( \mathbf{x} \right) = \mathbf{w}^{T}\mathbf{x}`
goes through the origin and has :math:`\mathbf{w}` as its normal. With a
fixed :math:`\mathbf{w}`,
:math:`\cos\left\langle \mathbf{\phi}_{i}\mathbf{,}\mathbf{w} \right\rangle = \frac{\mathbf{\phi}_{i}^{T}\mathbf{w}}{\left\| \mathbf{w} \right\|_{2}\left\| \mathbf{\phi}_{i} \right\|_{2}}`
will be all positive if :math:`\mathbf{\phi}_{i}` is at one side of
:math:`H` and all negative if at the other side,
:math:`d\left( \mathbf{\phi}_{i}\mathbf{,}H \right) = \frac{\mathbf{\phi}_{i}^{T}\mathbf{w}}{\left\| \mathbf{w} \right\|_{2}}`
is a signed distance from point :math:`\mathbf{\phi}_{i}` to the
hyperplane, and
:math:`\mathbf{\phi}_{i}^{T}\mathbf{w =}\left\| \mathbf{w} \right\|_{2}d\left( \mathbf{\phi}_{i}\mathbf{,}H \right)\mathbf{\propto}d\left( \mathbf{\phi}_{i}\mathbf{,}H \right)`.
The interpretation can be clearer if we restrict
:math:`\left\| \mathbf{w} \right\|_{2}\mathbf{=}1`, then we have
:math:`\mathbf{\phi}_{i}^{T}\mathbf{w =}d\left( \mathbf{\phi}_{i}\mathbf{,}H \right)`,
and “\ :math:`\mathcal{e}_{p}`\ ” is adding up the distances of
misclassified points to the hyperplane. It is trivial to verify the
optimal solution with or without the constraint are identical. Thus, in
practice, we might just proceed the optimization without the “unit
normal” constraint for simplicity, and normalize the optimal solution
:math:`\mathbf{w}^{*}` at the end. The perceptron objective
:math:`\operatorname{}\mathcal{e}_{p}` can be viewed as finding an
orientation of :math:`H` that best separates the data points, where the
“best” is in terms of minimum “\ :math:`\mathcal{e}_{p}`\ ”. The
*initialization* :math:`\mathbf{w}^{\left( 0 \right)}` for (2‑30) can be
random choice of any nonzero vector. The *advantage* of Perceptron is
its simplicity, its online updates and the guaranteed convergence in
ideal situation; while the *disadvantage* is its limited to binary
classification and it runs indefinitely in non-ideal situation when the
data points are not separable by a hyperplane (in this case a heuristic
rule is needed for termination).

**Fact** **2‑5** **Cauchy-Schwarz inequality** states that
:math:`\left\| \mathbf{x} \right\|_{\langle\rangle}\left\| \mathbf{y} \right\|_{\langle\rangle} \geq \left| \left\langle \mathbf{x},\mathbf{y} \right\rangle \right|`
for any :math:`\mathbf{x},\mathbf{y} \in \mathbb{R}^{n}` where
:math:`\left\langle \mathbf{x},\mathbf{y} \right\rangle` is an inner
product, and
:math:`\left\| \mathbf{x} \right\|_{\langle\rangle} = \sqrt{\left\langle \mathbf{x},\mathbf{x} \right\rangle}`
is the inner product induced norm.

**Theorem** **2‑5** **Perceptron Convergence Theorem**. In binary
classification, if there exists a hyperplane
:math:`H:\mathbf{w}^{T}\mathbf{x} = 0` s.t. the feature points
:math:`\left( \mathbf{\phi}_{1},y_{1} \right),\ldots,\left( \mathbf{\phi}_{N},y_{N} \right)`
satisfy
:math:`\left\{ \mathbf{\phi}_{i}:\mathbf{w}^{T}\mathbf{x} \geq 0 \right\}`
all have the same label and
:math:`\left\{ \mathbf{\phi}_{i}:\mathbf{w}^{T}\mathbf{x} < 0 \right\}`
all have the other label, then the feature points are **linearly
separable**. If the feature points are linearly separable by some
hyperplane :math:`H`, then there exist two constants :math:`a,b` s.t.
:math:`at^{2}\mathbf{\leq}\left\| \mathbf{w}^{\left( t \right)}\mathbf{-}\mathbf{w}^{\left( 0 \right)} \right\|_{2}^{2}\mathbf{\leq}\text{bt}`
for all :math:`t` if :math:`\mathcal{e}_{p} \neq 0` for the algorithm
defined by (2‑31). *As a result*, the algorithm must terminate within
:math:`t \leq \frac{b}{a}` iterations :math:`\mathcal{e}_{p} = 0` at
termination, otherwise :math:`at^{2} > bt` when :math:`t` is
sufficiently large. *For the lower bound*, let :math:`\mathbf{w}^{*}` be
a normal of the separation hyperplane :math:`H`, then

.. math:: \mathbf{w}^{\left( t \right)} = \mathbf{w}^{\left( 0 \right)} + \eta y_{i_{1}}\mathbf{\phi}_{i_{1}}\mathbf{+ \ldots +}\eta y_{i_{t}}\mathbf{\phi}_{i_{t}} \Rightarrow \mathbf{w}^{\left( t \right)} - \mathbf{w}^{\left( 0 \right)} = \eta y_{i_{1}}\mathbf{\phi}_{i_{1}}\mathbf{+ \ldots +}\eta y_{i_{t}}\mathbf{\phi}_{i_{t}} \Rightarrow \left( \mathbf{w}^{*} \right)^{T}\left( \mathbf{w}^{\left( t \right)} - \mathbf{w}^{\left( 0 \right)} \right) = \left( \mathbf{w}^{*} \right)^{T}\left( \eta y_{i_{1}}\mathbf{\phi}_{i_{1}}\mathbf{+ \ldots +}\eta y_{i_{t}}\mathbf{\phi}_{i_{t}} \right) \geq \text{ηt}\operatorname{}{y_{i}\left( \mathbf{w}^{*} \right)^{T}\mathbf{\phi}_{i}}

By Cauchy-Schwarz inequality,
:math:`\left| \left( \mathbf{w}^{*} \right)^{T}\left( \mathbf{w}^{\left( t \right)} - \mathbf{w}^{\left( 0 \right)} \right) \right| \leq \left\| \mathbf{w}^{*} \right\|_{2}\left\| \mathbf{w}^{\left( t \right)} - \mathbf{w}^{\left( 0 \right)} \right\|_{2}`,
therefore

.. math:: \left\| \mathbf{w}^{*} \right\|_{2}\left\| \mathbf{w}^{\left( t \right)} - \mathbf{w}^{\left( 0 \right)} \right\|_{2} \geq \text{ηt}\operatorname{}{y_{i}\left( \mathbf{w}^{*} \right)^{T}\mathbf{\phi}_{i}} \Rightarrow \left\| \mathbf{w}^{\left( t \right)} - \mathbf{w}^{\left( 0 \right)} \right\|_{2} \geq \frac{\eta\operatorname{}{y_{i}\left( \mathbf{w}^{*} \right)^{T}\mathbf{\phi}_{i}}}{\left\| \mathbf{w}^{*} \right\|_{2}}t

where
:math:`a = \frac{\eta\operatorname{}{y_{i}\left( \mathbf{w}^{*} \right)^{T}\mathbf{\phi}_{i}}}{\left\| \mathbf{w}^{*} \right\|_{2}}`
is independent of :math:`t`. *For the upper bound*, notice

.. math::

   {\mathbf{w}^{\left( 1 \right)}\mathbf{-}\mathbf{w}^{\left( 0 \right)}\mathbf{=}\eta y_{i_{1}}\mathbf{\phi}_{i_{1}}
   }{\mathbf{w}^{\left( 2 \right)}\mathbf{-}\mathbf{w}^{\left( 0 \right)}\mathbf{=}\eta y_{i_{1}}\mathbf{\phi}_{i_{1}}\mathbf{+}\eta y_{i_{2}}\mathbf{\phi}_{i_{2}}\mathbf{=}\left( \mathbf{w}^{\left( 1 \right)}\mathbf{-}\mathbf{w}^{\left( 0 \right)} \right) + \eta y_{i_{2}}\mathbf{\phi}_{i_{2}}
   }{\mathbf{w}^{\left( 3 \right)}\mathbf{-}\mathbf{w}^{\left( 0 \right)}\mathbf{=}\eta y_{i_{1}}\mathbf{\phi}_{i_{1}}\mathbf{+}\eta y_{i_{2}}\mathbf{\phi}_{i_{2}}\mathbf{+}\eta y_{i_{3}}\mathbf{\phi}_{i_{3}}\mathbf{=}\boxed{\left( \mathbf{w}^{\left( 1 \right)}\mathbf{-}\mathbf{w}^{\left( 0 \right)} \right)} + \left( \mathbf{w}^{\left( 2 \right)}\mathbf{-}\mathbf{w}^{\left( 0 \right)} \right) - \boxed{\left( \mathbf{w}^{\left( 1 \right)}\mathbf{-}\mathbf{w}^{\left( 0 \right)} \right)} + \eta y_{i_{3}}\mathbf{\phi}_{i_{3}}\mathbf{=}\left( \mathbf{w}^{\left( 2 \right)}\mathbf{-}\mathbf{w}^{\left( 0 \right)} \right) + \eta y_{i_{3}}\mathbf{\phi}_{i_{3}}}

and generally

.. math:: \mathbf{w}^{\left( t \right)}\mathbf{-}\mathbf{w}^{\left( 0 \right)}\mathbf{=}\left( \mathbf{w}^{\left( t - 1 \right)}\mathbf{-}\mathbf{w}^{\left( 0 \right)} \right) + \eta y_{i_{t}}\mathbf{\phi}_{i_{t}}

and

.. math:: \left\| \mathbf{w}^{\left( t \right)}\mathbf{-}\mathbf{w}^{\left( 0 \right)} \right\|_{2}^{2} = \left\| \mathbf{w}^{\left( t - 1 \right)}\mathbf{-}\mathbf{w}^{\left( 0 \right)} \right\|_{2}^{2} + \eta^{2}\left\| \mathbf{\phi}_{i_{t}} \right\|_{2}^{2}\mathbf{+}2\eta y_{i_{t}}\left( \mathbf{w}^{\left( t - 1 \right)}\mathbf{-}\mathbf{w}^{\left( 0 \right)} \right)^{T}\mathbf{\phi}_{i_{t}}

Note
:math:`y_{i_{t}}\left( \mathbf{w}^{\left( t - 1 \right)} \right)^{T}\mathbf{\phi}_{i_{t}}\mathbf{<}0`
because we assume :math:`\mathcal{e}_{p} \neq 0` at step :math:`t`.
Therefore,

.. math:: \left\| \mathbf{w}^{\left( t \right)}\mathbf{-}\mathbf{w}^{\left( 0 \right)} \right\|_{2}^{2} \leq \left\| \mathbf{w}^{\left( t - 1 \right)}\mathbf{-}\mathbf{w}^{\left( 0 \right)} \right\|_{2}^{2} + \eta^{2}\left\| \mathbf{\phi}_{i_{t}} \right\|_{2}^{2}\mathbf{-}2\eta y_{i_{t}}\left( \mathbf{w}^{\left( 0 \right)} \right)^{T}\mathbf{\phi}_{i_{t}}

which gives

.. math::

   {\left\| \mathbf{w}^{\left( 1 \right)}\mathbf{-}\mathbf{w}^{\left( 0 \right)} \right\|_{2}^{2} = \eta^{2}\left\| \mathbf{\phi}_{i_{1}} \right\|_{2}
   }{\left\| \mathbf{w}^{\left( 2 \right)}\mathbf{-}\mathbf{w}^{\left( 0 \right)} \right\|_{2}^{2} \leq \left\| \mathbf{w}^{\left( 1 \right)}\mathbf{-}\mathbf{w}^{\left( 0 \right)} \right\|_{2}^{2} + \eta^{2}\left\| \mathbf{\phi}_{i_{2}} \right\|_{2}^{2}\mathbf{-}2\eta y_{i_{2}}\left( \mathbf{w}^{\left( 0 \right)} \right)^{T}\mathbf{\phi}_{i_{2}}
   }{\ldots
   }{\left\| \mathbf{w}^{\left( t \right)}\mathbf{-}\mathbf{w}^{\left( 0 \right)} \right\|_{2}^{2} \leq \left\| \mathbf{w}^{\left( t - 1 \right)}\mathbf{-}\mathbf{w}^{\left( 0 \right)} \right\|_{2}^{2} + \eta^{2}\left\| \mathbf{\phi}_{i_{t}} \right\|_{2}^{2}\mathbf{-}2\eta y_{i_{t}}\left( \mathbf{w}^{\left( 0 \right)} \right)^{T}\mathbf{\phi}_{i_{t}}}

Adding up these inequalities, we can cancel terms on both sizes as the
following,

.. math::

   {\boxed{\left\| \mathbf{w}^{\left( 1 \right)}\mathbf{-}\mathbf{w}^{\left( 0 \right)} \right\|_{2}^{2}} = \eta^{2}\left\| \mathbf{\phi}_{i_{1}} \right\|_{2}
   }{\boxed{\left\| \mathbf{w}^{\left( 2 \right)}\mathbf{-}\mathbf{w}^{\left( 0 \right)} \right\|_{2}^{2}} \leq \boxed{\left\| \mathbf{w}^{\left( 1 \right)}\mathbf{-}\mathbf{w}^{\left( 0 \right)} \right\|_{2}^{2}} + \eta^{2}\left\| \mathbf{\phi}_{i_{2}} \right\|_{2}^{2}\mathbf{-}2\eta y_{i_{2}}\left( \mathbf{w}^{\left( 0 \right)} \right)^{T}\mathbf{\phi}_{i_{2}}
   }{\ldots
   }{\left\| \mathbf{w}^{\left( t \right)}\mathbf{-}\mathbf{w}^{\left( 0 \right)} \right\|_{2}^{2} \leq \boxed{\left\| \mathbf{w}^{\left( t - 1 \right)}\mathbf{-}\mathbf{w}^{\left( 0 \right)} \right\|_{2}^{2}} + \eta^{2}\left\| \mathbf{\phi}_{i_{t}} \right\|_{2}^{2}\mathbf{-}2\eta y_{i_{t}}\left( \mathbf{w}^{\left( 0 \right)} \right)^{T}\mathbf{\phi}_{i_{t}}}

and therefore

.. math:: \left\| \mathbf{w}^{\left( t \right)}\mathbf{-}\mathbf{w}^{\left( 0 \right)} \right\|_{2}^{2} \leq \eta^{2}\left( \left\| \mathbf{\phi}_{i_{1}} \right\|_{2}^{2}\mathbf{+ \ldots +}\left\| \mathbf{\phi}_{i_{t}} \right\|_{2}^{2} \right)\mathbf{-}2\mathbf{\eta}\left( \mathbf{w}^{\left( 0 \right)} \right)^{T}\left( y_{i_{2}}\mathbf{\phi}_{i_{2}}\mathbf{+ \ldots +}y_{i_{t}}\mathbf{\phi}_{i_{t}} \right)\mathbf{\leq}\eta^{2}\left( \operatorname{}\left\| \mathbf{\phi}_{i_{t}} \right\|_{2}^{2} \right)t - 2\eta\left( \operatorname{}{y_{i_{t}}\left( \mathbf{w}^{\left( 0 \right)} \right)^{T}\mathbf{\phi}_{i_{t}}} \right)t

where we see
:math:`b = \eta^{2}\left( \operatorname{}\left\| \mathbf{\phi}_{i_{t}} \right\|_{2}^{2} \right) - 2\eta\left( \operatorname{}{y_{i_{t}}\left( \mathbf{w}^{\left( 0 \right)} \right)^{T}\mathbf{\phi}_{i_{t}}} \right)`,
independent of :math:`t`.

Membership LSE
''''''''''''''

-  **Theorem** **2‑6**\ **Membership LSE**. Least squared error method
   for regression (see *Theorem 2‑1*) can be adapted for classification;
   however, direct use the categorical values of the class labels as
   regression target is usually less favorable due to both performance
   issues and interpretation difficulty.

   One adaption is to convert class labels into membership vectors.
   Define a new target vector :math:`\mathbf{z}_{j}`, roughly named the
   **membership vector**, for each class label
   :math:`y_{j},j = 1,\ldots,N`

   :math:`\mathbf{z}_{j}\mathbf{=}\begin{pmatrix}
   t_{1,j} \\
   \mathbf{\vdots} \\
   t_{K,j} \\
   \end{pmatrix} \in \left\{ 0,1 \right\}^{K},j = 1,\ldots,N` s.t.
   :math:`t_{i,j} = 1_{y_{j} = i}` for :math:`i = 1,\ldots,K`

   That is, only one component of :math:`\mathbf{z}_{j}` is assigned
   :math:`1` with all other components being :math:`0`. If
   :math:`t_{i,j} = 1` for some :math:`i`, then :math:`y_{j} = i`,
   indicating :math:`\mathbf{x}_{j} \in C_{i}`. Then the whole target
   matrix is
   :math:`\mathbf{Z} = \left( \mathbf{z}_{1},\ldots,\mathbf{z}_{N} \right)`.
   By (2‑4) we have the regression model
   :math:`\widehat{\mathbf{Z}}\mathbf{=}\mathbf{W}^{T}\mathbf{\Phi}`,
   and the objective of LSE is to minimize
   :math:`\left\| \mathbf{Z} - \widehat{\mathbf{Z}} \right\|_{F}^{2}`.
   We have the solution by (2‑6) and (2‑7) of *Theorem 2‑1* that

.. math:: \mathbf{W}^{\mathbf{*}}\mathbf{=}\left( \mathbf{\Phi}^{T} \right)^{\dagger}\mathbf{Z}^{T}

Using the fact that
:math:`\left( \mathbf{\Phi}^{T} \right)^{\dagger} = \left( \mathbf{\Phi}^{\mathbf{\dagger}} \right)^{T}`,
we have

.. math:: \mathbf{W}^{\mathbf{*}}\mathbf{=}\left( \mathbf{\Phi}^{\dagger} \right)^{T}\mathbf{Z}^{T}\mathbf{\Rightarrow}\left( \mathbf{W}^{\mathbf{*}} \right)^{T}\mathbf{= Z}\mathbf{\Phi}^{\dagger}

And the regression function is

.. math:: f\left( \mathbf{x} \right) = \mathbf{Z}\mathbf{\Phi}^{\dagger}\phi\left( \mathbf{x} \right)

Note the discriminant function is different, which needs to yield a
class label. We heuristically set it as

+-----------------------+-----------------------+-----------------------+
| .. math:: \mathcal{f} |                       | (2‑29)                |
| \left( \mathbf{x} \ri |                       |                       |
| ght) = \max{f\left( \ |                       |                       |
| mathbf{x} \right)}    |                       |                       |
+-----------------------+-----------------------+-----------------------+

As a special case, if :math:`\mathbf{\Phi} ≔ \begin{pmatrix}
\mathbf{1}^{T} \\
\mathbf{X} \\
\end{pmatrix}` and :math:`\mathbf{W} ≔ \begin{pmatrix}
\mathbf{w}_{0}^{T} \\
\mathbf{W} \\
\end{pmatrix}`, then by (2‑11) of *Theorem 2‑1*, we have

.. math::

   \left\{ \begin{matrix}
   \mathbf{w}_{0}^{*} = \overline{\mathbf{z}} - \left( \mathbf{W}^{\mathbf{*}} \right)^{T}\overline{\mathbf{x}} \\
   \mathbf{W}^{*} = \left( {\widetilde{\mathbf{X}}}^{\dagger} \right)^{T}{\widetilde{\mathbf{Z}}}^{T}\mathbf{,}\left( \mathbf{W}^{\mathbf{*}} \right)^{T}\mathbf{=}\widetilde{\mathbf{Z}}{\widetilde{\mathbf{X}}}^{\dagger} \\
   \end{matrix} \right.\

where :math:`\overline{\mathbf{x}},\overline{\mathbf{z}}` are the mean
vector of :math:`\mathbf{X},\mathbf{Z}`, and
:math:`\widetilde{\mathbf{X}},\widetilde{\mathbf{Z}}` are centralized
data. Then the regression function is

+-----------------------+-----------------------+-----------------------+
| .. math:: g\left( \ma |                       | (2‑30)                |
| thbf{x} \right) = \le |                       |                       |
| ft( \mathbf{W}^{\math |                       |                       |
| bf{*}} \right)^{T}\ma |                       |                       |
| thbf{x +}\mathbf{w}_{ |                       |                       |
| 0}^{*}\mathbf{=}\left |                       |                       |
| ( \mathbf{W}^{\mathbf |                       |                       |
| {*}} \right)^{T}\math |                       |                       |
| bf{x +}\overline{\mat |                       |                       |
| hbf{z}} - \left( \mat |                       |                       |
| hbf{W}^{\mathbf{*}} \ |                       |                       |
| right)^{T}\overline{\ |                       |                       |
| mathbf{x}} = \widetil |                       |                       |
| de{\mathbf{Z}}{\widet |                       |                       |
| ilde{\mathbf{X}}}^{\d |                       |                       |
| agger}\widetilde{\mat |                       |                       |
| hbf{x}}\mathbf{+}\ove |                       |                       |
| rline{\mathbf{z}}     |                       |                       |
+-----------------------+-----------------------+-----------------------+

There are many *limitations* of LSE for classification. It inherits the
sensitivity to outliers as mentioned in *Theorem 2‑1*; secondly, the
efficiency of LSE quickly degenerates when :math:`K` increases; thirdly,
the discriminant decision is made based on the maximum of the inferred
membership weights of :math:`\mathbf{z}`, but we lack good
interpretation of :math:`\mathbf{z}`; one example for (2‑30) is shown in
the following property.

**Property** **2‑1** If :math:`\mathbf{a}^{T}\mathbf{z}_{j} = b` for
every :math:`j = 1,\ldots,N`, then we have
:math:`\mathbf{a}^{T}g\left( \mathbf{x} \right) = b`. Let
:math:`\mathbf{b} = b\mathbf{1}`, then

.. math:: \mathbf{a}^{T}\overline{\mathbf{z}} = \frac{1}{N}\mathbf{a}^{T}\left( \mathbf{z}_{1} + \ldots + \mathbf{z}_{N} \right) = b \Rightarrow \mathbf{a}^{T}\overline{\mathbf{Z}} = \mathbf{b}\mathbf{\Rightarrow}\mathbf{a}^{T}\widetilde{\mathbf{Z}}\mathbf{=}\mathbf{a}^{T}\left( \mathbf{Z}\mathbf{-}\overline{\mathbf{Z}} \right)\mathbf{=}\mathbf{b}^{T}\mathbf{-}\mathbf{b}^{T}\mathbf{= 0}\mathbf{\Rightarrow}\mathbf{a}^{T}g\left( \mathbf{x} \right) = \mathbf{a}^{T}\left( \widetilde{\mathbf{Z}}{\widetilde{\mathbf{X}}}^{\dagger}\widetilde{\mathbf{x}}\mathbf{+}\overline{\mathbf{z}} \right) = \mathbf{a}^{T}\overline{\mathbf{z}} = b

In particular, if we let :math:`\mathbf{a} = \mathbf{1}`, then
:math:`b = 1`, and :math:`\mathbf{1}^{T}\mathbf{z}_{j} = 1` holds for
every :math:`j = 1,\ldots,N`. Therefore
:math:`\mathbf{1}^{T}g\left( \mathbf{x} \right) = 1` for any
:math:`\mathbf{x}`, i.e. the inferred membership of any data entry
:math:`\mathbf{x}` under :math:`g` in (2‑30) sums to 1. However, we
*note* it is not guaranteed :math:`\mathbf{z}_{j}` is a probabilistic
vector under, i.e. it may have components not in
:math:`\left\lbrack 0,1 \right\rbrack`, which is one limitation of LSE.

-  **Theorem** **2‑6**\ **Fisher’s Binary Linear Discriminant**. For
   simplicity and WLOG, we consider
   :math:`\phi\left( \mathbf{x} \right) ≔ \begin{pmatrix}
   1 \\
   \mathbf{x} \\
   \end{pmatrix}` and :math:`\mathbf{w} ≔ \begin{pmatrix}
   w_{0} \\
   \mathbf{w} \\
   \end{pmatrix}`, and we start with binary classification. By (2‑28),
   the general form of a binary discriminant is

.. math::

   \mathcal{f}\left( \mathbf{x} \right) = \left\{ \begin{matrix}
   1 & \mathbf{x}^{T}\mathbf{w} \geq - w_{0} \\
   0 & \mathbf{x}^{T}\mathbf{w} < - w_{0} \\
   \end{matrix} \right.\ \

The *idea* is those :math:`\mathcal{f}\left( \mathbf{x}^{0} \right)` for
:math:`\mathbf{x}^{0} \in C_{0}` should be as far away from those
:math:`\mathcal{f}\left( \mathbf{x}^{1} \right)` for
:math:`\mathbf{x}^{1} \in C_{1}`. The distance between two sets of data
entries can be characterized by the mean of the data.

.. math:: {\overline{\mathbf{x}}}^{i} = \frac{1}{N_{i}}\sum_{\mathbf{x} \in C_{i}}^{}\mathbf{x},i = 0,1

We would like to maximize

+-----------------------+-----------------------+-----------------------+
| .. math:: \operatorna |                       | (2‑31)                |
| me{}\left( \mathbf{w} |                       |                       |
| ^{T}{\overline{\mathb |                       |                       |
| f{x}}}^{1} - \mathbf{ |                       |                       |
| w}^{T}{\overline{\mat |                       |                       |
| hbf{x}}}^{0} \right)  |                       |                       |
| = \operatorname{}{\ma |                       |                       |
| thbf{w}^{T}\left( {\o |                       |                       |
| verline{\mathbf{x}}}^ |                       |                       |
| {1} - {\overline{\mat |                       |                       |
| hbf{x}}}^{0} \right)} |                       |                       |
+-----------------------+-----------------------+-----------------------+

where the normalization :math:`\left\| \mathbf{w} \right\|_{2} = 1` is
to prevent the objective from going to infinity. A less obvious issue is
that when :math:`\mathbf{w}^{T}C_{0}` or :math:`\mathbf{w}^{T}C_{1}` or
both have large variance, then
:math:`\mathbf{w}^{T}\left( {\overline{\mathbf{x}}}^{0} - {\overline{\mathbf{x}}}^{1} \right)`
could be large but :math:`\mathbf{w}^{T}C_{0}` or
:math:`\mathbf{w}^{T}C_{1}` may have considerable overlap. The variance
is written as the following,

.. math:: s_{i}^{2} = \sum_{\mathbf{x} \in C_{i}}^{}{\left( \mathbf{w}^{T}\left( \mathbf{x -}{\overline{\mathbf{x}}}^{i} \right) \right)\left( \mathbf{w}^{T}\left( \mathbf{x -}{\overline{\mathbf{x}}}^{i} \right) \right)^{T}},i = 0,1

Therefore, we would like to minimize the variance of
:math:`\mathbf{w}^{T}C_{0}` and :math:`\mathbf{w}^{T}C_{1}` as well.

+---------------------------------------------------------------+--+--------+
| .. math:: \operatorname{}\left( s_{0}^{2} + s_{1}^{2} \right) |  | (2‑32) |
+---------------------------------------------------------------+--+--------+

Therefore, combining (2‑31) and (2‑32) we give the **Fisher’s
criterion** as the squared distance of mean over the sum of variance,

.. math:: \max_{\left\| \mathbf{w} \right\|_{2} = 1}L = \operatorname{}\frac{\left( \mathbf{w}^{T}\left( {\overline{\mathbf{x}}}^{1} - {\overline{\mathbf{x}}}^{0} \right) \right)^{2}}{s_{0}^{2} + s_{1}^{2}} = \operatorname{}\frac{\mathbf{w}^{T}\mathbf{\text{Mw}}}{\mathbf{w}^{T}\left( \mathbf{S}_{0}\mathbf{+}\mathbf{S}_{1} \right)\mathbf{w}}

where
:math:`\mathbf{M =}\left( {\overline{\mathbf{x}}}^{1} - {\overline{\mathbf{x}}}^{0} \right)\left( {\overline{\mathbf{x}}}^{1} - {\overline{\mathbf{x}}}^{0} \right)^{T}`
and
:math:`\mathbf{S}_{i}\mathbf{=}\sum_{\mathbf{x} \in C_{i}}^{}{\left( \mathbf{x -}{\overline{\mathbf{x}}}^{i} \right)\left( \mathbf{x -}{\overline{\mathbf{x}}}^{i} \right)^{T}}`
is the covariance matrix of class :math:`C_{i}` for :math:`i = 0,1`, and
all these matrices are symmetric. Then, let
:math:`\mathbf{S} = \mathbf{S}_{0}\mathbf{+}\mathbf{S}_{1}` for
convenience,

.. math:: \nabla L \propto \left( \mathbf{w}^{T}\mathbf{\text{Sw}} \right)\mathbf{\text{Mw}}\mathbf{-}\left( \mathbf{w}^{T}\mathbf{\text{Mw}} \right)\left( \mathbf{\text{Sw}} \right)\mathbf{=}0\mathbf{\Rightarrow}\left( \mathbf{w}^{T}\mathbf{\text{Sw}} \right)\mathbf{\text{Mw}}\mathbf{=}\left( \mathbf{w}^{T}\mathbf{\text{Mw}} \right)\left( \mathbf{\text{Sw}} \right)

We *note* there is no need for Lagrange multiplier for the constraint;
if we can find the direction of optimal :math:`\mathbf{w}`, we just
normalize it to unit vector. Note
:math:`\mathbf{Mw =}\left( {\overline{\mathbf{x}}}^{1} - {\overline{\mathbf{x}}}^{0} \right)\left( {\overline{\mathbf{x}}}^{1} - {\overline{\mathbf{x}}}^{0} \right)^{T}\mathbf{w}\mathbf{\propto}{\overline{\mathbf{x}}}^{1} - {\overline{\mathbf{x}}}^{0}`,
then f or some constant :math:`c`,

.. math:: \left( \mathbf{w}^{T}\mathbf{\text{Mw}} \right)\left( \mathbf{\text{Sw}} \right)\mathbf{=}c\left( \mathbf{w}^{T}\mathbf{\text{Sw}} \right)\left( {\overline{\mathbf{x}}}^{1} - {\overline{\mathbf{x}}}^{0} \right)\mathbf{\Rightarrow}\mathbf{w =}\frac{c\left( \mathbf{w}^{T}\mathbf{\text{Sw}} \right)}{\mathbf{w}^{T}\mathbf{\text{Mw}}}\mathbf{S}^{- 1}\left( {\overline{\mathbf{x}}}^{1} - {\overline{\mathbf{x}}}^{0} \right)

Since we need to normalize :math:`w` anyway, we do not care the
constant, therefore we have the following **Fisher’s discriminant**.

.. math:: \mathbf{w}^{\mathbf{*}}\mathbf{\propto}\mathbf{S}^{- 1}\left( {\overline{\mathbf{x}}}^{1} - {\overline{\mathbf{x}}}^{0} \right)

**Connection to LSE & Optimal** :math:`w_{0}^{*}`. There are many
strategies to determine :math:`w_{0}`, e.g. one way to do this is to set
:math:`w_{0}` naively as
:math:`w_{0}^{*} = \frac{1}{2}\left( \mathbf{w}^{*} \right)^{T}\left( \mathbf{x}_{1} - \mathbf{x}_{0} \right)`.
One better quantitative approach to find optimal :math:`w_{0}` is
through LSE, by direct regression to class labels (rather than
membership vectors in *Theorem 2‑5*). We show Fisher’s discriminant is
equivalent to a LSE with special label coding, and therefore
:math:`w_{0}` can be found by taking derivative. First, LSE minimizes

.. math:: \sum_{\left( \mathbf{x,}y \right) \in C_{0}}^{}\left( y - w_{0}\mathbf{-}\mathbf{x}^{T}\mathbf{w} \right)^{2} + \sum_{\left( \mathbf{x,}y \right) \in C_{1}}^{}\left( y - w_{0}\mathbf{-}\mathbf{x}^{T}\mathbf{w} \right)^{2} \Rightarrow \sum_{\left( \mathbf{x,}y \right) \in C_{0}}^{}{\left( y - w_{0}\mathbf{-}\mathbf{x}^{T}\mathbf{w} \right)\mathbf{x}} + \sum_{\left( \mathbf{x,}y \right) \in C_{1}}^{}{\left( y - w_{0}\mathbf{-}\mathbf{x}^{T}\mathbf{w} \right)\mathbf{x}} = 0 \Rightarrow \sum_{\left( \mathbf{x,}y \right) \in C_{0}}^{}{\left( w_{0}\mathbf{+}\mathbf{x}^{T}\mathbf{w} \right)\mathbf{x}} + \sum_{\left( \mathbf{x,}y \right) \in C_{1}}^{}{\left( w_{0}\mathbf{+}\mathbf{x}^{T}\mathbf{w} \right)\mathbf{x}} = \sum_{i = 1}^{N}{y_{i}\mathbf{x}} \Rightarrow \sum_{\left( \mathbf{x,}y \right) \in C_{0}}^{}{\mathbf{x}\mathbf{x}^{T}\mathbf{w}} + \sum_{\left( \mathbf{x,}y \right) \in C_{1}}^{}{\mathbf{x}\mathbf{x}^{T}\mathbf{w}} + \sum_{\left( \mathbf{x,}y \right) \in C_{0}}^{}{w_{0}\mathbf{x}} + \sum_{\left( \mathbf{x,}y \right) \in C_{1}}^{}{w_{0}\mathbf{x}} = \sum_{i = 1}^{N}{y_{i}\mathbf{x}_{i}}

Then plug in (2‑9) of *Theorem 2‑1* and have

+-----------------------+-----------------------+-----------------------+
| .. math:: \left( \sum |                       | (2‑33)                |
| _{\left( \mathbf{x,}y |                       |                       |
|  \right) \in C_{0}}^{ |                       |                       |
| }{\mathbf{x}\mathbf{x |                       |                       |
| }^{T}} + \sum_{\left( |                       |                       |
|  \mathbf{x,}y \right) |                       |                       |
|  \in C_{1}}^{}{\mathb |                       |                       |
| f{x}\mathbf{x}^{T}} - |                       |                       |
|  \sum_{\left( \mathbf |                       |                       |
| {x,}y \right) \in C_{ |                       |                       |
| 0}}^{}{\mathbf{x}{\ov |                       |                       |
| erline{\mathbf{x}}}^{ |                       |                       |
| T}} - \sum_{\left( \m |                       |                       |
| athbf{x,}y \right) \i |                       |                       |
| n C_{1}}^{}{\mathbf{x |                       |                       |
| }{\overline{\mathbf{x |                       |                       |
| }}}^{T}} \right)\math |                       |                       |
| bf{w} = \sum_{i = 1}^ |                       |                       |
| {N}\left( y_{i} - \ov |                       |                       |
| erline{y} \right)\mat |                       |                       |
| hbf{x}_{i}            |                       |                       |
+-----------------------+-----------------------+-----------------------+

By Corollary 1‑1, we have

.. math:: \sum_{\mathbf{x} \in C_{0}}^{}{\mathbf{x}\mathbf{x}^{T}} + \sum_{\mathbf{x} \in C_{1}}^{}{\mathbf{x}\mathbf{x}^{T}} = \mathbf{S} + N_{0}{\overline{\mathbf{x}}}^{0}\left( {\overline{\mathbf{x}}}^{0} \right)^{T} + N_{1}{\overline{\mathbf{x}}}^{1}\left( {\overline{\mathbf{x}}}^{1} \right)^{T}

Notice
:math:`\overline{\mathbf{x}}\mathbf{=}\frac{1}{N}\left( N_{0}{\overline{\mathbf{x}}}^{0} + N_{1}{\overline{\mathbf{x}}}^{1} \right)`,
we have

.. math:: \sum_{\mathbf{x} \in C_{0}}^{}{\mathbf{x}{\overline{\mathbf{x}}}^{T}} = \left( \sum_{\mathbf{x} \in C_{0}}^{}{\frac{N_{0}}{N}\mathbf{x}} \right)\left( {\overline{\mathbf{x}}}^{0} \right)^{T} + \frac{N_{1}}{N}\left( \sum_{\mathbf{x} \in C_{0}}^{}\mathbf{x} \right)\left( {\overline{\mathbf{x}}}^{1} \right)^{T} = \frac{N_{0}^{2}}{N}{\overline{\mathbf{x}}}^{0}\left( {\overline{\mathbf{x}}}^{0} \right)^{T} + \frac{N_{0}N_{1}}{N}{\overline{\mathbf{x}}}^{0}\left( {\overline{\mathbf{x}}}^{1} \right)^{T}

.. math:: \sum_{\mathbf{x} \in C_{1}}^{}{\mathbf{x}{\overline{\mathbf{x}}}^{T}} = \left( \sum_{\mathbf{x} \in C_{1}}^{}{\frac{N_{0}}{N}\mathbf{x}} \right)\left( {\overline{\mathbf{x}}}^{0} \right)^{T} + \frac{N_{1}}{N}\left( \sum_{\mathbf{x} \in C_{1}}^{}\mathbf{x} \right)\left( {\overline{\mathbf{x}}}^{1} \right)^{T} = \frac{N_{0}N_{1}}{N}{\overline{\mathbf{x}}}^{1}\left( {\overline{\mathbf{x}}}^{0} \right)^{T} + \frac{N_{1}^{2}}{N}{\overline{\mathbf{x}}}^{1}\left( {\overline{\mathbf{x}}}^{1} \right)^{T}

Then the LHS of (2‑33) becomes

+-----------------------+-----------------------+-----------------------+
| .. math:: \mathbf{S}  |                       | (2‑34)                |
| + N_{0}{\overline{\ma |                       |                       |
| thbf{x}}}^{0}\left( { |                       |                       |
| \overline{\mathbf{x}} |                       |                       |
| }^{0} \right)^{T} + N |                       |                       |
| _{1}{\overline{\mathb |                       |                       |
| f{x}}}^{1}\left( {\ov |                       |                       |
| erline{\mathbf{x}}}^{ |                       |                       |
| 1} \right)^{T} - \lef |                       |                       |
| t( \frac{N_{0}^{2}}{N |                       |                       |
| }{\overline{\mathbf{x |                       |                       |
| }}}^{0}\left( {\overl |                       |                       |
| ine{\mathbf{x}}}^{0}  |                       |                       |
| \right)^{T} + \frac{N |                       |                       |
| _{0}N_{1}}{N}{\overli |                       |                       |
| ne{\mathbf{x}}}^{0}\l |                       |                       |
| eft( {\overline{\math |                       |                       |
| bf{x}}}^{1} \right)^{ |                       |                       |
| T} + \frac{N_{0}N_{1} |                       |                       |
| }{N}{\overline{\mathb |                       |                       |
| f{x}}}^{1}\left( {\ov |                       |                       |
| erline{\mathbf{x}}}^{ |                       |                       |
| 0} \right)^{T} + \fra |                       |                       |
| c{N_{1}^{2}}{N}{\over |                       |                       |
| line{\mathbf{x}}}^{1} |                       |                       |
| \left( {\overline{\ma |                       |                       |
| thbf{x}}}^{1} \right) |                       |                       |
| ^{T} \right) = \mathb |                       |                       |
| f{S} + \frac{N_{0}N_{ |                       |                       |
| 1}}{N}\left( {\overli |                       |                       |
| ne{\mathbf{x}}}^{0}\l |                       |                       |
| eft( {\overline{\math |                       |                       |
| bf{x}}}^{0} \right)^{ |                       |                       |
| T} + {\overline{\math |                       |                       |
| bf{x}}}^{1}\left( {\o |                       |                       |
| verline{\mathbf{x}}}^ |                       |                       |
| {1} \right)^{T} - {\o |                       |                       |
| verline{\mathbf{x}}}^ |                       |                       |
| {0}\left( {\overline{ |                       |                       |
| \mathbf{x}}}^{1} \rig |                       |                       |
| ht)^{T} - {\overline{ |                       |                       |
| \mathbf{x}}}^{1}\left |                       |                       |
| ( {\overline{\mathbf{ |                       |                       |
| x}}}^{1} \right)^{T}  |                       |                       |
| \right)\mathbf{=}\mat |                       |                       |
| hbf{S}\mathbf{+}\frac |                       |                       |
| {N_{0}N_{1}}{N}\mathb |                       |                       |
| f{M}                  |                       |                       |
+-----------------------+-----------------------+-----------------------+

For the RHS of (2‑33), suppose :math:`y_{i} = c_{0}` for all
:math:`\mathbf{x}_{i} \in C_{0}` and :math:`y_{i} = c_{1}` for all
:math:`\mathbf{x}_{i} \in C_{1}`, then

.. math:: \sum_{i = 1}^{N}\left( y_{i} - \overline{y} \right)\mathbf{x}_{i}\mathbf{=}\left( c_{0} - \overline{y} \right)\sum_{\mathbf{x} \in C_{0}}^{}\mathbf{x} + \left( c_{1} - \overline{y} \right)\sum_{\mathbf{x} \in C_{1}}^{}\mathbf{x} = \left( c_{0} - \overline{y} \right)N_{0}{\overline{\mathbf{x}}}^{0} + \left( c_{1} - \overline{y} \right)N_{1}{\overline{\mathbf{x}}}^{1}

As a result, if we let
:math:`\left( c_{1} - \overline{y} \right)N_{1} = - \left( c_{0} - \overline{y} \right)N_{0}`,
then the RHS will be
:math:`{\overline{\mathbf{x}}}^{1} - {\overline{\mathbf{x}}}^{0}` times
a constant. This happens when simply

+-----------------------+-----------------------+-----------------------+
| .. math::             |                       | (2‑35)                |
|                       |                       |                       |
|    \left\{ \begin{mat |                       |                       |
| rix}                  |                       |                       |
|    c_{0} = - \frac{N} |                       |                       |
| {N_{0}} \\            |                       |                       |
|    c_{1} = \frac{N}{N |                       |                       |
| _{1}} \\              |                       |                       |
|    \end{matrix} \righ |                       |                       |
| t.\  \Rightarrow \ove |                       |                       |
| rline{y} = \frac{1}{N |                       |                       |
| }\left( N_{0}c_{0} +  |                       |                       |
| N_{1}c_{1} \right) =  |                       |                       |
| \frac{1}{N}\left( - N |                       |                       |
|  + N \right) = 0 \Rig |                       |                       |
| htarrow \left( c_{1}  |                       |                       |
| - \overline{y} \right |                       |                       |
| )N_{1} = - \left( c_{ |                       |                       |
| 0} - \overline{y} \ri |                       |                       |
| ght)N_{0} = N \Righta |                       |                       |
| rrow \sum_{i = 1}^{N} |                       |                       |
| \left( y_{i} - \overl |                       |                       |
| ine{y} \right)\mathbf |                       |                       |
| {x}_{i} = N\left( {\o |                       |                       |
| verline{\mathbf{x}}}^ |                       |                       |
| {1} - {\overline{\mat |                       |                       |
| hbf{x}}}^{0} \right)  |                       |                       |
+-----------------------+-----------------------+-----------------------+

Combining (2‑33), (2‑34) and (2‑35), we have

.. math:: \left( \mathbf{S}\mathbf{+}\frac{N_{0}N_{1}}{N}\mathbf{M} \right)\mathbf{w} = N\left( {\overline{\mathbf{x}}}^{1} - {\overline{\mathbf{x}}}^{0} \right) \Rightarrow \mathbf{Sw =}\left( N - \frac{N_{0}N_{1}}{N}\left( {\overline{\mathbf{x}}}^{1} - {\overline{\mathbf{x}}}^{0} \right)^{T}\mathbf{w} \right)\left( {\overline{\mathbf{x}}}^{1} - {\overline{\mathbf{x}}}^{0} \right) \Rightarrow \mathbf{w}^{\mathbf{*}}\mathbf{\propto}\mathbf{S}^{- 1}\left( {\overline{\mathbf{x}}}^{1} - {\overline{\mathbf{x}}}^{0} \right)

By this equivalence, we can find best :math:`w_{0}` in terms LSE by
(2‑9) of **Theorem 2‑1**,

.. math:: w_{0}^{*} = \overline{y} - {\overline{\mathbf{x}}}^{T}\mathbf{w}^{\mathbf{*}}

Also, by this equivalence to LSE, Fisher’s discriminant has all
limitations of LSE.

**Theorem** **2‑7**\ **Fisher’s Linear Discriminant for Mutiple
Classes**. For

Sigmoid & Softmax Function
''''''''''''''''''''''''''

In above models, each data point is hard-assigned to a class, by
designing activation function and hand-crafting objectives to optimize.
In contrast, the probabilistic models find distributions for
:math:`p\left( C_{i}|\mathbf{\phi} \right)` for each feature data
:math:`\mathbf{\phi}` rather than a hard assignment. Many classic
probabilistic classification models involve sigmoid function and its
extension softmax function.

+-----------------------+-----------------------+-----------------------+
| |image4|              |                       | |image5|              |
+=======================+=======================+=======================+
| *(a) Sigmoid          |                       | *(b) Logit function*  |
| function*             |                       | :math:`a\left( \sigma |
| :math:`\sigma\left( a |                       |  \right) = \ln\frac{\ |
|  \right) = \frac{1}{1 |                       | sigma}{1 - \sigma}`   |
|  + e^{- a}}`          |                       |                       |
| *and*                 |                       |                       |
| :math:`1 - \sigma\lef |                       |                       |
| t( a \right)`         |                       |                       |
+-----------------------+-----------------------+-----------------------+

*Figure* *2‑2 Sigmoid function and logit function.*

The famous **sigmoid function** is defined as
:math:`\sigma\left( a \right) = \frac{1}{1 + e^{- a}}\mathbb{:R \rightarrow}\left( 0,1 \right)`,
representing an S-shaped curve. Its inverse
:math:`a = \ln\frac{\sigma}{1 - \sigma}` is called a **logit function**.
Also note :math:`\sigma\left( a \right) \in \left( 0,1 \right)` and

+-----------------------+-----------------------+-----------------------+
| .. math:: 1 - \sigma\ |                       | (2‑39)                |
| left( a \right) = 1 - |                       |                       |
|  \frac{1}{1 + e^{- a} |                       |                       |
| } = \frac{e^{- a}}{1  |                       |                       |
| + e^{- a}} = \frac{1} |                       |                       |
| {1 + e^{a}} \Rightarr |                       |                       |
| ow 1 - \sigma\left( a |                       |                       |
|  \right) = \sigma\lef |                       |                       |
| t( - a \right)        |                       |                       |
+-----------------------+-----------------------+-----------------------+

We therefore *note*
:math:`\left( \sigma\left( a \right),\sigma\left( - a \right) \right)`
can be viewed as a Bernoulli distribution. The **softmax function**, or
**normalized exponential function**, can be viewed as an extension of
the distribution
:math:`\left( \sigma\left( a \right),\sigma\left( - a \right) \right)`
to higher dimensions. Given a vector
:math:`\mathbf{\alpha} = \left( \alpha_{1},\ldots,\alpha_{K} \right)`,
then the softmax outputs a multinomial distribution

.. math:: \sigma\left( \mathbf{\alpha} \right) = \left( \frac{e^{\alpha_{1}}}{\sum_{k = 1}^{K}e^{\alpha_{k}}},\ldots,\frac{e^{\alpha_{K}}}{\sum_{k = 1}^{K}e^{\alpha_{k}}} \right):\mathbb{R}^{K} \rightarrow \mathbb{R}^{K}

The name “softmax” contains “max” because it is a probability
distribution that preserves the information to find the maximum;
meanwhile, it is “soft” in contrast to another function called **hardmax
function**, which outputs a vector of zero components except for the
component corresponding to the largest value in :math:`\mathbf{\alpha}`
is set :math:`1`. In the *special case* of :math:`K = 2`, the softmax
function is

+-----------------------+-----------------------+-----------------------+
| .. math:: \sigma\left |                       | (2‑40)                |
| ( \alpha_{1},\alpha_{ |                       |                       |
| 2} \right) = \left( \ |                       |                       |
| frac{e^{\alpha_{1}}}{ |                       |                       |
| e^{\alpha_{1}} + e^{\ |                       |                       |
| alpha_{2}}},\frac{e^{ |                       |                       |
| \alpha_{2}}}{e^{\alph |                       |                       |
| a_{1}} + e^{\alpha_{2 |                       |                       |
| }}} \right) = \left(  |                       |                       |
| \frac{1}{1 + e^{- \le |                       |                       |
| ft( \alpha_{1} - \alp |                       |                       |
| ha_{2} \right)}},\fra |                       |                       |
| c{1}{1 + e^{- \left(  |                       |                       |
| \alpha_{2} - \alpha_{ |                       |                       |
| 1} \right)}} \right)  |                       |                       |
+-----------------------+-----------------------+-----------------------+

Comparing (2‑39) and (2‑40) we can see the relation between the sigmoid
parameter :math:`a` and the two softmax parameter is
:math:`a = \alpha_{1} - \alpha_{2}`. This extension is more concrete in
(2‑45) when both sigmoid and softmax functions are applied in
constructing probabilistic discriminants below.

Sigmoid Assumption
''''''''''''''''''

The models we discuss here are based on **sigmoid assumption**: for
:math:`i = 1,2`, the probability that a data point belongs to a class
:math:`C_{i}` is a sigmoid curve :math:`\sigma\left( a_{i} \right)` of
some parameter :math:`a_{i}`. It turns out that :math:`a` has nice
interpretation and therefore the assumption makes sense. Given a feature
data :math:`\mathbf{\phi}`, assume

+-----------------------+-----------------------+-----------------------+
| .. math:: p\left( C_{ |                       | (2‑41)                |
| i}|\mathbf{\phi} \rig |                       |                       |
| ht) = \sigma\left( a_ |                       |                       |
| {i} \right) = \frac{1 |                       |                       |
| }{1 + e^{- a_{i}}},i  |                       |                       |
| = 1,2                 |                       |                       |
+-----------------------+-----------------------+-----------------------+

where “\ :math:`p\left( C_{i}|\mathbf{\phi} \right)`\ ” is simplified
notation for
“\ :math:`p\left( \mathbf{\phi} \in C_{i}|\mathbf{\phi} \right)`\ ”.
Then we can solve :math:`a_{1}` by

+-----------------------+-----------------------+-----------------------+
| .. math:: p\left( C_{ |                       | (2‑42)                |
| 1}|\mathbf{\phi} \rig |                       |                       |
| ht) = \frac{p\left( \ |                       |                       |
| mathbf{\phi}|C_{1} \r |                       |                       |
| ight)p\left( C_{1} \r |                       |                       |
| ight)}{p\left( \mathb |                       |                       |
| f{\phi}|C_{1} \right) |                       |                       |
| p\left( C_{1} \right) |                       |                       |
|  + p\left( \mathbf{\p |                       |                       |
| hi}|C_{2} \right)p\le |                       |                       |
| ft( C_{2} \right)} =  |                       |                       |
| \sigma\left( a_{1} \r |                       |                       |
| ight) = \frac{1}{1 +  |                       |                       |
| e^{- a_{1}}} \Rightar |                       |                       |
| row a_{1} = \ln\frac{ |                       |                       |
| \sigma}{1 - \sigma} = |                       |                       |
|  \ln\frac{p\left( \ma |                       |                       |
| thbf{\phi}|C_{1} \rig |                       |                       |
| ht)p\left( C_{1} \rig |                       |                       |
| ht)}{p\left( \mathbf{ |                       |                       |
| \phi}|C_{2} \right)p\ |                       |                       |
| left( C_{2} \right)}  |                       |                       |
| = \ln\frac{p\left( \m |                       |                       |
| athbf{\phi},C_{1} \ri |                       |                       |
| ght)}{p\left( \mathbf |                       |                       |
| {\phi},C_{2} \right)} |                       |                       |
+-----------------------+-----------------------+-----------------------+

And we have
:math:`a_{2} = \ln\frac{p\left( \mathbf{\phi}|C_{2} \right)p\left( C_{2} \right)}{p\left( \mathbf{\phi}|C_{1} \right)p\left( C_{1} \right)} = \ln\frac{p\left( \mathbf{\phi},C_{2} \right)}{p\left( \mathbf{\phi},C_{1} \right)} = - a_{1}`,
which is consistent with (2‑39). We can also verify

.. math:: p\left( C_{1}|\mathbf{\phi} \right) + p\left( C_{2}|\mathbf{\phi} \right) = \frac{1}{1 + e^{- a_{1}}} + \frac{1}{1 + e^{- a_{2}}} = \frac{e^{a_{1}}}{e^{a_{1}} + 1} + \frac{1}{1 + e^{a_{1}}} = 1

These expressions are highly *interpretable*: note
:math:`p\left( \mathbf{\phi} \right) = p\left( \mathbf{\phi},C_{1} \right) + p\left( \mathbf{\phi},C_{2} \right)`,
then if :math:`p\left( \mathbf{\phi},C_{1} \right)` is much larger than
:math:`p\left( \mathbf{\phi},C_{2} \right)`, then naturally it is more
likely for :math:`\mathbf{x}\mathbf{\in}C_{1}`, and the
increase/decrease of the probability is S-shaped. Now for the general
case of multiple classes :math:`C_{i},i = 1,\ldots,K`, we assume

+-----------------------+-----------------------+-----------------------+
| .. math:: p\left( C_{ |                       | (2‑43)                |
| i}|\mathbf{\phi} \rig |                       |                       |
| ht) = \sigma_{i}\left |                       |                       |
| ( \mathbf{\alpha} \ri |                       |                       |
| ght) = \frac{e^{\alph |                       |                       |
| a_{i}}}{\sum_{k = 1}^ |                       |                       |
| {K}e^{\alpha_{k}}}    |                       |                       |
+-----------------------+-----------------------+-----------------------+

Expand (2‑43) and we further have

.. math:: p\left( C_{i}|\mathbf{\phi} \right) = \frac{p\left( \mathbf{\phi}|C_{i} \right)p\left( C_{i} \right)}{\sum_{k = 1}^{K}{p\left( \mathbf{\phi}|C_{k} \right)p\left( C_{k} \right)}} = \frac{e^{\alpha_{i}}}{\sum_{k = 1}^{K}e^{\alpha_{k}}},i = 1,\ldots,K

The solution for :math:`\alpha_{i},i = 1,\ldots,K` is not unique, but
the most intuitive one is

+-----------------------+-----------------------+-----------------------+
| .. math:: e^{\alpha_{ |                       | (2‑44)                |
| i}} = p\left( \mathbf |                       |                       |
| {\phi}|C_{i} \right)p |                       |                       |
| \left( C_{i} \right)  |                       |                       |
| \Rightarrow \alpha_{i |                       |                       |
| } = \ln{p\left( \math |                       |                       |
| bf{\phi}|C_{i} \right |                       |                       |
| )p\left( C_{i} \right |                       |                       |
| )} = \ln{p\left( \mat |                       |                       |
| hbf{\phi,}C_{i} \righ |                       |                       |
| t)}                   |                       |                       |
+-----------------------+-----------------------+-----------------------+

*We can now turn to* model both
:math:`p\left( \mathbf{\phi}|C_{i} \right)` and
:math:`p\left( C_{i} \right)`; for example, *Lemma 2‑1* shows the
maximum likelihood of :math:`p\left( C_{i} \right)` is proportional to
the class size observed in the training data regardless of what
:math:`p\left( \mathbf{\phi}|C_{i} \right)` is, and *Theorem 2‑9* shows
:math:`p\left( \mathbf{\phi}|C_{i} \right)` can be modelled Gaussian
maximum likelihood. If both :math:`p\left( \mathbf{\phi}|C_{i} \right)`
and :math:`p\left( C_{i} \right)`\ have analytical form, then
:math:`\alpha_{i}` will have an analytical form. *In practice*, we could
find
:math:`\alpha_{i} = \ln c + \ln{p\left( \mathbf{\phi,}C_{i} \right)}`
for some constant :math:`c`. This does not matter as it is scaling every
:math:`e^{\alpha_{i}}` by the constant :math:`c` (e.g. see (2‑49)),

+-----------------------+-----------------------+-----------------------+
| .. math:: e^{\alpha_{ |                       | (2‑44)                |
| i}} = c \times p\left |                       |                       |
| ( \mathbf{\phi}|C_{i} |                       |                       |
|  \right)p\left( C_{i} |                       |                       |
|  \right) \Rightarrow  |                       |                       |
| \alpha_{i} = \ln c +  |                       |                       |
| \ln{p\left( \mathbf{\ |                       |                       |
| phi,}C_{i} \right)}   |                       |                       |
+-----------------------+-----------------------+-----------------------+

*For the special case* of :math:`K = 2`, (2‑43) is consistent with
(2‑42), because by (2‑40) we have

+-----------------------+-----------------------+-----------------------+
| .. math:: a_{1} = \al |                       | (2‑45)                |
| pha_{1} - \alpha_{2}  |                       |                       |
| = \ln{p\left( \mathbf |                       |                       |
| {\phi,}C_{1} \right)} |                       |                       |
|  - \ln{p\left( \mathb |                       |                       |
| f{\phi,}C_{2} \right) |                       |                       |
| } = \ln\frac{p\left(  |                       |                       |
| \mathbf{\phi,}C_{1} \ |                       |                       |
| right)}{p\left( \math |                       |                       |
| bf{\phi,}C_{2} \right |                       |                       |
| )}                    |                       |                       |
+-----------------------+-----------------------+-----------------------+

At last, we note a multinomial distribution
:math:`p\left( C_{i}|\mathbf{\phi} \right)` is analogous to the
deterministic membership vector defined in *Theorem 2‑6*, and can be
referred to as **probabilistic membership** or **mixed membership**, and
we say membership defined by (2‑43) is **softmax membership**. One
standard measure of the performance of a model that yields mixed
memberships is **cross-entropy error** as discussed earlier. In the case
of classification, it is
:math:`\mathcal{e}_{c} = \sum_{i = 1}^{N}{\ln{p\left( y_{i}|\mathbf{\phi}_{i} \right)}}`
given data
:math:`\mathbf{\Phi =}\left( \mathbf{\phi}_{1}\mathbf{,\ldots,}\mathbf{\phi}_{N} \right),\mathbf{y =}\left( y_{1},\ldots,y_{n} \right)^{T}`,
simply adding up the negative log probabilities of observed labels. The
negative cross entropy :math:`\mathcal{- e}` also equals the
log-likelihood of :math:`p\left( \mathbf{\Phi}|\mathbf{y} \right)`;
thus, maximizing the likelihood
:math:`p\left( \mathbf{\Phi}|\mathbf{y} \right)` is equivalent to
minimizing the cross-entropy error. The `logistic
regression <#logistic-regression>`__ later aims at maximizing
:math:`p\left( \mathbf{\Phi}|\mathbf{y} \right)` and hence minimizing
the cross-entropy error.

Class Probability ML
''''''''''''''''''''

-  **Lemma** **2‑1**\ **Class Probability ML**. We refer to
   :math:`p\left( C_{1} \right),\ldots,p\left( C_{K} \right)` as **class
   probabilities**. We now show the MLE of class probabilities are
   proportional to class size. For binary classification, we have the
   likelihood

+-----------------------+-----------------------+-----------------------+
| .. math:: p\left( \ma |                       | (2‑46)                |
| thbf{\Phi},\mathbf{y} |                       |                       |
|  \right) = p\left( \m |                       |                       |
| athbf{\Phi}|\mathbf{y |                       |                       |
| } \right)p\left( \mat |                       |                       |
| hbf{y} \right) = \pro |                       |                       |
| d_{i = 1}^{N}{\left(  |                       |                       |
| p\left( C_{1} \right) |                       |                       |
| p\left( \mathbf{\phi} |                       |                       |
| _{i}|C_{1} \right) \r |                       |                       |
| ight)^{y_{i}}\left( p |                       |                       |
| \left( C_{2} \right)p |                       |                       |
| \left( \mathbf{\phi}_ |                       |                       |
| {i}|C_{2} \right) \ri |                       |                       |
| ght)^{1 - y_{i}}} \Ri |                       |                       |
| ghtarrow \ln p = \sum |                       |                       |
| _{i = 1}^{N}\left( y_ |                       |                       |
| {i}\left( \ln{p\left( |                       |                       |
|  C_{1} \right)} + \ln |                       |                       |
| {p\left( \mathbf{\phi |                       |                       |
| }_{i}|C_{1} \right)}  |                       |                       |
| \right) + \left( 1 -  |                       |                       |
| y_{i} \right)\left( \ |                       |                       |
| ln{p\left( C_{2} \rig |                       |                       |
| ht)} + \ln{p\left( \m |                       |                       |
| athbf{\phi}_{i}|C_{2} |                       |                       |
|  \right)} \right) \ri |                       |                       |
| ght)                  |                       |                       |
+-----------------------+-----------------------+-----------------------+

Let :math:`p\left( C_{1} \right) = \pi`, then
:math:`p\left( C_{2} \right) = 1 - \pi`, and the terms dependent on
:math:`\pi` are only

.. math:: \ln p \propto \sum_{i = 1}^{N}\left( y_{i}\ln\pi + \left( 1 - y_{i} \right)\ln\left( 1 - \pi \right) \right)

Taking derivative, we have

.. math:: \sum_{i = 1}^{N}\left( \frac{y_{i}}{\pi} - \frac{1 - y_{i}}{1 - \pi} \right) = 0 \Rightarrow \frac{1}{\pi}\sum_{i = 1}^{N}y_{i} = \frac{1}{1 - \pi}\sum_{i = 1}^{N}{1 - y_{i}} = 0 \Rightarrow \frac{1}{\pi}N_{1} = \frac{1}{1 - \pi}N_{2} = 0 \Rightarrow \pi = \frac{N_{1}}{N_{1} + N_{2}} = \frac{N_{1}}{N}

where :math:`N_{1}` is the size of class :math:`C_{1}`, and
:math:`N_{2}` is the size of class :math:`C_{2}`. This can be easily
extended to multiple classes. Let
:math:`p\left( C_{i} \right) = \pi_{i},i = 1,\ldots,K`, then note
:math:`\pi_{K} = 1 - \pi_{1} - \ldots - \pi_{K - 1}` is dependent of
previous probabilities, and

+-----------------------+-----------------------+-----------------------+
| .. math:: p\left( \ma |                       | (2‑47)                |
| thbf{\Phi},\mathbf{y} |                       |                       |
|  \right) = \prod_{\ma |                       |                       |
| thbf{\phi} \in C_{1}} |                       |                       |
| ^{}{\pi_{1}p\left( \m |                       |                       |
| athbf{\phi}|C_{1} \ri |                       |                       |
| ght)} \times \ldots \ |                       |                       |
| times \prod_{\mathbf{ |                       |                       |
| \phi} \in C_{K}}^{}{\ |                       |                       |
| pi_{K}p\left( \mathbf |                       |                       |
| {\phi}|C_{K} \right)} |                       |                       |
|  \Rightarrow \ln p \p |                       |                       |
| ropto \sum_{i = 1}^{N |                       |                       |
| _{1}}{\ln\pi_{1}} + \ |                       |                       |
| ldots + \sum_{i = 1}^ |                       |                       |
| {N_{K}}{\ln\pi_{K}} = |                       |                       |
|  N_{1}\ln\pi_{1} + \l |                       |                       |
| dots + N_{K}\ln\pi_{K |                       |                       |
| }                     |                       |                       |
+-----------------------+-----------------------+-----------------------+

Taking derivatives w.r.t. :math:`\pi_{1},\ldots,\pi_{K - 1}` we have

+-----------------------+-----------------------+-----------------------+
| .. math:: \frac{N_{i} |                       | (2‑48)                |
| }{\pi_{i}} = \frac{N_ |                       |                       |
| {K}}{\pi_{K}} \Righta |                       |                       |
| rrow \pi_{i} = \frac{ |                       |                       |
| N_{i}}{N_{K}}\pi_{K}, |                       |                       |
| i = 1,\ldots,K - 1    |                       |                       |
+-----------------------+-----------------------+-----------------------+

which implies

.. math:: 1 - \pi_{K} = \sum_{i = 1}^{K - 1}\pi_{i} = \pi_{K}\sum_{i = 1}^{K - 1}\frac{N_{i}}{N_{K}} = \pi_{K}\frac{N - N_{K}}{N_{K}} \Rightarrow \pi_{K} = \frac{N_{K}}{N}

Plug this back in (2‑48), we have a general result

.. math:: {\widehat{\pi}}_{i} = \frac{N_{i}}{N},i = 1,\ldots,K

GaussianML-Softmax Discriminant
'''''''''''''''''''''''''''''''

-  **Theorem** **2‑9**\ **GaussianML-Softmax Discriminant**. For this
   model, we assume the feature points in each class are Gaussian
   distributed. The Gaussians of each class could have different means
   :math:`\mathbf{\mu}_{1},\ldots,\mathbf{\mu}_{K}` but the same
   covariance :math:`\mathbf{\Sigma}` to make the inferences for each
   class dependent. That is,

.. math:: p\left( \mathbf{\phi}|C_{i} \right)\sim\operatorname{Gaussian}\left( \mathbf{\mu}_{i},\mathbf{\Sigma} \right),i = 1,\ldots,K

*Note* we may not assume each class has its own covariance, otherwise
this model reduces to independent maximum likelihoods of Gaussian
distributions. The shared covariance is clearly one limitation of
Gaussian ML; the other main limitation is it high complexity with
quadratically growing number of parameters (:math:`K` considered fixed).
We show also again emphasize, like other MLE, it tends to suffer the
problem of overfitting and not robust to outliers (see *Theorem 2‑2*).
Using (2‑45), we have

.. math:: \ln p \propto \sum_{\mathbf{\phi \in}C_{1}}^{}{\ln{p\left( \mathbf{\phi}|C_{1} \right)}} + \ldots + \sum_{\mathbf{\phi \in}C_{K}}^{}{\ln{p\left( \mathbf{\phi}|C_{K} \right)}}

Clearly
:math:`\frac{\partial\ln p}{\partial\mathbf{\mu}_{i}} = \frac{\partial}{\partial\mathbf{\mu}_{i}}\left( \sum_{\mathbf{\phi \in}C_{i}}^{}{\ln{p\left( \mathbf{\phi}|C_{i} \right)}} \right),i = 1,\ldots,K`,
which is the same as the MLE for :math:`\mathbf{\mu}_{i}` Gaussian
likelihood
:math:`\operatorname{Gaussian}\left( \mathbf{\mu}_{i},\mathbf{\Sigma} \right)`
given only the data in class :math:`C_{i}`, and therefore by (1‑4) of
*Theorem 1‑4*,

.. math:: {\widehat{\mathbf{\mu}}}_{i} = \frac{1}{N_{i}}\sum_{\mathbf{\phi \in}C_{i}}^{}\mathbf{\phi},i = 1,\ldots,K

For covariance :math:`\mathbf{\Sigma}`, using (1‑3) of *Theorem 1‑4*, we
have

.. math:: \frac{\partial\ln p}{\partial\mathbf{\Sigma}^{- 1}} = \sum_{i = 1}^{K}\left( \frac{N_{i}}{2}\mathbf{\Sigma} - \frac{1}{2}\sum_{\mathbf{\phi \in}C_{i}}^{}{\left( \mathbf{\phi} - \mathbf{\mu}_{i} \right)\left( \mathbf{\phi} - \mathbf{\mu}_{i} \right)^{T}} \right) = \frac{N}{2}\mathbf{\Sigma -}\frac{1}{2}\sum_{i = 1}^{K}\left( \sum_{\mathbf{\phi \in}C_{i}}^{}{\left( \mathbf{\phi} - \mathbf{\mu}_{i} \right)\left( \mathbf{\phi} - \mathbf{\mu}_{i} \right)^{T}} \right) = 0 \Rightarrow \widehat{\mathbf{\Sigma}}\mathbf{=}\frac{1}{N}\sum_{i = 1}^{K}\left( \sum_{\mathbf{\phi \in}C_{i}}^{}{\left( \mathbf{\phi} - \mathbf{\mu}_{i} \right)\left( \mathbf{\phi} - \mathbf{\mu}_{i} \right)^{T}} \right)

Let
:math:`{\widehat{\mathbf{\Sigma}}}_{i} = \frac{1}{N_{i}}\sum_{\mathbf{\phi \in}C_{i}}^{}{\left( \mathbf{\phi} - \mathbf{\mu}_{i} \right)\left( \mathbf{\phi} - \mathbf{\mu}_{i} \right)^{T}},i = 1,\ldots,K`,
which is the biased sample covariance of each class, or the MLE of
:math:`\mathbf{\Sigma}` based only on the data from class :math:`C_{i}`,
then

.. math:: \widehat{\mathbf{\Sigma}}\mathbf{=}\sum_{i = 1}^{K}{\frac{N_{i}}{N}{\widehat{\mathbf{\Sigma}}}_{i}} = \sum_{i = 1}^{K}{{\widehat{\pi}}_{i}{\widehat{\mathbf{\Sigma}}}_{i}}

Recall our eventual goal of a probabilistic discriminant is to solve
:math:`p\left( C_{i}|\mathbf{\phi} \right)`. With sigmoid assumption,
:math:`p\left( C_{i}|\mathbf{\phi} \right) = \sigma\left( \mathbf{\alpha} \right)`.
Note by discussion of (2‑43) and (2‑44), the constant is not important
and can be removed, and therefore

+-----------------------+-----------------------+-----------------------+
| .. math:: \ln{p\left( |                       | (2‑49)                |
|  \mathbf{\phi}|C_{i}  |                       |                       |
| \right)p\left( C_{i}  |                       |                       |
| \right)} = \ln\frac{1 |                       |                       |
| }{\left( 2\pi \right) |                       |                       |
| ^{\frac{m}{2}}} + \ln |                       |                       |
| \frac{1}{\left| \wide |                       |                       |
| hat{\mathbf{\Sigma}}  |                       |                       |
| \right|^{\frac{1}{2}} |                       |                       |
| } - \frac{1}{2}\left( |                       |                       |
|  \mathbf{\phi} - {\wi |                       |                       |
| dehat{\mathbf{\mu}}}_ |                       |                       |
| {i} \right)^{T}{\wide |                       |                       |
| hat{\mathbf{\Sigma}}} |                       |                       |
| ^{- 1}\left( \mathbf{ |                       |                       |
| \phi} - {\widehat{\ma |                       |                       |
| thbf{\mu}}}_{i} \righ |                       |                       |
| t) + \ln\frac{N_{i}}{ |                       |                       |
| N} \Rightarrow \alpha |                       |                       |
| _{i} = \left( {\wideh |                       |                       |
| at{\mathbf{\Sigma}}}^ |                       |                       |
| {- 1}{\widehat{\mathb |                       |                       |
| f{\mu}}}_{i} \right)^ |                       |                       |
| {T}\mathbf{\phi}\math |                       |                       |
| bf{-}\frac{1}{2}{\wid |                       |                       |
| ehat{\mathbf{\mu}}}_{ |                       |                       |
| i}^{T}{\widehat{\math |                       |                       |
| bf{\Sigma}}}^{- 1}{\w |                       |                       |
| idehat{\mathbf{\mu}}} |                       |                       |
| _{i} + \ln\frac{N_{i} |                       |                       |
| }{N}                  |                       |                       |
+-----------------------+-----------------------+-----------------------+

We can rewrite

.. math:: \mathbf{\alpha} = \mathbf{W}^{T}\mathbf{\phi}\mathbf{+}\mathbf{b}

where
:math:`\mathbf{W} = \left( \mathbf{w}_{1},\ldots,\mathbf{w}_{K} \right)`,
:math:`\mathbf{w}_{i} = {\widehat{\mathbf{\Sigma}}}^{- 1}{\widehat{\mathbf{\mu}}}_{i}`,
and
:math:`\mathbf{b}\mathbf{=}\left( b_{1}\mathbf{,\ldots,}b_{K} \right)\mathbf{,}b_{i} = \mathbf{-}\frac{1}{2}{\widehat{\mathbf{\mu}}}_{i}^{T}{\widehat{\mathbf{\Sigma}}}^{- 1}{\widehat{\mathbf{\mu}}}_{i} + \ln\frac{N_{i}}{N}`.
Therefore, the softmax parameter of Gaussian ML discriminant is linear
in :math:`\mathbf{\phi}`.

Logistic Regression
'''''''''''''''''''

-  The complexity of above Gaussian ML discriminant in *Theorem 2‑9* is
   usually considered too high. To reduce the complexity, the linearity
   of softmax parameter in :math:`\mathbf{x}` provides inspiration. We
   may conversely first restrict softmax parameter
   :math:`\mathbf{\alpha} = \mathbf{W}^{T}\mathbf{\phi}` with
   :math:`\mathbf{W}` as parameters. Such model has linear complexity if
   :math:`K` is fixed. Logistic regression is also more robust (see
   (2‑55)) with Newton’s method for optimization. However, with reduced
   model complexity, it loses a closed form solution for the multi-class
   classification, as shown below.

   **Binary logistic regression**. For simplicity, we first consider
   binary classification with labels :math:`l_{1} = 1,l_{2} = 0`. The
   model makes an i.i.d. Bernoulli *assumption* that
   :math:`y_{i}\sim\text{Bernoulli}\left( \sigma\left( \mathbf{\phi}_{i}^{T}\mathbf{w} \right) \right),\ i = 1,\ldots,N`,
   or equivalently,

+-----------------------+-----------------------+-----------------------+
| .. math:: p\left( C_{ |                       | (2‑51)                |
| 1}|\mathbf{\phi}_{i}  |                       |                       |
| \right) = \sigma\left |                       |                       |
| ( \mathbf{\phi}_{i}^{ |                       |                       |
| T}\mathbf{w} \right), |                       |                       |
| i = 1,\ldots,N        |                       |                       |
+-----------------------+-----------------------+-----------------------+

And we optimize the following likelihood (negative cross entropy),

+-----------------------+-----------------------+-----------------------+
| .. math:: p\left( \ma |                       | (2‑52)                |
| thbf{\Phi,y} \right)  |                       |                       |
| \propto p\left( \math |                       |                       |
| bf{y|\Phi} \right) =  |                       |                       |
| \prod_{i = 1}^{N}{\le |                       |                       |
| ft( \sigma\left( \mat |                       |                       |
| hbf{\phi}_{i}^{T}\mat |                       |                       |
| hbf{w} \right) \right |                       |                       |
| )^{y_{i}}\left( 1 - \ |                       |                       |
| sigma\left( \mathbf{\ |                       |                       |
| phi}_{i}^{T}\mathbf{w |                       |                       |
| } \right) \right)^{1  |                       |                       |
| - y_{i}}} \Rightarrow |                       |                       |
|  \ln p = \sum_{i = 1} |                       |                       |
| ^{N}{y_{i}\ln{\sigma\ |                       |                       |
| left( \mathbf{\phi}_{ |                       |                       |
| i}^{T}\mathbf{w} \rig |                       |                       |
| ht)}} + \sum_{i = 1}^ |                       |                       |
| {N}{\left( 1 - y_{i}  |                       |                       |
| \right)\ln\left( 1 -  |                       |                       |
| \sigma\left( \mathbf{ |                       |                       |
| \phi}_{i}^{T}\mathbf{ |                       |                       |
| w} \right) \right)}   |                       |                       |
+-----------------------+-----------------------+-----------------------+

*Closed form solution*. The derivative of sigmoid
:math:`\sigma\left( a \right) = \frac{1}{1 + e^{- a}}` is
:math:`\frac{d\sigma}{da} = \sigma\left( 1 - \sigma \right)`. Applying
this and *Fact 1‑1* we have

.. math:: \nabla_{\mathbf{w}}\ln p = \sum_{i = 1}^{N}{\frac{y_{i}}{\sigma}\sigma\left( 1 - \sigma \right)\mathbf{\phi}_{i}} + \sum_{i = 1}^{N}{- \frac{\left( 1 - y_{i} \right)}{1 - \sigma}\sigma\left( 1 - \sigma \right)\mathbf{\phi}_{i}}\mathbf{=}\sum_{i = 1}^{N}{y_{i}\left( 1 - \sigma \right)\mathbf{\phi}_{i} - \left( 1 - y_{i} \right)\sigma\mathbf{\phi}_{i}}\mathbf{=}\sum_{i = 1}^{N}{\left( y_{i} - \sigma\left( \mathbf{\phi}_{i}^{T}\mathbf{w} \right) \right)\mathbf{\phi}_{i}}

Let :math:`\mathbf{\sigma =}\begin{pmatrix}
\sigma_{1} \\
 \vdots \\
\sigma_{N} \\
\end{pmatrix} = \begin{pmatrix}
\sigma\left( \mathbf{\phi}_{1}^{T}\mathbf{w} \right) \\
 \vdots \\
\sigma\left( \mathbf{\phi}_{N}^{T}\mathbf{w} \right) \\
\end{pmatrix}`, then

+-----------------------+-----------------------+-----------------------+
| .. math:: \nabla_{\ma |                       | (2‑53)                |
| thbf{w}}\ln p = \math |                       |                       |
| bf{\Phi}\left( \mathb |                       |                       |
| f{y}\mathbf{-}\mathbf |                       |                       |
| {\sigma} \right)      |                       |                       |
+-----------------------+-----------------------+-----------------------+

and :math:`\nabla_{\mathbf{w}}\ln p = \mathbf{0}` yields

.. math:: \mathbf{\sigma}^{\mathbf{*}}\mathbf{=}\left( \mathbf{\Phi} \right)^{\mathbf{\dagger}}\mathbf{\text{Φy}}

Let
:math:`\mathbf{\sigma}^{*} = \left( \sigma_{1}^{*},\ldots,\sigma_{N}^{*} \right)^{T}`,
above can be further solved as

.. math::

   \mathbf{\Phi}^{T}\mathbf{w} = \begin{pmatrix}
   \ln\frac{\sigma_{1}^{*}}{1 - \sigma_{1}^{*}} \\
    \vdots \\
   \ln\frac{\sigma_{N}^{*}}{1 - \sigma_{N}^{*}} \\
   \end{pmatrix} \Rightarrow \mathbf{w}^{\mathbf{*}}\mathbf{=}\left( \mathbf{\Phi}^{T} \right)^{\dagger}\begin{pmatrix}
   \ln\frac{\sigma_{1}^{*}}{1 - \sigma_{1}^{*}} \\
    \vdots \\
   \ln\frac{\sigma_{N}^{*}}{1 - \sigma_{N}^{*}} \\
   \end{pmatrix}

*Iterative solutions*. Notice :math:`\ln p` in (2‑50) is a strictly
concave function w.r.t. :math:`\mathbf{w}` since logarithm is strictly
concave and every :math:`\mathbf{\phi}_{i}^{T}\mathbf{w}` is linear, so
the MLE for :math:`\ln p` has unique solution and is solvable by classic
convex optimization algorithms like gradient ascent using (2‑51). Let
:math:`{\overline{\mathbf{\phi}}}_{1}` denote the sample mean of the
first class, and
:math:`\mathbb{E}_{C_{1}}\left\lbrack \mathbf{\phi} \right\rbrack`
denote the expected data of the first class, then
:math:`\mathbf{\Phi y =}N_{1}{\overline{\mathbf{\phi}}}_{1}` is the
sample feature sum of the first class, and
:math:`\mathbf{\text{Φσ}}\mathbf{=}\sum_{i = 1}^{N}\left( p\left( C_{1}|\mathbf{\phi}_{i} \right)\mathbf{\phi}_{i} \right) = N\mathbb{E}_{C_{1}}\left\lbrack \mathbf{\phi} \right\rbrack`
is the expected feature sum of the first class, and therefore
:math:`\nabla_{\mathbf{w}}\ln p = N_{1}{\overline{\mathbf{\phi}}}_{1} - N\mathbb{E}_{C_{1}}\left\lbrack \mathbf{\phi} \right\rbrack`,
i.e. the fastest ascending direction of :math:`\mathbf{w}` points from
the expected sum to the sample sum, intuitively meaning the model is
evolving from the prior assumed distribution to fit the observed data.

We many also use **Newton’s method** that comes with quadratic
convergence, and its update is as the following,

+-----------------------+-----------------------+-----------------------+
| .. math:: \mathbf{w}^ |                       | (2‑51)                |
| {\left( t + 1 \right) |                       |                       |
| } = \mathbf{w}^{\left |                       |                       |
| ( t \right)} - \mathb |                       |                       |
| f{H}_{\mathbf{w}}^{-  |                       |                       |
| 1}\nabla_{\mathbf{w}} |                       |                       |
| \ln p                 |                       |                       |
+-----------------------+-----------------------+-----------------------+

where :math:`\mathbf{H}_{\mathbf{w}}` is the negative definite (because
of being strictly concave) Hessian matrix of :math:`\ln p` w.r.t.
:math:`\mathbf{w}`, consisting of all second-order derivatives of
:math:`\ln p` w.r.t. components of :math:`\mathbf{w}`. Recall Hessian
matrix is the Jacobian of gradient. Denote Jacobian matrix using bold
symbol “\ :math:`\mathbf{\nabla}`\ ”, we have

.. math::

   \mathbf{\nabla}_{\mathbf{w}}\left( \mathbf{\sigma} \right)\mathbf{=}\begin{pmatrix}
   \sigma_{1}\left( 1 - \sigma_{1} \right)\mathbf{\phi}_{1}^{T} \\
    \vdots \\
   \sigma_{N}\left( 1 - \sigma_{N} \right)\mathbf{\phi}_{N}^{T} \\
   \end{pmatrix} = \begin{pmatrix}
   \sigma_{1}\left( 1 - \sigma_{1} \right) & & \\
    & \ddots & \\
    & & \sigma_{N}\left( 1 - \sigma_{N} \right) \\
   \end{pmatrix}\mathbf{\Phi}^{T}

Recall the model makes an i.i.d. Bernoulli assumption s.t.
:math:`y_{i}\sim\text{Bernoulli}\left( \sigma\left( \mathbf{\phi}_{i}^{T}\mathbf{w} \right) \right)`,
and therefore the diagonal matrix :math:`\begin{pmatrix}
\sigma_{1}\left( 1 - \sigma_{1} \right) & & \\
 & \ddots & \\
 & & \sigma_{N}\left( 1 - \sigma_{N} \right) \\
\end{pmatrix}` is the covariance matrix of RV :math:`\mathbf{y}`. Denote
it as :math:`\mathbf{\Sigma}`, we arrive at

.. math:: \mathbf{\nabla}_{\mathbf{w}}\mathbf{\Phi}\left( \mathbf{y}\mathbf{-}\mathbf{\sigma} \right)\mathbf{=}\mathbf{\nabla}_{\mathbf{w}}\left( \mathbf{-}\mathbf{\text{Φσ}} \right)\mathbf{= -}\mathbf{\text{ΦΣ}}\mathbf{\Phi}^{T}\mathbf{\Rightarrow}\mathbf{H}_{\mathbf{w}} = \mathbf{\nabla}_{\mathbf{w}}\mathbf{\Phi}\left( \mathbf{y}\mathbf{-}\mathbf{\sigma} \right)\mathbf{= -}\mathbf{\text{ΦΣ}}\mathbf{\Phi}^{T}

Plug this back to (2‑51); *note* both
:math:`\mathbf{\Sigma},\mathbf{\sigma}` are dependent on
:math:`\mathbf{w}^{\left( t \right)}` and hence on :math:`t`, but we
omit the superscript for simplicity. Then we have

+-----------------------+-----------------------+-----------------------+
| .. math:: \mathbf{w}^ |                       | (2‑54)                |
| {\left( t + 1 \right) |                       |                       |
| } = \mathbf{w}^{\left |                       |                       |
| ( t \right)} + \left( |                       |                       |
|  \mathbf{\text{ΦΣ}}\m |                       |                       |
| athbf{\Phi}^{T} \righ |                       |                       |
| t)^{- 1}\mathbf{\Phi} |                       |                       |
| \left( \mathbf{y - \s |                       |                       |
| igma} \right)\mathbf{ |                       |                       |
| =}\left( \mathbf{\tex |                       |                       |
| t{ΦΣ}}\mathbf{\Phi}^{ |                       |                       |
| T} \right)^{- 1}\left |                       |                       |
| ( \mathbf{\text{ΦΣ}}\ |                       |                       |
| mathbf{\Phi}^{T}\math |                       |                       |
| bf{w}^{\left( t \righ |                       |                       |
| t)} + \mathbf{\Phi}\l |                       |                       |
| eft( \mathbf{y - \sig |                       |                       |
| ma} \right) \right) = |                       |                       |
|  \left( \mathbf{\text |                       |                       |
| {ΦΣ}}\mathbf{\Phi}^{T |                       |                       |
| } \right)^{- 1}\mathb |                       |                       |
| f{\text{ΦΣ}}\left( \m |                       |                       |
| athbf{\Phi}^{T}\mathb |                       |                       |
| f{w}^{\left( t \right |                       |                       |
| )} + \mathbf{\Sigma}^ |                       |                       |
| {- 1}\left( \mathbf{y |                       |                       |
|  - \sigma} \right) \r |                       |                       |
| ight)                 |                       |                       |
+-----------------------+-----------------------+-----------------------+

Note (*2*\ ‑\ *54*) coincide with the solution of weighted LSE as in EX
1, and therefore we have

+-----------------------+-----------------------+-----------------------+
| .. math:: \mathbf{w}^ |                       | (2‑55)                |
| {\left( t + 1 \right) |                       |                       |
| } = \arg{\operatornam |                       |                       |
| e{}\left\| \mathbf{\S |                       |                       |
| igma}^{\frac{1}{2}}\l |                       |                       |
| eft( \mathbf{\Phi}^{T |                       |                       |
| }\mathbf{w}^{\left( t |                       |                       |
|  \right)} + \mathbf{\ |                       |                       |
| Sigma}^{- 1}\left( \m |                       |                       |
| athbf{y - \sigma} \ri |                       |                       |
| ght) - \mathbf{\Phi}^ |                       |                       |
| {T}\mathbf{w}^{\left( |                       |                       |
|  t \right)} \right) \ |                       |                       |
| right\|_{2}^{2}} = \a |                       |                       |
| rg{\operatorname{}\le |                       |                       |
| ft\| \mathbf{\Sigma}^ |                       |                       |
| {- \frac{1}{2}}\left( |                       |                       |
|  \mathbf{y - \sigma}  |                       |                       |
| \right) \right\|_{2}^ |                       |                       |
| {2}}                  |                       |                       |
+-----------------------+-----------------------+-----------------------+

That is, :math:`\mathbf{w}^{\left( t + 1 \right)}` is trying to minimize
the squared error between :math:`\mathbf{y}` and :math:`\mathbf{\sigma}`
weighted by the current inversed standard deviation
:math:`\left( \mathbf{\Sigma}^{- \frac{1}{2}} \right)^{\left( t \right)}`.
Now if :math:`\mathbf{w}^{\left( t \right)}` goes to extreme values that
make some :math:`\sigma_{i} \approx y_{i}`, then corresponding inversed
standard deviation in the diagonal of
:math:`\left( \mathbf{\Sigma}^{- \frac{1}{2}} \right)^{\left( t \right)}`
will increase to upweight the corresponding errors
:math:`\sigma_{i} - y_{i}`; therefore, we see weights in
:math:`\mathbf{\Sigma}^{- \frac{1}{2}}` make it harder for each
:math:`\sigma` to overfit :math:`\mathbf{y}`, or equivalently prevent
components of :math:`\mathbf{w}` from becoming very large or very small.
The algorithm of (2‑54) also falls into the category of
**iteratively-reweighted least square algorithms**.

**Multi-Class logistic regression**. For multi-class classification, we
first make an assumption similar to (2‑49) using softmax. Let
:math:`\mathbf{W} = \left( \mathbf{w}_{1},\ldots,\mathbf{w}_{K} \right)`,
and let :math:`\sigma_{k}` denote the :math:`k`\ th component of the
softmax function :math:`\sigma`, then

+-----------------------+-----------------------+-----------------------+
| .. math:: p\left( C_{ |                       | (2‑57)                |
| k}|\mathbf{\phi}_{i}  |                       |                       |
| \right) = \sigma_{k}\ |                       |                       |
| left( \mathbf{W}^{T}\ |                       |                       |
| mathbf{\phi}_{i} \rig |                       |                       |
| ht) = \frac{e^{\mathb |                       |                       |
| f{w}_{k}^{T}\mathbf{\ |                       |                       |
| phi}_{i}}}{\sum_{j =  |                       |                       |
| 1}^{K}e^{\mathbf{w}_{ |                       |                       |
| j}^{T}\mathbf{\phi}_{ |                       |                       |
| i}}},i = 1,\ldots,N,k |                       |                       |
|  = 1,\ldots,K         |                       |                       |
+-----------------------+-----------------------+-----------------------+

The log-likelihood of :math:`p\left( C_{k}|\mathbf{\phi}_{i} \right)`
(negative cross-entropy) is

.. math:: \ln p \propto \sum_{\mathbf{\phi \in}C_{1}}^{}{\ln{p\left( C_{1}|\mathbf{\phi} \right)}} + \ldots + \sum_{\mathbf{\phi \in}C_{K}}^{}{\ln{p\left( C_{K}|\mathbf{\phi} \right)}} = \sum_{\mathbf{\phi \in}C_{1}}^{}{\ln\frac{e^{\mathbf{w}_{1}^{T}\mathbf{\phi}}}{\sum_{j = 1}^{K}e^{\mathbf{w}_{j}^{T}\mathbf{\phi}}}} + \ldots + \sum_{\mathbf{\phi \in}C_{K}}^{}{\ln\frac{e^{\mathbf{w}_{K}^{T}\mathbf{\phi}}}{\sum_{j = 1}^{K}e^{\mathbf{w}_{j}^{T}\mathbf{\phi}}}} = \sum_{\mathbf{\phi \in}C_{1}}^{}\left( \mathbf{w}_{1}^{T}\mathbf{\phi -}\ln\left( \sum_{j = 1}^{K}e^{\mathbf{w}_{j}^{T}\mathbf{\phi}} \right) \right) + \ldots + \sum_{\mathbf{\phi \in}C_{K}}^{}\left( \mathbf{w}_{K}^{T}\mathbf{\phi -}\ln\left( \sum_{j = 1}^{K}e^{\mathbf{w}_{j}^{T}\mathbf{\phi}} \right) \right)

Let :math:`{\overline{\mathbf{\phi}}}_{k}` denote the sample mean of the
:math:`k`\ th class, and
:math:`\mathbb{E}_{C_{k}}\left\lbrack \mathbf{\phi} \right\rbrack`
denote the expected data of the :math:`k`\ th class. Taking derivative
of the above likelihood, and we have

.. math:: \frac{\partial\ln p}{\partial\mathbf{w}_{k}} = \sum_{\mathbf{\phi \in}C_{k}}^{}\mathbf{\phi} - \sum_{l = 1}^{K}{\sum_{\mathbf{\phi \in}C_{l}}^{}\left( \frac{e^{\mathbf{w}_{k}^{T}\mathbf{\phi}}}{\sum_{j = 1}^{K}e^{\mathbf{w}_{j}^{T}\mathbf{\phi}}}\mathbf{\phi} \right)} = \sum_{\mathbf{\phi \in}C_{k}}^{}\mathbf{\phi} - \sum_{i = 1}^{N}\left( p\left( C_{k}|\mathbf{\phi}_{i} \right)\mathbf{\phi}_{i} \right) = N_{k}{\overline{\mathbf{\phi}}}_{k} - N\mathbb{E}_{C_{k}}\left\lbrack \mathbf{\phi} \right\rbrack

Again, we can iteratively apply gradient ascent for each
:math:`\mathbf{w}_{k}`, and the fastest ascending direction of
:math:`\mathbf{w}_{k}` points from the expected sum to the sample sum,
consistent with the binary logistic regression, intuitively meaning the
model is evolving from the prior assumed distribution to fit the
observed data.

**Bayesian Logistic regression**.

-  Probit regression.

**Support Vector Machine**
^^^^^^^^^^^^^^^^^^^^^^^^^^

Let’s consider binary classification and assume the label set
:math:`\mathbb{L =}\left\{ + 1, - 1 \right\}`. Given a
:math:`m \times n` data set :math:`\mathbf{X}` and its labels
:math:`\mathbf{y}`, the basic goal of **Support Vector Machine** (SVM)
is to find a hyperplane :math:`H:\mathbf{a}^{T}\mathbf{x} + \mathbf{b}`
that best separates the data points with different labels, same as all
other linear classifiers discussed earlier; *in addition*, it requires
equal distance from :math:`H` to the nearest positive point and the
nearest negative point be equal, and it also wants this distance as wide
as possible; such hyperplane is named **maximum-margin hyperplane**, and
the positive/negative points nearest to :math:`H` are named **support
vectors**. If we apply some **basis functions**
:math:`\phi(\mathbf{x}):\mathbb{R}^{m} \rightarrow \mathbb{R}^{M}` to
transform the data, just as before, then we aim to find the hyperplane
:math:`H:\mathbf{a}^{T}\mathbf{\phi}\left( \mathbf{x} \right) + \mathbf{b}`
in the space transformed by :math:`\phi`. The basis functions are
referred to as the **kernel** in the context of SVM.

TODO Illustration of 2D MMH

+-----------------------------------+-----------------------------------+
| :math:`l` is too near positive    | a better choice of :math:`l`      |
| points, and the “street” is not   |                                   |
| wide enough                       |                                   |
+-----------------------------------+-----------------------------------+
| *Figure* *2‑2 In (a), (b) and (c) |                                   |
| the hyperplane* :math:`H` *makes  |                                   |
| correct classification, but in    |                                   |
| (a)* :math:`H` *does not have     |                                   |
| maximum margin, in (b)* :math:`H` |                                   |
| *does have equal distance to      |                                   |
| support vectors. (c) is the       |                                   |
| solution chosen by SVM.*          |                                   |
+-----------------------------------+-----------------------------------+

For a labeled point :math:`\mathbf{x}`, we denote it as
:math:`\mathbf{x}_{+}` if it is positively labelled, and
:math:`\mathbf{x}_{-}` if it is negatively labelled. We then try to
solve :math:`\mathbf{\omega}` and :math:`b` by
:math:`\left\{ \begin{matrix}
\mathbf{\omega \cdot}\mathbf{x}_{\mathbf{+}} + b \geq 1 \\
\mathbf{\omega \cdot}\mathbf{x}_{\mathbf{-}} + b \leq - 1 \\
\end{matrix} \right.\ ` for every :math:`\mathbf{x}_{\mathbf{+}}` and
:math:`\mathbf{x}_{\mathbf{-}}`. Note on the right-hand side of the
inequalities, the constants are :math:`\pm 1`, not :math:`0`. That is
because in the above decision rule
:math:`\frac{\mathbf{\omega \cdot u}}{\left| \mathbf{\omega} \right|^{2}} = 1`
is the median line, and we want the nearest positive point and the
nearest negative point have certain equal distance to the median line,
i.e., :math:`\left\{ \begin{matrix}
\frac{\mathbf{\omega \cdot}\mathbf{x}_{\mathbf{+}}}{\left| \mathbf{\omega} \right|^{2}} \geq 1 + \epsilon \\
\frac{\mathbf{\omega \cdot}\mathbf{x}_{\mathbf{-}}}{\left| \mathbf{\omega} \right|^{2}} \leq 1 - \epsilon \\
\end{matrix} \right.\  \Rightarrow \left\{ \begin{matrix}
\mathbf{\omega \cdot}\mathbf{x}_{\mathbf{+}} + b \geq \epsilon \\
\mathbf{\omega \cdot}\mathbf{x}_{\mathbf{-}} + b \leq - \epsilon \\
\end{matrix} \right.\  \Rightarrow \left\{ \begin{matrix}
\frac{\mathbf{\omega}}{\epsilon}\mathbf{\cdot}\mathbf{x}_{\mathbf{+}} + \frac{b}{\epsilon} \geq 1 \\
\frac{\mathbf{\omega}}{\epsilon}\mathbf{\cdot}\mathbf{x}_{\mathbf{-}} + \frac{b}{\epsilon} \leq - 1 \\
\end{matrix} \right.\ ` for some :math:`\epsilon \geq 0` as the
distance. The normalization by :math:`\epsilon` does not affect the
nature of our problem since :math:`\mathbf{\omega,}b` are unknowns, and
thus :math:`\left\{ \begin{matrix}
\mathbf{\omega \cdot}\mathbf{x}_{\mathbf{+}} + b \geq 1 \\
\mathbf{\omega \cdot}\mathbf{x}_{\mathbf{-}} + b \leq - 1 \\
\end{matrix} \right.\ ` is indeed the inequalities we are looking for.
We notice we can further simplify it by applying the label of a point
:math:`y = \left\{ \begin{matrix}
 + 1 & \text{for}\ \mathbf{x}_{+} \\
 - 1 & \text{for}\ \mathbf{x}_{-} \\
\end{matrix} \right.\ ` on both sides of both inequalities, we merge
them into a single form
:math:`y(\mathbf{x})(\mathbf{\omega \cdot x} + b) \geq 1` for every
:math:`\mathbf{x}`. Note that

1) From this moment on :math:`\mathbf{\omega}` does not necessarily have
   its end point on the median line due to the normalization of
   :math:`\epsilon` to :math:`1`, but is still a vector perpendicular to
   the median line.

2) We have deliberately let positive points on the right-hand side to
   have this neat inequality, otherwise it would be
   “\ :math:`\leq - 1`\ ”.

Now we want to implement the idea that the “street” should be as wide as
possible. Note there must be at least one positive point
:math:`\mathbf{x}_{+}^{'}` and one negative point
:math:`\mathbf{x}_{-}^{'}` on each side of the street (so at least two
support vectors), and it is not hard to see the width of the street is
exactly

.. math:: \frac{\mathbf{\omega}}{\mathbf{|}\mathbf{\omega}\mathbf{|}} \cdot \mathbf{x}_{+}^{'} - \frac{\mathbf{\omega}}{\left| \mathbf{\omega} \right|} \cdot \mathbf{x}_{-}^{'} = \frac{\mathbf{\omega}}{\left| \mathbf{\omega} \right|} \cdot \mathbf{x}_{+}^{'} + b - \left( \frac{\mathbf{\omega}}{\left| \mathbf{\omega} \right|} \cdot \mathbf{x}_{-}^{'} + b \right) = \ \frac{\mathbf{\omega}}{\left| \mathbf{\omega} \right|} \cdot \left( \mathbf{x}_{+}^{'} - \mathbf{x}_{-}^{'} \right) = \frac{2}{|\mathbf{\omega}|}

Thus we want to maximize :math:`\frac{2}{|\mathbf{\omega}|}`, and
equivalently we can minimize
:math:`\frac{1}{2}\left| \mathbf{\omega} \right|^{2}` for mathematical
convenience of taking derivatives later. Now the SVM construction is
converted to an optimization problem

:math:`\min{\ \frac{1}{2}\left| \mathbf{\omega} \right|^{2}}` s.t.
:math:`y(\mathbf{x})(\mathbf{\omega \cdot x} + b) \geq 1` for every
:math:`\mathbf{x \in}D`

**Generalized Linear Models**
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Graphical Models
~~~~~~~~~~~~~~~~

**Clustering**
--------------

Classic Clustering Analysis. Classic clustering methods are typically based on mutual distances of data points and some threshold distances. We first introduce the **Hierarchical Clustering Analysis** (HCA), from which data points are visualized through a tree-like structure, called **dendrogram tree**, where each non-leaf node is a set of partitions, and each leaf is a set of data points. One algorithm is called **agglomerative HCA**, described as below,
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

1) At the beginning, each data point is treated as a cluster itself,
   i.e. a singleton cluster.

2) Merge two “closest” clusters to merge together, where different
   measures of closeness might apply.

3) Repeat step 2) until only one cluster remains.

Taking the following simple example with data :math:`\{ a,b,c,d,e\}` in
*Figure 0‑1*. At the beginning we have singleton clusters
:math:`\left\{ a \right\},\left\{ b \right\},\left\{ c \right\},\left\{ d \right\},\{ e\}`,
and we use minimum point distance :math:`\mathcal{d}` below as the
distance measure of two clusters, where :math:`d` could be any metric of
points,

.. math:: \mathcal{d}\left( C_{1},C_{2} \right) = \min{\{ d\left( x_{1},x_{2} \right):x_{1} \in C_{1},x_{2} \in C_{2}\}}

For example, by *Figure 0‑1* we have

.. math:: \mathcal{d}\left( \left\{ a \right\},\left\{ b \right\} \right) = d\left( a,b \right)\mathcal{,\ d}\left( \left\{ b \right\},\left\{ c,d \right\} \right) = d\left( b,c \right)\mathcal{,d}\left( \left\{ a,b \right\},\left\{ c,d \right\} \right) = d(b,c)

We first merge two nearest clusters :math:`\{ c\}` and :math:`\{ d\}` to
derive four clusters
:math:`\left\{ a \right\},\left\{ b \right\},\left\{ c,d \right\},\{ e\}`.
Again we merge two nearest clusters :math:`\{ a\}` and :math:`\{ b\}`,
and then merge :math:`\{ a,b\}` and :math:`\{ c,d\}`. The last step is
to merge :math:`\{ e\}` to form a single cluster. Then we might draw a
line of threshold time for desired clusters.

*Figure* *0‑1 An example of agglomerative HCA*

One may look for other closeness measures like

.. math:: \mathcal{d}\left( C_{1},C_{2} \right) = \max{\{ d\left( x_{1},x_{2} \right):x_{1} \in C_{1},x_{2} \in C_{2}\}}

1) average distance:

.. math:: \mathcal{d}\left( C_{1},C_{2} \right) = \operatorname{}{\{ d\left( x_{1},x_{2} \right):x_{1} \in C_{1},x_{2} \in C_{2}\}}

2) centroid distance:

.. math:: \mathcal{d}\left( C_{1},C_{2} \right) = d\left( \frac{1}{\left| C_{1} \right|}\sum_{x \in C_{1}}^{}x,\frac{1}{|C_{2}|}\sum_{x \in C_{2}}^{}x \right)

Note :math:`\mathcal{d}` is not generally a metric, due to that it might
not satisfy the triangular inequality. Still taking *Figure 0‑1* and
minimum distance for example, we have

.. math:: \mathcal{d}\left( \left\{ a,b \right\},\left\{ c,d \right\} \right)\mathcal{+ d}\left( \left\{ c,d \right\},\left\{ e \right\} \right) = d\left( b,c \right) + d\left( d,e \right)\mathcal{< d}\left( \left\{ a,b \right\},\left\{ e \right\} \right) = d(b,e)

Another simple algorithm is **divisive HCA**, which is kind of converse
of agglomerative HCA. Divisive HCA starts with the whole dataset as a
single cluster, and at each step points with farthest distance to the
remaining points in the same cluster are picked out, until in each
cluster every point has equal distance to the remaining part and no new
clusters can be formed. In *Figure 0‑1*, that process would be

1) First picking out :math:`\{ e\}` and it is easy to verify that
   :math:`\{ e\}` has the farthest distance to the remaining part.

2) Then we look at non-singleton cluster :math:`\{ a,b,c,d\}` and find
   :math:`a,b` have the equal distance to the remaining part of that
   cluster, and hence :math:`\{ a,b\}` are picked out to form a cluster
   itself. Note no new cluster can be found in :math:`\{ a,b\}`.

3) Last, :math:`\{ c,d\}` form a cluster where no further cluster can be
   found, and the divisive HCA is done.

Spectral Clustering
~~~~~~~~~~~~~~~~~~~

**Probabilistic Modeling**
--------------------------

Basic Distributions. Some frequently used distributions, like exponential distribution, Poisson distribution and Gaussian distribution arise from analysis of **random process**. To our purpose, we can consider a random process as a family of RVs :math:`X_{t},t \in T` where :math:`T` is an index set. Even although random process is a very sophisticated mathematical concept whose rigorous treatment requires advanced mathematical knowledge such like measure theory, probability theory, etc.
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

A **discrete-time Bernoulli process** is a countable random process
:math:`X_{t},t = 1,2,\ldots` where each RV :math:`X_{t}` obeys
**Bernoulli distribution** :math:`Bernoulli(p)`, and the number of
successes before time :math:`t = n` follows **binomial distribution**
:math:`\text{Binomial}(n,p)`. Bernoulli process is equivalent to a
random walk on a trivial Markov chain characterized by two states
:math:`\{ 0,1\}`, initial distribution
:math:`\mathbf{\mu} = \left( 1 - p,p \right)` and transition matrix
:math:`\mathbf{P} = \begin{pmatrix}
1 - p & p \\
1 - p & p \\
\end{pmatrix}`. Since :math:`\mathbf{\mu} = \mathbf{\text{μP}}`,
:math:`\mathbf{\mu}` is also the stationary distribution.

Binomial distribution can be viewed as a model of drawing balls from an
urn containing :math:`N` balls of which :math:`K` are red balls and the
rest are blue balls. The drawing is with replacement, i.e. the drawn
ball will be put back into the urn. When considered this way, the
probability distribution of drawing :math:`k` red balls in :math:`n`
trials is :math:`\mathbb{P\{}X = k\} = \begin{pmatrix}
n \\
k \\
\end{pmatrix}{(\frac{K}{N})}^{k}{(\frac{N - K}{N})}^{n - k}`. These
drawings with replacements are independent Bernoulli trials.

Now if the drawn balls are not returned to the urn, then the
distribution of drawing :math:`k` red balls in :math:`n` trials becomes
**hypergeometric distribution**. The support of a binomial distribution
ranges from :math:`0` to :math:`n`, while the support of the
hypergeometric distribution is
:math:`\left\lbrack \max\left( 0,n - \left( N - K \right) \right),\min\left( n,K \right) \right\rbrack`
since

1) At least :math:`n - (N - K)` red balls are drawn when the number of
   trials :math:`n` is more than the total number of blue balls
   :math:`N - K`;

2) it is impossible to draw more than :math:`K` red balls from the urn.

For example, if the urn has :math:`7` red balls and :math:`3` blue
balls, and we conduct :math:`5` trials, then minimum number of possible
red balls is :math:`2` and the maximum is :math:`5`. The distribution is
given by the following formula:

.. math::

   \mathbb{P\{}X = k\} = \frac{\begin{pmatrix}
   K \\
   k \\
   \end{pmatrix}\begin{pmatrix}
   N - K \\
   n - k \\
   \end{pmatrix}}{\begin{pmatrix}
   N \\
   n \\
   \end{pmatrix}}

where :math:`\begin{pmatrix}
N \\
n \\
\end{pmatrix}` is the total number of ways of drawing :math:`n` balls
from a population of :math:`N`, :math:`\begin{pmatrix}
K \\
k \\
\end{pmatrix}` is the total number of ways of drawing :math:`k` red
balls from a total of :math:`K` red balls, and :math:`\begin{pmatrix}
N - K \\
n - k \\
\end{pmatrix}` is the total number of ways of drawing :math:`n - k` blue
balls from a total of :math:`N - K` blue balls. If a random variable
:math:`X` follows the hypergeometric distribution, it is denoted as
:math:`X\sim\text{Hypergeometric}(N,K,n)` or :math:`X\sim H(N,K,n)` for
short.

The numerator of the formula can be interpreted as a two-step selection
– first choosing :math:`k` red balls from a total of :math:`K` red
balls, and then choosing :math:`n - k` blue balls from a total of
:math:`N - K` blue balls, and the total number of different combinations
are :math:`\begin{pmatrix}
K \\
k \\
\end{pmatrix}\begin{pmatrix}
N - K \\
n - k \\
\end{pmatrix}`. The support can also be seen directly from the formula,
i.e. :math:`k \leq K` and :math:`n - k \leq N - K`, which yields the
above-mentioned support.

-  *Theorem* *0‑1* **Vandermonde's identity** :math:`\begin{pmatrix}
   m + n \\
   r \\
   \end{pmatrix} = \sum_{k = 0}^{r}{\begin{pmatrix}
   m \\
   k \\
   \end{pmatrix}\begin{pmatrix}
   n \\
   r - k \\
   \end{pmatrix}}`. The PMF of hypergeometric distribution sums to
   :math:`1`: :math:`\sum_{k = 0}^{n}\frac{\begin{pmatrix}
   K \\
   k \\
   \end{pmatrix}\begin{pmatrix}
   N - K \\
   n - k \\
   \end{pmatrix}}{\begin{pmatrix}
   N \\
   n \\
   \end{pmatrix}} = \frac{\sum_{k = 0}^{n}{\begin{pmatrix}
   K \\
   k \\
   \end{pmatrix}\begin{pmatrix}
   N - K \\
   n - k \\
   \end{pmatrix}}}{\begin{pmatrix}
   N \\
   n \\
   \end{pmatrix}} = 1`, whereby we can see that
   :math:`\sum_{k = 0}^{n}{\begin{pmatrix}
   K \\
   k \\
   \end{pmatrix}\begin{pmatrix}
   N - K \\
   n - k \\
   \end{pmatrix}} = \begin{pmatrix}
   N \\
   n \\
   \end{pmatrix}`. Replace :math:`n` by :math:`k`, and :math:`K` by
   :math:`m` reassign :math:`n = N - K = N - m`, then
   :math:`\sum_{k = 0}^{r}{\begin{pmatrix}
   m \\
   k \\
   \end{pmatrix}\begin{pmatrix}
   n \\
   r - k \\
   \end{pmatrix}} = \begin{pmatrix}
   m + n \\
   r \\
   \end{pmatrix}`.

-  *Theorem* *0‑2* Sum of independent Binomial RVs is still Binomial.
   Suppose :math:`X_{1}\sim\text{Binomial}(n_{1},p)` and
   :math:`X_{2}\sim\text{Binomial}(n_{2},p)` and they are independent,
   then the mass function of :math:`X_{1} + X_{2}` is

.. math::

   p\left\{ X_{1} + X_{2} = k \right\} = \sum_{k_{1} + k_{2} = k}^{}\begin{pmatrix}
   n_{1} \\
   k_{1} \\
   \end{pmatrix}\left( 1 - p \right)^{n_{1} - k_{1}}p^{k_{1}}\begin{pmatrix}
   n_{2} \\
   k_{2} \\
   \end{pmatrix}\left( 1 - p \right)^{n_{2} - k_{2}}p^{k_{2}} = \begin{pmatrix}
   n_{1} + n_{2} \\
   k \\
   \end{pmatrix}\left( 1 - p \right)^{n_{1} + n_{2} - k}p^{k}\sum_{k_{1} + k_{2} = k}^{}\frac{\begin{pmatrix}
   n_{1} \\
   k_{1} \\
   \end{pmatrix}\begin{pmatrix}
   n_{2} \\
   k_{2} \\
   \end{pmatrix}}{\begin{pmatrix}
   n_{1} + n_{2} \\
   k \\
   \end{pmatrix}} = \begin{pmatrix}
   n_{1} + n_{2} \\
   k \\
   \end{pmatrix}{(1 - p)}^{n_{1} + n_{2} - k}p^{k}

   where :math:`\sum_{k_{1} + k_{2} = k}^{}\frac{\begin{pmatrix}
   n_{1} \\
   k_{1} \\
   \end{pmatrix}\begin{pmatrix}
   n_{2} \\
   k_{2} \\
   \end{pmatrix}}{\begin{pmatrix}
   n_{1} + n_{2} \\
   k \\
   \end{pmatrix}}` is the sum of individual probabilities of the PMF of
   the hypergeometric distribution. This indicates
   :math:`X_{1} + X_{2}\sim\text{Binomial}(n_{1} + n_{2},\ p)`.

-  The **first arrival time** :math:`\tau` of Bernoulli process is a RV
   representing the time of first success. It is easy to see
   :math:`\mathbb{P}\left\{ \tau = t \right\} = \left( 1 - p \right)^{t - 1}p`
   with support :math:`t \in \lbrack 1, + \infty)`. The distribution
   with this PMF is called the **geometric distribution**.

   Fix :math:`\tau_{0} = 0`, and let :math:`\tau_{k}` denote the
   **waiting time** for the :math:`k`\ th arrival, then
   :math:`\omega_{k} = \tau_{k} - \tau_{k - 1}` is the **inter-arrival
   time** for the :math:`k`\ th arrival. The PMF of :math:`\tau_{k}` is
   :math:`P\left\{ \tau_{k} = t \right\} = \begin{pmatrix}
   t - 1 \\
   k - 1 \\
   \end{pmatrix}\left( 1 - p \right)^{t - 1 - (k - 1)}p^{k - 1}p = \begin{pmatrix}
   t - 1 \\
   k - 1 \\
   \end{pmatrix}\left( 1 - p \right)^{t - k}p^{k}` where
   :math:`t \geq k`, since all trials before the :math:`k`\ th arrival
   are :math:`t - 1` times of independent Bernoulli trials with
   :math:`k - 1` successes. The distribution of this PMF is named
   **Pascal distribution**, whose special case :math:`k = 1` is the
   geometric distribution.

+-----------------------------------+-----------------------------------+
| |image8|                          | |image9|                          |
+===================================+===================================+
| *Pascal distribution with the     | *Pascal distribution with the     |
| same* :math:`k = 10` *but         | same* :math:`p = 0.4` *but        |
| varying* :math:`p`\ *.*           | varying* :math:`k`\ *.*           |
| :math:`\tau_{k}` *is more likely  | :math:`\tau_{k}` *is more likely  |
| to have smaller value with        | to have large value when*         |
| larger* :math:`p`\ *, as          | :math:`k` *grows, as expected.*   |
| expected.*                        |                                   |
|                                   | .. math:: k = 5,\ k = 8,\ k = 10  |
| .. math:: p = 0.4,\ p = 0.5,\ p = |                                   |
|  0.6                              |                                   |
+-----------------------------------+-----------------------------------+
| *Figure* *0‑1 Illustrations of    |                                   |
| Pascal Distribution.*             |                                   |
+-----------------------------------+-----------------------------------+

:math:`\{\tau_{k}\}` are not independent, since for any
:math:`i < j,s < t`, we have
:math:`\mathbb{P}\left\{ \tau_{j} = t|\tau_{i} = s \right\} = 0` for
:math:`t \leq s` (if the :math:`i`\ th success happens at time
:math:`s`, a later :math:`j`\ th success must happen later than time
:math:`s`), however,
:math:`\mathbb{P}\left\{ \tau_{j} = t \right\} \neq 0` for any
:math:`t \geq j` and it is possible that :math:`j \leq s`.

:math:`\{\omega_{k}\}` are independent and they obey the geometric
distribution. A rigorous way to see this is the strong Markov property.
Note :math:`\tau_{k}` is a stopping time, since the event
:math:`\{\tau_{k} = t\}` can be determined by just looking at
:math:`X_{1},\ldots,X_{t}`, and hence waiting times
:math:`\omega_{1} = \tau_{1},\omega_{2} = \tau_{2} - \tau_{1},\ldots,\omega_{n} = \tau_{n} - \tau_{n - 1}`
are independent.

The **continuous-time Bernoulli process** is a set of RVs
:math:`X_{t},t \in (0, + \infty)` where each :math:`X_{t}` obeys
Bernoulli distribution :math:`Bernoulli(p)`. The objective is to find
the probability of having :math:`k` arrivals before time :math:`t`. The
assumption is that if :math:`t` is divided into :math:`n` time slots of
equal length :math:`\delta`, then the probability of having :math:`k`
arrivals in each interval is

.. math::

   p\left( k,\delta \right) = o\left( \delta^{2} \right) + \left\{ \begin{matrix}
   1 - \lambda\delta & k = 0 \\
   \text{λδ} & k = 1 \\
   0 & k > 1 \\
   \end{matrix} \right.\

where :math:`o\left( \delta^{2} \right)` is a second-order infinitesimal
of :math:`\delta` and :math:`\lambda > 0`. This assumption has following
implications,

1) The probability of have one arrival in the small time window is
   proportional to its length by a constant ratio :math:`\lambda`, which
   is named **arrival rate** or **success rate**.

2) When :math:`\delta \rightarrow 0`, the probability of have no
   arrivals in the small time interval :math:`\delta` approaches 1.

3) When :math:`\delta \rightarrow 0`, the probability of getting two or
   more arrivals in the small interval :math:`\delta` approaches 0;
   moreover, this probability degenerates quadratically, faster than the
   probability of having one arrival.

Let :math:`\mathbb{P(}k;\lambda,t)` be the probability of have :math:`k`
arrivals before time :math:`t` with arrival rate :math:`\lambda`. Under
above assumption, :math:`\mathbb{P(}k;\lambda,t)` becomes the PMF of a
binomial distribution, that is, :math:`k` successes in
:math:`n = \frac{t}{\delta}` trials. To see this, check that

.. math::

   \mathbb{P}\left( k;\lambda,t \right) = \lim_{n \rightarrow \infty}\sum_{n_{0} + n_{1} + n_{2} = n}^{}{\left( 1 - \frac{\text{λt}}{n} + o\left( \delta^{2} \right) \right)^{n_{0}}\left( \frac{\text{λt}}{n} + o\left( \delta^{2} \right) \right)^{n_{1}}}\left( o\left( \delta^{2} \right) \right)^{n_{2}} = \lim_{n \rightarrow \infty}\begin{pmatrix}
   n \\
   k \\
   \end{pmatrix}\left( \frac{\text{λt}}{n} \right)^{k}\left( 1 - \frac{\text{λt}}{n} \right)^{n - k}

where :math:`n_{0}` is the number of slots without arrivals,
:math:`n_{1}` the number of slots with one arrival and :math:`n_{2}` the
number of slots with more than one arrivals. Continue the limit
calculation we will have

.. math::

   \mathbb{P}\left( k;\lambda,t \right) = \lim_{n \rightarrow \infty}\begin{pmatrix}
   n \\
   k \\
   \end{pmatrix}\left( \frac{\text{λt}}{n} \right)^{k}\left( 1 - \frac{\text{λt}}{n} \right)^{n - k} = \frac{\left( \text{λt} \right)^{k}}{k!}\lim_{n \rightarrow \infty}\frac{n!}{\left( n - k \right)! \times n^{k}}\lim_{n \rightarrow \infty}\left( 1 - \frac{\text{λt}}{n} \right)^{n - k} = \frac{\left( \text{λt} \right)^{k}}{k!}e^{- \text{λt}}

where
:math:`\lim_{n \rightarrow \infty}\frac{n!}{\left( n - k \right)! \times n^{k}} = 1`
and
:math:`\lim_{n \rightarrow \infty}\left( 1 - \frac{\text{λt}}{n} \right)^{n - k} = e^{- \lambda t}`.
Define a discrete distribution of a RV :math:`X` with PMF
:math:`\mathbb{P}\left\{ X = k \right\} = \frac{\lambda^{k}}{k!}e^{- \lambda}`
as the **Poisson distribution** with parameter :math:`\lambda`, which is
the probability of have :math:`k` arrivals before time :math:`1`, and we
use :math:`X\sim\text{Poisson}(\lambda)` to say :math:`X` obeys Poisson
distribution with parameter :math:`\lambda`.
:math:`\mathbb{P(}k;\lambda,t)` obeys Poisson distribution with
parameter :math:`\text{λt}`. :math:`\text{λt}` is both the expected
value of the Poisson distribution and the underlying binomial
distribution. :math:`\text{λt}` is also the variance of the Poisson, as
well as the limiting variance of the underlying binomial distribution:
as :math:`\delta \rightarrow 0`, :math:`p = \lambda\delta \rightarrow 0`
and :math:`(1 - p) \rightarrow 1`, therefore
:math:`\text{np}\left( 1 - p \right) \rightarrow np = n\lambda\delta = \lambda t`.

-  Sum of independent Poisson RVs is also a Poisson. Suppose
   :math:`X_{1}\sim\text{Poisson}\left( \lambda_{1} \right)`,
   :math:`X_{2}\sim\text{Poisson}\left( \lambda_{2} \right)`, then the
   PMF of :math:`X_{1} + X_{2}` is calculated as the following,

.. math::

   {\mathbb{P}\left\{ X_{1} + X_{2} = k \right\} = \sum_{k_{1} + k_{2} = k}^{}{\frac{\lambda_{1}^{k_{1}}}{k_{1}!}e^{- \lambda_{1}} \times \frac{\lambda_{2}^{k_{2}}}{k_{2}!}e^{- \lambda_{2}}} = \frac{e^{- (\lambda_{1} + \lambda_{2})}}{k!}\sum_{k_{1} + k_{2} = k}^{}\frac{k!}{k_{1}!k_{2}!}\lambda_{1}^{k_{1}}\lambda_{2}^{k_{2}}
   }{= \frac{e^{- (\lambda_{1} + \lambda_{2})}}{k!}\sum_{k_{1} + k_{2} = k}^{}\begin{pmatrix}
   k \\
   k_{1} \\
   \end{pmatrix}\lambda_{1}^{k_{1}}\lambda_{2}^{k_{2}} = \frac{e^{- (\lambda_{1} + \lambda_{2})}}{k!}{(\lambda_{1} + \lambda_{2})}^{k}}

where\ :math:`{(\lambda_{1} + \lambda_{2})}^{k} = \sum_{k_{1} + k_{2} = k}^{}\begin{pmatrix}
k \\
k_{1} \\
\end{pmatrix}\lambda_{1}^{k_{1}}\lambda_{2}^{k_{2}}` is by the famous
binomial theorem. It follows that
:math:`X_{1} + X_{2}\sim\ \text{Poisson}\left( \lambda_{1} + \lambda_{2} \right)`.

-  The **first arrival time** :math:`\text{τ\ }`\ for the continuous
   Bernoulli process is derived by calculating the probability of no
   success taking places within time :math:`t`, :math:`1` success
   happening between time :math:`t` and time :math:`t + \delta`, and
   then let :math:`\delta \rightarrow 0`, as shown below.

..

   The probability of this particular event happening is
   :math:`\mathbb{P}\left( 0;\lambda,t \right) \times \left( \lambda\delta + o\left( \delta^{2} \right) \right)`.
   Denote the density function of :math:`\tau` as :math:`f(t;\lambda)`
   and we have
   :math:`\int_{t}^{t + \delta}{f(t;\lambda)dt}\mathbb{= P}\left( 0;\lambda,t \right) \times (\lambda\delta + o(\delta^{2}))`.
   As :math:`\delta \rightarrow 0`

.. math:: f\left( t;\lambda \right) = \frac{\partial\left( \int_{t}^{t + \delta}{f\left( t;\lambda \right)\text{dt}} \right)}{\partial\delta} = \operatorname{}\frac{\int_{t}^{t + \delta}{f(t;\lambda)dt}}{\delta} = \operatorname{}{\frac{\mathbb{P}\left( 0;\lambda,t \right)\left( \lambda\delta + o\left( \delta^{2} \right) \right)}{\delta} = \lambda e^{- \lambda t}}

   Define a distribution with PDF
   :math:`f\left( x \right) = \lambda e^{- \lambda x}` as the
   **exponential distribution** with parameter :math:`\lambda`. Clearly
   the first arrival time :math:`\tau` obeys exponential distribution
   with parameter :math:`\lambda`.The probability of not getting a
   success along with time decreases exponentially. Exponential
   distribution can be viewed as the continuous version of geometric
   distribution. Generally, when :math:`p` is smaller, the geometric
   distribution is more near the exponential distribution with parameter
   :math:`\lambda = p`.

|image10|

:math:`p = \lambda = 0.2,` :math:`p = \lambda = 0.8`

*Figure* *0‑2 Comparison between exponential distribution and geometric
distribution*

-  The **waiting time** for the :math:`k`\ th success (:math:`k \geq 1`)
   is derived by letting :math:`k - 1` successes happen within time
   :math:`t` and :math:`1` success happen between time :math:`t` and
   time :math:`t + \delta`, and them making
   :math:`\delta \rightarrow 0`. By a similar inference shown above, we
   have the PDF
   :math:`f\left( t;\lambda,k \right) = \frac{\left( \text{λτ} \right)^{k - 1}}{(k - 1)!}\text{λe}^{- \text{λτ}}`
   (:math:`k \geq 1`). The distribution of this PDF is named **Erlang
   Distribution**, of which the special case :math:`k = 1` is the
   exponential distribution.

+-----------------------------------+-----------------------------------+
| |image13|                         | |image14|                         |
+===================================+===================================+
| *Erlang distribution with the     | *Erlang distribution with the     |
| same* :math:`k = 10` *but         | same* :math:`\lambda = 0.4` *but  |
| varying* :math:`\lambda`          | varying* :math:`k`                |
|                                   |                                   |
| .. math:: p = 0.4,\ p = 0.5,\ p = | .. math:: k = 5,\ k = 8,\ k = 10  |
|  0.6                              |                                   |
+-----------------------------------+-----------------------------------+

..

   *Figure* *0‑3 Illustrations of Erlang Distribution.*

   Not surprisingly, Erlang distribution is the continuous version of
   Pascal distribution, and generally when :math:`p` is smaller, the
   Pascal distribution is more near the Erlang with parameter
   :math:`\lambda = p`.

+-----------------------------------------------------------------------+
| |image16|                                                             |
+=======================================================================+
| *Figure* *0‑4 Comparison between Erlang distribution and Pascal       |
| distribution when* :math:`k = 5`                                      |
|                                                                       |
| .. math:: p = \lambda = 0.2,\ p = \lambda = 0.4                       |
+-----------------------------------------------------------------------+

With the above discussion, the relation between discrete-time and
continuous-time Bernoulli process can be clearly seen from the following
table.

+-----------------------+-----------------------+-----------------------+
|                       | Continuous-time       | Discrete-time         |
+=======================+=======================+=======================+
| Time continuity       | Continuous            | Discrete              |
+-----------------------+-----------------------+-----------------------+
| Arrival rate          | :math:`\lambda` per   | :math:`p` per time    |
|                       | unit time             | slot                  |
+-----------------------+-----------------------+-----------------------+
| # of arrivals within  | Poisson               | Binomial              |
| a time interval       |                       |                       |
+-----------------------+-----------------------+-----------------------+
| Waiting time for      | Exponential           | Geometric             |
| arrival               |                       |                       |
+-----------------------+-----------------------+-----------------------+
| Time of the           | Erlang                | Pascal                |
| :math:`\mathbf{k}`\ t |                       |                       |
| h                     |                       |                       |
| arrival               |                       |                       |
+-----------------------+-----------------------+-----------------------+

MLE, MAP & Naïve Bayes
~~~~~~~~~~~~~~~~~~~~~~

**Maximum Likelihood Estimator** (MLE) is the most classic estimator for
parameters or latent variables. It is taking derivatives of logarithms
of :math:`\mathbb{P(}X|\mathbf{\theta})` for discrete distributions or
:math:`f(X|\mathbf{\theta})` for continuous distributions w.r.t.
:math:`\mathbf{\theta}` where :math:`X` is the sample, and estimate
:math:`\mathbf{\theta}` as the one :math:`\widehat{\mathbf{\theta}}`
that maximizes :math:`\mathbb{P(}X|\mathbf{\theta})` or
:math:`f(X|\mathbf{\theta})`. For example, if :math:`X_{1},\ldots,X_{n}`
are i.i.d. RV from an exponential distribution, then

.. math:: L\left( X_{1},\ldots,X_{n}|\theta \right) = \log{f\left( X_{1},\ldots,X_{n} \middle| \theta \right)} = \log{\prod_{i = 1}^{n}{\theta e^{- \theta x_{i}}}} = \sum_{i = 1}^{n}{\log{\theta e^{- \theta x_{i}}}} = n\log\theta - \theta\sum_{i = 1}^{n}x_{i} \Rightarrow \frac{dL}{d\theta} = \frac{n}{\theta} - \sum_{i = 1}^{n}x_{i} \Rightarrow \widehat{\theta} = \frac{1}{\overline{X}}

In general, we write
:math:`{\widehat{\mathbf{\theta}}}_{\text{MLE}} = \arg{\max{\mathbb{P(}X|\mathbf{\theta})}}`
or
:math:`{\widehat{\mathbf{\theta}}}_{\text{MLE}} = \arg{\max{f(X|\mathbf{\theta})}}`.
On the contrary, **Maximum a Posterior Estimator** (MAP) finds
:math:`{\widehat{\mathbf{\theta}}}_{\text{MAP}} = \arg{\max{\mathbb{P(}\mathbf{\theta}|X)}}`
or
:math:`{\widehat{\mathbf{\theta}}}_{\text{MAP}} = \arg{\max{f(\mathbf{\theta}|X)}}`.
WLOG, from this point on we “misuse” :math:`\mathbb{P}` to also
represent density. Check that

.. math:: {\widehat{\mathbf{\theta}}}_{\text{MAP}} = \arg{\max{\mathbb{P(}\mathbf{\theta}|X)}} = {\widehat{\mathbf{\theta}}}_{\text{MAP}} = \arg{\max\frac{\mathbb{P}\left( X \middle| \mathbf{\theta} \right)\mathbb{P(}\mathbf{\theta})}{\mathbb{P(}X)}} = \arg{\max{\mathbb{P}\left( X \middle| \mathbf{\theta} \right)\mathbb{P(}\mathbf{\theta})}}

The last equation holds since :math:`\mathbb{P(}X)` is not dependent on
:math:`\theta` so it can be treated like a constant in the maximization,
even though it might be hard to compute what it really is. MAP allows
specification of prior distribution. When
:math:`\mathbb{P(}\mathbf{\theta})` is uniform, MAP degenerates to an
MLE.

**Naïve Bayes** is a simplest probabilistic classification model making
direct use of Bayes Theorem. Given an i.i.d. labelled sample
:math:`\left( X^{\left( i \right)}\mathbf{,}Y_{i} \right)\mathbf{\sim}\mathbb{P}_{\mathbf{\theta}},i = 1,\ldots,n`
where :math:`\mathbf{\theta}` is the parameter vector of
:math:`\mathbb{P}` and each :math:`X^{\left( i \right)}` is equipped
with :math:`k` featuers, i.e.
:math:`X^{\left( i \right)} = (X_{1}^{\left( i \right)},\ldots,X_{k}^{\left( i \right)})`.
We then evaluate the label of new observation
:math:`X\mathbf{= (}X_{1}\mathbf{,\ldots,}X_{k}\mathbf{)}` by the
following,

.. math:: \mathbb{P}_{\mathbf{\theta}}\left( Y \middle| X \right) = \frac{\mathbb{P}_{\mathbf{\theta}}(X|Y)\mathbb{P}_{\mathbf{\theta}}(Y)}{\mathbb{P}_{\mathbf{\theta}}(X)} \propto \mathbb{P}_{\mathbf{\theta}}(X|Y)\mathbb{P}_{\mathbf{\theta}}(Y)

The **Naive Bayes assumption** is that: given the label :math:`Y`, the
features are independent (but not necessarily identically distributed).
Thus,

.. math:: \mathbb{P}_{\mathbf{\theta}}\left( Y = y \middle| X \right) \propto \mathbb{P}_{\mathbf{\theta}}(Y = y)\prod_{i = 1}^{k}{\mathbb{P}_{\mathbf{\theta}}(X_{i}|Y = y)}

The model is now obviously
:math:`\widehat{y} = \arg{\operatorname{}{\mathbb{P}_{\widehat{\mathbf{\theta}}}(Y = y)\prod_{i = 1}^{k}{\mathbb{P}_{\widehat{\mathbf{\theta}}}(X_{i}|Y = y)}}}`.
The interpretation of this model is that the data are generated from the
following process:

1) The label is first drawn from according to
   :math:`\mathbb{P}_{\widehat{\mathbf{\theta}}}(Y = y)`.

2) Then each feature of :math:`\mathbf{x}` is drawn from
   :math:`\mathbb{P}_{\widehat{\mathbf{\theta}}}\left( X \middle| Y = y \right) = \prod_{i = 1}^{k}{\mathbb{P}_{\mathbf{\theta}}(X_{i}|Y = y)}`.

We typically assume
:math:`\mathbb{P}_{\widehat{\mathbf{\theta}}}(Y = y)` is a multinomial,
while :math:`\mathbb{P}_{\widehat{\mathbf{\theta}}}(X|Y = y)` is subject
to choice. An important reason for the Naïve Bayes assumption is that it
makes estimation of :math:`\theta` much easier by using every feature as
a sample point; an assumption on the joint distribution
:math:`\mathbb{P}_{\mathbf{\theta}}(X|Y)` might work, however it uses
every single feature vector :math:`\mathbf{x}` as a sample point and
requires a larger size of data for estimation. That is also a main
rational behind the claim that “inaccurate but simple models could work
better than accurate but complex models”.

If we assume the labels are from a multinomial :math:`\mathbf{\Phi}`,
and given the label, features are also from a multinomial
:math:`\mathbf{\Phi}_{l}` dependent on :math:`l`, then we can simply
evaluate both :math:`\mathbb{P(}l)` and
:math:`\mathbb{P(}\mathbf{x(}i\mathbf{)}|l)` by counts, and let

.. math:: \widehat{l} = \arg{\operatorname{}{\left( \frac{1}{n}\sum_{i = 1}^{n}\mathbb{I}_{l\left( \mathbf{x}_{i} \right) = l} \right) \times \prod_{i = 1}^{k}\frac{\mathbb{I}_{l\left( \mathbf{x}_{i} \right) = l}\sum_{j = 1}^{n}\mathbb{I}_{\mathbf{x}\left( j \right) = \mathbf{x}_{i}(j)}}{}}}

EM Algorithm. **Expectation Maximization Algorithm** (EM Algorithm) is to perform MLE or MAP when some variables are unobservable. Let’s first see an example. The algorithm emerges from estimating the latent means of **Gaussian mixtures**. Suppose we have a set of data :math:`X_{1},X_{2},\ldots,X_{n}`, and they are assumed to be generated from the following process,
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

1) Let
   :math:`(\mathbf{\mu}_{1},\mathbf{\Sigma}_{1}),\ldots,(\mathbf{\mu}_{k},\mathbf{\Sigma}_{k})`
   be the set of possible Gaussians, draw indexes
   :math:`Z_{1},\ldots,Z_{n}` from a multinomial
   :math:`(\phi_{1},\ldots,\phi_{k})` where it is possible that
   :math:`Z_{i} = Z_{j}` for some :math:`i \neq j`.

2) :math:`X_{1},X_{2},\ldots,X_{n}` are drawn from
   :math:`N\text{orm}\text{al}\left( \mathbf{\mu}_{Z_{1}},\mathbf{\Sigma}_{Z_{1}} \right),\ldots,\text{Normal}(\mathbf{\mu}_{Z_{n}},\mathbf{\Sigma}_{Z_{n}})`.

Here :math:`Z_{1},\ldots,Z_{n}` are latent variables. If we just assume
:math:`Z_{1} = z_{1},\ldots,Z_{n} = z_{n}` and
:math:`z_{1},\ldots,z_{n}` are known, then by MLE :math:`\phi_{i}` is
estimated by counts, and :math:`\mu_{i}` is estimated by the mean of the
data generated from
:math:`\text{Normal}\left( \mathbf{\mu}_{i},\mathbf{\Sigma}_{i} \right)`.

.. math::

   \left\{ \begin{matrix}
   {\widehat{\phi}}_{j} = \frac{1}{n}\sum_{i = 1}^{n}\mathbb{I}_{z_{i} = j} \\
   {\widehat{\mathbf{\mu}}}_{j} = \frac{\sum_{i = 1}^{n}{X_{i}\mathbb{I}_{z_{i} = j}}}{\sum_{i = 1}^{n}\mathbb{I}_{z_{i} = j}} \\
   {\widehat{\mathbf{\Sigma}}}_{j} = \frac{\sum_{i = 1}^{n}{\left( X_{i} - {\widehat{\mathbf{\mu}}}_{j} \right)^{T}\left( X_{i} - {\widehat{\mathbf{\mu}}}_{j} \right)\mathbb{I}_{z_{i} = j}}}{\sum_{i = 1}^{n}\mathbb{I}_{z_{i} = j}} \\
   \end{matrix} \right.\ ,j = 1,\ldots,k

Now by Bayesian rules,

.. math:: \omega_{i,j}\mathbb{:: = P}\left( Z_{i} = j \middle| X_{i} \right) = \frac{\mathbb{P}\left( X_{i} \middle| Z_{i} = j \right)\mathbb{P}\left( Z_{i} = j \right)}{\sum_{j = 1}^{k}{\mathbb{P}\left( X_{i} \middle| Z_{i} = j \right)\mathbb{P}\left( Z_{i} = j \right)}}

where (omits parameters for clarity)

.. math:: \mathbb{P}\left( X_{i} \middle| Z_{i} = j \right)\mathbb{P}\left( Z_{i} = j \right) = \left( \frac{1}{\left( 2\pi \right)^{\frac{n}{2}}\left| \mathbf{\Sigma}_{j} \right|^{\frac{1}{2}}}\exp{( - \frac{1}{2}\left( \mathbf{x}_{i} - \mathbf{\mu}_{j} \right)^{T}\mathbf{\Sigma}_{j}^{- 1}(\mathbf{x}_{i} - \mathbf{\mu}_{j}))} \right) \times \phi_{j}

The E-Step for EM Algorithm for Gaussian Mixtures is first guess some
initial values for :math:`\phi_{j}`, :math:`\mathbf{\mu}_{j}` and
:math:`\mathbf{\Sigma}_{j}`, and them compute the quantity
:math:`\omega_{i,j}` for every :math:`i,j`, and then the M-step updates
:math:`\phi_{j}`, :math:`\mathbf{\mu}_{j}` and
:math:`\mathbf{\Sigma}_{j}` by the following formula, where basically
:math:`\omega_{i,j}` takes place of :math:`\mathbb{I}_{z_{i} = j}` of
those estimations of MLE.

.. math::

   \left\{ \begin{matrix}
   {\widehat{\phi}}_{j} = \frac{1}{n}\sum_{i = 1}^{n}\omega_{i,j} \\
   {\widehat{\mathbf{\mu}}}_{j} = \frac{\sum_{i = 1}^{n}{X_{i}\omega_{i,j}}}{\sum_{i = 1}^{n}\omega_{i,j}} \\
   {\widehat{\mathbf{\Sigma}}}_{j} = \frac{1}{\sum_{i = 1}^{n}\omega_{i,j}}\sum_{i = 1}^{n}{\left( X_{i} - {\widehat{\mathbf{\mu}}}_{j} \right)^{T}\left( X_{i} - {\widehat{\mathbf{\mu}}}_{j} \right)\omega_{i,j}} \\
   \end{matrix} \right.\ ,j = 1,\ldots,k

The above formulas are to be derived later. The EM Algorithm iteratively
repeats the E-step and M-step until a satisfactory convergence is
reached. Now we introduce the general EM algorithm. Given a
probabilistic model :math:`\mathbb{P}` parameterized by
:math:`\mathbf{\theta}`, and data
:math:`X_{1} = \mathbf{x}_{1},\ldots,X_{n} = \mathbf{x}_{n}`, a mixture
model over a set of distributions :math:`\mathbb{P}_{k},k \in K`, where
:math:`K` is an index set, assumes the data is generated by the
following process,

1) Indexes :math:`Z_{1},\ldots,Z_{n}` are drawn from a distribution
   :math:`\mathbb{Q}` over :math:`K`.

2) :math:`X_{1},X_{2},\ldots,X_{n}` are drawn\ :math:`\ `\ from
   :math:`\mathbb{P}_{Z_{1}},\mathbb{P}_{Z_{2}},\ldots,\mathbb{P}_{Z_{n}}`.

And we want to maximize the following objective function,

.. math:: L\left( \mathbf{\theta} \right) = \sum_{i = 1}^{n}{\log{\mathbb{P(}X_{i}|\mathbf{\theta})}} = \sum_{i = 1}^{n}{\log{\sum_{Z_{i} \in K}^{}{\mathbb{P(}X_{i},Z_{i}|\mathbf{\theta})}}}\  = \sum_{i = 1}^{n}{\log{\sum_{Z_{i} \in K}^{}\frac{\mathbb{Q(}Z_{i}\mathbb{)P}\left( X_{i},Z_{i} \middle| \mathbf{\theta} \right)}{\mathbb{Q(}Z_{i})}}} = \sum_{i = 1}^{n}{\log{\mathbb{E}_{Z_{i}}\left\lbrack \frac{\mathbb{P}\left( X_{i},Z_{i} \middle| \mathbf{\theta} \right)}{\mathbb{Q(}Z_{i})} \right\rbrack}}

where :math:`\mathbb{Q}` is the model of :math:`Z_{i}`\ s and
:math:`\mathbb{E}_{Z_{i}}` is an expectation w.r.t. :math:`Z_{i}`. Now
we need a called Jensen’s inequality (a rigorous proof requires measure
theory), which states,

1) If :math:`f` is a convex function, then
   :math:`f\left( \mathbb{E}X \right)\mathbb{\leq E}\left\lbrack f(X) \right\rbrack`
   for any random variable :math:`X`.

2) If :math:`f` is strictly convex, then
   :math:`f\left( \mathbb{E}X \right)\mathbb{= E}\left\lbrack f(X) \right\rbrack`
   iff :math:`X = \mathbb{E}X` almost surely (in this case :math:`X` is
   almost surely a constant).

+-----------------------------------+
| Figure ‑ The intuition behind EM. |
+-----------------------------------+

We also need to discuss the motive behind EM. EM is actually a method of
lower bounds. The idea is that :math:`L\left( \mathbf{\theta} \right)`
is very often a concave function, and we can approach its maximum by, at
the :math:`i`\ th step, first constructing a concave lower tight bounds
:math:`l_{i}(\mathbf{\theta})` s.t.
:math:`L\left( \mathbf{\theta} \right) \geq l_{i}(\mathbf{\theta})` and
:math:`L\left( \mathbf{\theta}_{i}^{\mathbf{*}} \right) = l_{i}(\mathbf{\theta}_{i}^{*})`
for a given :math:`\mathbf{\theta}_{i}^{*}`, and then climbing up by
finding the maximum of :math:`l_{i}` as
:math:`\mathbf{\theta}_{i + 1}^{*}`. Now we carry on the calculation,
using the fact that logarithm is a concave function,

.. math:: L\left( \mathbf{\theta} \right) = \sum_{i = 1}^{n}{\log{\mathbb{E}_{Z_{i}}\left\lbrack \frac{\mathbb{P}\left( X_{i},Z_{i} \middle| \mathbf{\theta} \right)}{\mathbb{Q(}Z_{i})} \right\rbrack}} \geq \sum_{i = 1}^{n}{\mathbb{E}_{Z_{i}}\left\lbrack \log\frac{\mathbb{P}\left( X_{i},Z_{i} \middle| \mathbf{\theta} \right)}{\mathbb{Q}\left( Z_{i} \right)} \right\rbrack} = \sum_{i = 1}^{n}{\sum_{Z_{i} \in K}^{}{\mathbb{Q}\left( Z_{i} \right)\log\frac{\mathbb{P}\left( X_{i},Z_{i} \middle| \mathbf{\theta} \right)}{\mathbb{Q}\left( Z_{i} \right)}}} = l\mathbb{(Q,}\mathbf{\theta})

We need to further identify when the equality holds. Note the equality
holds when
:math:`\frac{\mathbb{P}\left( X_{i},Z_{i} \middle| \mathbf{\theta} \right)}{\mathbb{Q(}Z_{i})}`
is a constant, and it is positive since both numerator and denominator
are probabilities (or densities), then

.. math:: \mathbb{Q}\left( Z_{i} \right)\mathbb{\propto P}\left( X_{i},Z_{i} \middle| \mathbf{\theta} \right)\mathbb{\Rightarrow Q}\left( Z_{i} \right) = \frac{\mathbb{P}\left( X_{i},Z_{i} \middle| \mathbf{\theta} \right)}{\sum_{Z_{i} \in K}^{}{\mathbb{P}\left( X_{i},Z_{i} \middle| \mathbf{\theta} \right)}} = \frac{\mathbb{P}\left( X_{i},Z_{i} \middle| \mathbf{\theta} \right)}{\mathbb{P}\left( X_{i} \middle| \mathbf{\theta} \right)}\mathbb{= P}\left( Z_{i} \middle| X_{i}\mathbf{,\theta} \right)

Thus
:math:`L\left( \mathbf{\theta} \right) = l\mathbb{(Q,}\mathbf{\theta})`
if
:math:`\mathbb{Q = P}\left( Z_{i} \middle| X_{i}\mathbf{,\theta} \right)`,
and we then maximize :math:`l` w.r.t. :math:`\mathbf{\theta}`. This
gives the general version of EM algorithm,

1) Initialize :math:`\mathbf{\theta}` to some initial guess.

2) E-step: set
   :math:`\mathbb{Q}\left( Z_{i} \right)\mathbb{= P}\left( Z_{i} \middle| X_{i}\mathbf{,\theta} \right)`.

3) M-step: updates
   :math:`\mathbf{\theta} = \arg{\max{\sum_{i = 1}^{n}{\sum_{Z_{i} \in K}^{}{\mathbb{Q}\left( Z_{i} \right)\log\frac{\mathbb{P}\left( X_{i},Z_{i} \middle| \mathbf{\theta} \right)}{\mathbb{Q}\left( Z_{i} \right)}}}}}`.

4) Repeat E-step and M-step until a satisfactory convergence is reached.

Back to our intuition `above <#b3>`__,
:math:`\mathbb{Q,}\mathbf{\theta}` are both arguments of :math:`l`. When
we initialize the EM, we let :math:`\mathbf{\theta}_{0}^{*}` be a proper
guess, and
:math:`\mathbb{Q}_{0}^{*}\mathbb{= P}\left( Z_{i} \middle| X_{i}\mathbf{,}\mathbf{\theta}_{0}^{*} \right)`,
then :math:`L = l`. We then perform maximization to acquire
:math:`\mathbf{\theta}_{1}^{*}`, and let
:math:`\mathbb{Q}_{1}^{*}\mathbb{= P}\left( Z_{i} \middle| X_{i}\mathbf{,}\mathbf{\theta}_{1}^{*} \right)`,
and so on. Note in this process clearly
:math:`l\left( \mathbb{Q}_{i}^{*},\mathbf{\theta}_{i}^{*} \right) \leq l\left( \mathbb{Q}_{i}^{*},\mathbf{\theta}_{i + 1}^{*} \right)`,
and
:math:`l\left( \mathbb{Q}_{i}^{*},\mathbf{\theta}_{i + 1}^{*} \right) \leq l\left( \mathbb{Q}_{i + 1}^{*},\mathbf{\theta}_{i + 1}^{*} \right) = L`.
Thus :math:`l` is monotonically increasing for every E-step and M-step.
This process is also actually a coordinate ascent.

We now derive the EM formulas for Gaussian mixture. `FUTURE
WORK <http://cs229.stanford.edu/notes/cs229-notes8.pdf>`__.

Bayesian Network. Given RVs :math:`X_{1},\ldots,X_{n}`, we know that its joint probability distribution :math:`\mathbb{P(}X)`, which is a probability mass function or a probability density function, can be factorized by the chain rule,
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. math:: \mathbb{P}\left( X_{1},\ldots,X_{n} \right)\mathbb{= P(}X_{n}|X_{1},\ldots,X_{n - 1}\mathbb{)\ldots P(}X_{3}|X_{1},X_{2}\mathbb{)P}\left( X_{2} \middle| X_{1} \right)\mathbb{P(}X_{1})

where notation like :math:`\mathbb{P}(Y|X)` is short for
:math:`\mathbb{P(}Y = y|X = x)`. We draw a graph
:math:`G = (\left\{ 1,\ldots,n \right\},E)` by letting
:math:`\left( i,j \right) \in E` if there is a factor of form
:math:`\mathbb{P(}X_{j}|\ldots,X_{i},\ldots)`. Taking :math:`n = 4` for
example, the resulting graph for the chain rule would look like the one
in *Figure 0‑1* *(a)*. It is obvious that the graph representation of
the chain rule is a complete directed acyclic graph (DAG), whose edge
set is :math:`E = \{\left( i,j \right):i < j\}`.

+-----------------------------------+-----------------------------------+
| |image19|                         | |image20|                         |
+===================================+===================================+
| *(a) chain rule*                  | *(b) with*                        |
|                                   | :math:`X_{2}\bot X_{3}|X_{1}`     |
+-----------------------------------+-----------------------------------+
| *Figure* *0‑1 The DAGs of a 4-RV  |                                   |
| factorization.*                   |                                   |
+-----------------------------------+-----------------------------------+

We can have other factorizations of the joint distribution given
information of conditional independence. For example, if
:math:`X_{2}\bot X_{3}|X_{1}`, then
:math:`\mathbb{P}\left( X_{3} \middle| X_{1},X_{2} \right)\mathbb{= P(}X_{3}|X_{1})`
(see `remark <#b8>`__ below), and :math:`2 \rightarrow 3` edge of
*Figure 0‑1* *(a)* will be removed to form *Figure 0‑1* *(b)*.
Conversely, given a DAG :math:`G = (\left\{ 1,\ldots,n \right\},E)`, we
can construct a factorization by letting

.. math:: \mathbb{P}\left( X_{1},\ldots,X_{n} \right) = \prod_{i = 1}^{n}{\mathbb{P(}X_{i}\mathcal{|p(}X_{i}))}

where :math:`\mathcal{p}\left( X_{i} \right)` denotes the set of all
parents of :math:`X_{i}` in :math:`G` and could be :math:`\varnothing`
for those without parents. Note when
:math:`\mathbb{P(}X_{i}\mathcal{|p(}X_{i}))` is a well-defined
distribution, then :math:`\mathbb{P}\left( X_{1},\ldots,X_{n} \right)`
is well defined. Recall that a DAG has a topological order
:math:`\mathcal{X}_{1},\ldots,\mathcal{X}_{k}`. WLOG, assume
:math:`X_{i}` are indexed according to their topological order, that is,
:math:`\mathcal{X}_{1}` consists of RV indexed by
:math:`1,\ldots,i_{1}`, :math:`\mathcal{X}_{2}` consists of RVs indexed
by :math:`i_{1} + 1,\ldots,i_{2}`, etc.

.. math:: \sum_{x_{1},\ldots,x_{n}}^{}{\mathbb{P}\left( X_{1} = x_{1},\ldots,X_{n} = x_{n} \right)} = \sum_{x_{1},\ldots,x_{n}}^{}{\prod_{i = 1}^{n}{\mathbb{P(}X_{i}\mathcal{|p(}X_{i}))}} = \sum_{x_{1},\ldots,x_{k - 1}}^{}\left( \sum_{x_{i_{k - 1} + 1},\ldots,x_{i_{k}}}^{}{\prod_{i = 1}^{n}{\mathbb{P}\left( X_{i} \middle| \mathcal{p}\left( X_{i} \right) \right)}} \right) = \left( \sum_{x_{i_{k - 1} + 1},\ldots,x_{i_{k}}}^{}{\prod_{i = i_{k - 1} + 1}^{i_{k}}{\mathbb{P}\left( X_{i} \middle| \mathcal{p}\left( X_{i} \right) \right)}} \right)\left( \sum_{x_{1},\ldots,x_{k - 1}}^{}{\prod_{i = 1}^{i_{k - 1}}{\mathbb{P}\left( X_{i} \middle| \mathcal{p}\left( X_{i} \right) \right)}} \right)

Here the “summation” is misused to represent integration if
:math:`\mathbb{P}` is a PDF. The above last identity is because only
factors in
:math:`\prod_{i = i_{k - 1} + 1}^{i_{k}}{\mathbb{P}\left( X_{i} \middle| \mathcal{p}\left( X_{i} \right) \right)}`
are dependent on :math:`x_{i_{k - 1} + 1},\ldots,x_{i_{k}}`. Then check
that

.. math:: \sum_{x_{i_{k - 1} + 1},\ldots,x_{i_{k}}}^{}{\prod_{i = i_{k - 1} + 1}^{i_{k}}{\mathbb{P}\left( X_{i} \middle| \mathcal{p}\left( X_{i} \right) \right)}} = \prod_{i = i_{k - 1} + 1}^{i_{k}}{\sum_{x_{i}}^{}{\mathbb{P}\left( X_{i} \middle| \mathcal{p}\left( X_{i} \right) \right)}} = 1

since each factor in
:math:`\prod_{i = i_{k - 1} + 1}^{i_{k}}{\mathbb{P}\left( X_{i} \middle| \mathcal{p}\left( X_{i} \right) \right)}`
is only dependent on :math:`x_{i}`, and we arrive at

.. math:: \sum_{x_{1},\ldots,x_{n}}^{}{\mathbb{P}\left( X_{1} = x_{1},\ldots,X_{n} = x_{n} \right)} = \sum_{x_{1},\ldots,x_{k - 1}}^{}{\prod_{i = 1}^{i_{k - 1}}{\mathbb{P}\left( X_{i} \middle| \mathcal{p}\left( X_{i} \right) \right)}}

Now only factors in
:math:`\prod_{i = i_{k - 2} + 1}^{i_{k - 1}}{\mathbb{P}\left( X_{i} \middle| \mathcal{p}\left( X_{i} \right) \right)}`
are dependent on :math:`x_{i_{k - 2} + 1},\ldots,x_{i_{k - 1}}`. Repeat
the same trick above until we have
:math:`\sum_{x_{1},\ldots,x_{n}}^{}{\mathbb{P}\left( X_{1} = x_{1},\ldots,X_{n} = x_{n} \right)} = 1`,
which validates :math:`\mathbb{P}\left( X_{1},\ldots,X_{n} \right)` is
indeed a probability distribution. We call the above DAG representation
of a factorization of joint distribution as a **Directed Graph Model**,
or more commonly the **Bayesian Network**, and we say joint distribution
:math:`\mathbb{P}` **respects** :math:`G` if :math:`\mathbb{P}` can be
represented by :math:`G`. Pay attention that a :math:`\mathbb{P}` might
respect more than one DAGs. Every :math:`\mathbb{P}` at least respects
the complete DAG as a result of the chain rule, and conditional
independence makes :math:`\mathbb{P}` respect more DAGs; one example is
the above *Figure 0‑1*.

*Theorem* *0‑1* Before further discussion, we want to discuss a basic
theorem regarding conditional independence. We claim two RVs :math:`X,Y`
are conditionally independent given :math:`Z`, i.e.

.. math:: \mathbb{P}\left( X = x,Y = y \middle| Z = z \right)\mathbb{= P}\left( X = x \middle| Z = z \right)\mathbb{P(}Y = y|Z = z)

for any :math:`x,y,z`, or in short
:math:`\mathbb{P}\left( X,Y \middle| Z \right)\mathbb{= P}\left( X \middle| Z \right)\mathbb{P(}Y|Z)`,
iff one of the following conditions holds. Note for
:math:`\{ z\mathbb{:P}\left( Z = z \right) = 0\}` the equation
:math:`\mathbb{P}\left( X,Y \middle| Z \right)\mathbb{= P}\left( X \middle| Z \right)\mathbb{P(}Y|Z)`
automatically holds, so in the proof we only consider
:math:`\mathbb{P}\left( Z \right) > 0`.

1) :math:`\mathbb{P}\left( X \middle| Y,Z \right)\mathbb{= P(}X|Z)` on
   :math:`\{\left( y,z \right)\mathbb{:P}\left( Y = y,Z = z \right) > 0\}`.
   Note it is possible that
   :math:`\mathbb{P}\left( X \middle| Y,Z \right) = 0` but
   :math:`\mathbb{P}\left( X \middle| Z \right) \neq 0` on
   :math:`\{\left( y,z \right)\mathbb{:P}\left( Y = y,Z = z \right) = 0\}`.

   *Necessity*. When :math:`\mathbb{P}\left( Y,Z \right) > 0`, we have

.. math:: \mathbb{P}\left( X \middle| Y,Z \right) = \frac{\mathbb{P}\left( X,Y|Z \right)\mathbb{P(}Z)}{\mathbb{P(}Y,Z)} = \frac{\mathbb{P}\left( X \middle| Z \right)\mathbb{P}\left( Y \middle| Z \right)\mathbb{P(}Z)}{\mathbb{P(}Y,Z)} = \frac{\mathbb{P}\left( X \middle| Z \right)\mathbb{P(}Y,Z)}{\mathbb{P(}Y,Z)}\mathbb{= P(}X|Z)

*Sufficiency*. When :math:`\mathbb{P}\left( Z \right) = 0`, independence
trivially holds by
:math:`\mathbb{P}\left( X,Y \middle| Z \right)\mathbb{= P}\left( X \middle| Z \right)\mathbb{P}\left( Y \middle| Z \right) = 0`.
When :math:`\mathbb{P}\left( X \middle| Y,Z \right)\mathbb{= P(}X|Z)`
given :math:`\mathbb{P}\left( Y,Z \right) > 0`, we have

.. math:: \mathbb{P}\left( X,Y \middle| Z \right)\mathbb{P}\left( Z \right)\mathbb{= P}\left( X \middle| Y,Z \right)\mathbb{P}\left( Y,Z \right)\mathbb{= P}\left( X \middle| Z \right)\mathbb{P}\left( Y,Z \right)\mathbb{\Rightarrow P}\left( X,Y \middle| Z \right)\mathbb{= P}\left( X \middle| Z \right)\frac{\mathbb{P}\left( Y,Z \right)}{\mathbb{P}\left( Z \right)}\mathbb{= P}\left( X \middle| Z \right)\mathbb{P(}Y|Z)

Independence automatically holds for
:math:`\{\left( y,z \right)\mathbb{:P}\left( Y = y,Z = z \right) = 0\}`,
in which case
:math:`\mathbb{P}\left( Y \middle| Z \right)\mathbb{P}\left( Z \right) = 0`,
then :math:`\mathbb{P}\left( Y \middle| Z \right) = 0`, implying
:math:`\mathbb{P}\left( X,Y \middle| Z \right)\mathbb{= P}\left( X \middle| Z \right)\mathbb{P}\left( Y \middle| Z \right) = 0`.

2) :math:`\mathbb{P}\left( X \middle| Y,Z \right) = f(X,Z)` for some
   function :math:`f` dependent only on :math:`X,Z` (or not dependent on
   :math:`Y`).

   *Necessity*. When :math:`X\bot Y`, then the following holds,

.. math:: \mathbb{P}\left( X,Y,Z \right)\mathbb{= P}\left( X,Y \middle| Z \right)\mathbb{P}\left( Z \right)\mathbb{= P}\left( X \middle| Z \right)\mathbb{P}\left( Y \middle| Z \right)\mathbb{P}\left( Z \right)\mathbb{\Rightarrow P}\left( X,Z \right)\mathbb{P}\left( Y,Z \right)\mathbb{= P}\left( X,Y,Z \right)\mathbb{P}\left( Z \right)\mathbb{= P}\left( X|Y,Z \right)\mathbb{P}\left( Y,Z \right)\mathbb{P}\left( Z \right)\mathbb{\Rightarrow P}\left( X,Y,Z \right)\mathbb{= P}\left( X|Y,Z \right)\mathbb{P}\left( Y,Z \right)

For :math:`\{(y,z\mathbb{):P}\left( Y = y,Z = z \right) > 0\}`, we have
:math:`\mathbb{P}\left( X|Y,Z \right) = \frac{\mathbb{P}\left( X,Z \right)}{\mathbb{P(}Z)\ }`,
which is a function dependent only on :math:`X,Z`. For
:math:`\{(y,z\mathbb{):P}\left( Y = y,Z = z \right) = 0\}`, we have
:math:`\mathbb{P}\left( X \middle| Y = y,Z = z \right) = 0`, which is a
constant not dependent on :math:`Y`.

*Sufficiency*. By 1), we can instead show
:math:`\mathbb{P}\left( X \middle| Y,Z \right) = f\left( X,Z \right)`
implies
:math:`\mathbb{P}\left( X \middle| Z \right) = f\left( X,Z \right)\mathbb{= P}\left( X \middle| Y,Z \right)`
for any :math:`f` on
:math:`\{(y,z\mathbb{):P}\left( Y = y,Z = z \right) > 0\}`.

.. math:: \mathbb{P}\left( X,Y,Z \right) = \mathbb{P}\left( X \middle| Y,Z \right)\mathbb{P}\left( Y,Z \right)\mathbb{= P}\left( Y,Z \right)f\left( X,Z \right)\mathbb{\Rightarrow P}\left( X,Z \right) = \sum_{y}^{}{\mathbb{P}\left( Y = y,Z \right)f\left( X,Z \right)} = f\left( X,Z \right)\sum_{y}^{}{\mathbb{P}\left( Y = y,Z \right)} = f\left( X,Z \right)\mathbb{P}\left( Z \right) \Rightarrow f\left( X,Z \right) = \frac{\mathbb{P}\left( X,Z \right)}{\mathbb{P}\left( Z \right)}\mathbb{= P(}X|Z)

3) :math:`\mathbb{P}\left( X,Y \middle| Z \right) = f\left( X,Z \right)g\left( Y,Z \right)`
   for some function :math:`f,g` dependent on :math:`X,Z` and
   :math:`Y,Z` respectively.

   *Necessity* is trivial. When :math:`X\bot Y`,
   :math:`\mathbb{P}\left( X,Y \middle| Z \right)\mathbb{= P}\left( X \middle| Z \right)\mathbb{P(}Y|Z)`,
   which are functions dependent on :math:`X,Z` and :math:`Y,Z`
   respectively.

   *Sufficiency.* We have

.. math:: \mathbb{P}\left( X,Y \middle| Z \right) = f\left( X,Z \right)g\left( Y,Z \right)\mathbb{\Rightarrow P}\left( X,Y,Z \right)\mathbb{= P}\left( X,Y \middle| Z \right)\mathbb{P(}Z) = f\left( X,Z \right)g\left( Y,Z \right)\mathbb{P(}Z)

Then check the following,

.. math:: \mathbb{P}\left( X,Z \right) = \sum_{y}^{}{\mathbb{P}\left( X,Y = y,Z \right)} = \sum_{y}^{}{f\left( X,Z \right)g\left( Y = y,Z \right)\mathbb{P(}Z)} = f\left( X,Z \right)\mathbb{P}\left( Z \right)\left( \sum_{y}^{}{g\left( Y = y,Z \right)} \right)\mathbb{\Rightarrow P}\left( Z \right)\mathbb{= P}\left( Z \right)\left( \sum_{x}^{}{f\left( X = x,Z \right)} \right)\left( \sum_{y}^{}{g\left( Y = y,Z \right)} \right) \Rightarrow \left( \sum_{x}^{}{f\left( X = x,Z \right)} \right)\left( \sum_{y}^{}{g\left( Y = y,Z \right)} \right) = 1

The above implies independence
:math:`\mathbb{P}\left( X \middle| Z \right)\mathbb{P}\left( Y \middle| Z \right)\mathbb{= P}\left( X,Y \middle| Z \right)`,

.. math:: \mathbb{P}\left( X \middle| Z \right)\mathbb{P}\left( Y \middle| Z \right) = \frac{\mathbb{P}\left( X,Z \right)\mathbb{P}\left( Y,Z \right)}{\mathbb{P}^{2}\left( Z \right)} = \frac{\sum_{y}^{}{\mathbb{P}\left( X,Y = y,Z \right)}\sum_{x}^{}{\mathbb{P}\left( X = x,Y,Z \right)}}{\mathbb{P}^{2}\left( Z \right)} = \frac{\mathbb{P}^{2}\left( Z \right)\left( \sum_{x}^{}{f\left( X = x,Z \right)g\left( Y,Z \right)} \right)\left( \sum_{y}^{}{f\left( X,Z \right)g\left( Y = y,Z \right)} \right)}{\mathbb{P}^{2}\left( Z \right)} = f\left( X,Z \right)g\left( Y,Z \right)\left( \sum_{x}^{}{f\left( X = x,Z \right)} \right)\left( \sum_{y}^{}{g\left( Y = y,Z \right)} \right) = f\left( X,Z \right)g\left( Y,Z \right)\mathbb{= P}\left( X,Y \middle| Z \right)

4) :math:`\mathbb{P(}X,Y,Z) = f(X,Z)g(Y,Z)` for some function
   :math:`f,g` dependent on :math:`X,Z` and :math:`Y,Z` respectivelys.

..

   *Necessity* is trivial. When :math:`X\bot Y`,
   :math:`\mathbb{P}(X,Y,Z\mathbb{) = P}\left( X \middle| Z \right)\mathbb{P}\left( Y \middle| Z \right)\mathbb{P(}Z)`,
   where we can let
   :math:`f\left( X,Z \right)\mathbb{= P}\left( X \middle| Z \right),g\left( Y,Z \right)\mathbb{= P}\left( Y \middle| Z \right)\mathbb{P}\left( Z \right)`.

   *Sufficiency.* We have

.. math:: \mathbb{P}\left( X,Y,Z \right) = f\left( X,Z \right)g\left( Y,Z \right)\mathbb{\Rightarrow P}\left( X,Y \middle| Z \right) = \frac{\mathbb{P}\left( X,Y,Z \right)}{\mathbb{P(}Z)} = \frac{f\left( X,Z \right)}{\mathbb{P(}Z)} \times g\left( Y,Z \right)

By 3) it is clear that the conditional independence holds.

Define the **ancestral set** of :math:`\mathcal{X}` in a DAG graph as
:math:`\mathcal{X}` union all its ancestors, denoted as
:math:`\mathcal{a(X)}`. For an example shown below, in the following
graph,
:math:`\mathcal{a}\left( \left\{ G,I \right\} \right) = \{ J,L,G,I,K,M\}`.
Say :math:`\mathcal{X}` is self-ancestral if
:math:`\mathcal{a}\left( \mathcal{X} \right)\mathcal{= X}`. Define a
node with no outbound edges as a **leaf**.

   |image21|

   *Figure* *0‑2 Illustration of a Bayesian Network and ancestral set*
   :math:`\mathcal{a(\{}G,I\})`\ *.*

-  *Lemma* *0‑1* A set :math:`\mathcal{X}` is self-ancestral iff
   :math:`\mathcal{p}\left( W \right)\mathcal{\subseteq X}` for any
   :math:`W \subseteq X`. *Necessity* is by definition of self-ancestral
   set. *Sufficiency*. If :math:`W` is an ancestor of :math:`W`, then
   there must be a
   :math:`X \rightarrow W_{k} \rightarrow \ldots \rightarrow W_{1} \rightarrow W`
   path. Inductively
   :math:`W_{1}\mathcal{\in p}\left( W \right)\mathcal{\subseteq X,\ldots,}W_{k}\mathcal{\in p}\left( W_{k - 1} \right)\mathcal{\subseteq X,}X \in \mathcal{p}\left( W_{k} \right)\mathcal{\subseteq X}`.

-  *Lemma* *0‑2* Let :math:`Y` be a leaf node. Let
   :math:`\mathcal{X = N\backslash\{}Y\}` and :math:`\mathcal{X}` be all
   RVs in :math:`\mathcal{X}`, then
   :math:`\mathbb{P}_{\mathcal{N}}\mathcal{(X)}\mathbf{=}\mathbb{P}_{\mathcal{X}}\mathcal{(X)}`,
   where :math:`\mathbb{P}_{\mathcal{N}}` is used to denote a
   distribution w.r.t. the Bayesian network :math:`\mathcal{N}`, and we
   overload :math:`\mathcal{X}` to also represent a sub network induced
   by node set :math:`\mathcal{X}`. Check that

.. math:: \mathbb{P}_{\mathcal{N}}\mathcal{(X)}\mathbf{=}\sum_{y}^{}{\mathbb{P}_{\mathcal{N}}\mathcal{(X,}Y = y)}\mathbf{=}\sum_{y}^{}{\prod_{X\mathbf{\in}\mathcal{X}}^{}{\mathbb{P}_{\mathcal{N}}\left( X \middle| \mathcal{p}\left( X \right) \right)\mathbb{P}_{\mathcal{N}}(Y = y|\mathcal{p(}Y))}}\mathbf{=}\prod_{X\mathbf{\in}\mathcal{X}}^{}{\mathbb{P}_{\mathcal{N}}\left( X \middle| \mathcal{p}\left( X \right) \right)}\sum_{y}^{}{\mathbb{P}_{\mathcal{N}}(Y = y|\mathcal{p(}Y))}\mathbf{=}\prod_{X\mathbf{\in}\mathcal{X}}^{}{\mathbb{P}_{\mathcal{N}}\left( X \middle| \mathcal{p}\left( X \right) \right)}\mathbf{=}\mathbb{P}_{\mathcal{X}}(X)

-  *Lemma* *0‑3* Suppose :math:`\mathcal{X \subseteq N}` is
   self-ancestral, then the marginal probability

.. math:: \mathbb{P}_{\mathcal{N}}\left( \mathcal{X} \right) = \prod_{W \in \mathcal{X}}^{}{\mathbb{P}\left( W \middle| \mathcal{p}\left( W \right) \right)} = \mathbb{P}_{\mathcal{X}}\mathcal{(X)}

We use two methods to prove this lemma. First by *Lemma 0‑1*, Check that

.. math:: \mathbb{P}_{\mathcal{N}}\left( \mathcal{X} \right) = \sum_{W \in \mathcal{N\backslash X}}^{}{\mathbb{P}\left( \mathcal{N} \right)} = \sum_{W \in \mathcal{N\backslash X}}^{}{\prod_{W \in \mathcal{N}}^{}{\mathbb{P(}W|\mathcal{p(}W))}} = \sum_{W \in \mathcal{N\backslash X}}^{}{\left( \prod_{W \in \mathcal{X}}^{}{\mathbb{P}\left( W \middle| \mathcal{p}\left( W \right) \right)} \right)\left( \prod_{W \in \mathcal{N\backslash X}}^{}{\mathbb{P}\left( W \middle| \mathcal{p}\left( W \right) \right)} \right)} = \left( \prod_{W \in \mathcal{X}}^{}{\mathbb{P}\left( W \middle| \mathcal{p}\left( W \right) \right)} \right)\sum_{W \in \mathcal{N\backslash X}}^{}\left( \prod_{W \in \mathcal{N\backslash X}}^{}{\mathbb{P}\left( W \middle| \mathcal{p}\left( W \right) \right)} \right) = \prod_{W \in \mathcal{X}}^{}{\mathbb{P}\left( W \middle| \mathcal{p}\left( W \right) \right)}

where
:math:`\sum_{W \in \mathcal{N\backslash X}}^{}\left( \prod_{W \in \mathcal{N\backslash X}}^{}{\mathbb{P}\left( W \middle| \mathcal{p}\left( W \right) \right)} \right) = \prod_{W \in \mathcal{N\backslash X}}^{}{\sum_{W \in \mathcal{N\backslash X}}^{}{\mathbb{P}\left( W \middle| \mathcal{p}\left( W \right) \right)}} = 1`
for the same reason as `above <#b10>`__. It is important that
:math:`\mathcal{X}` has to be self-ancestral, otherwise there is no way
to guarantee that a RV in :math:`\mathcal{N\backslash X}` is not a
parent of RV in :math:`\mathcal{X}`, and then there would be

.. math:: \sum_{W \in \mathcal{N\backslash X}}^{}{\left( \prod_{W \in \mathcal{X}}^{}{\mathbb{P}\left( W \middle| \mathcal{p}\left( W \right) \right)} \right)\left( \prod_{W \in \mathcal{N\backslash X}}^{}{\mathbb{P}\left( W \middle| \mathcal{p}\left( W \right) \right)} \right)} \neq \left( \prod_{W \in \mathcal{X}}^{}{\mathbb{P}\left( W \middle| \mathcal{p}\left( W \right) \right)} \right)\sum_{W \in \mathcal{N\backslash X}}^{}\left( \prod_{W \in \mathcal{N\backslash X}}^{}{\mathbb{P}\left( W \middle| \mathcal{p}\left( W \right) \right)} \right)

It can also be proved by *Lemma 0‑2*. When
:math:`\mathcal{N\backslash X}` is not empty, we can always find a leaf
in it (because of the finite many nodes). Removing this leave does not
change the probability by *Lemma 0‑2*. Repeat removal of leaf until
:math:`\mathcal{N\backslash X}` is empty.

-  In a Bayesian network, given a set of RV :math:`\mathcal{Z}`, two
   other RVs :math:`X,Y` are said to be directional-separated, or
   **d-separated** w.r.t. :math:`\mathcal{Z}`, or **d-blocked** w.r.t.
   :math:`\mathcal{Z}` iff one of the following is true for every
   undirected :math:`X\sim Y` simple path. An undirected simple path
   does not have cycles and disregards the edge directions. For example,
   in *Figure 0‑2* :math:`F \rightarrow C \leftarrow E` is considered as
   an undirected path. For another example,
   :math:`K \rightarrow H \rightarrow D \leftarrow B \leftarrow E` is an
   undirected path. However,
   :math:`L \rightarrow J \rightarrow G \leftarrow J` is not an
   undirected simple path because it has cycle. We denote an undirected
   simple path from node :math:`X` to :math:`Y` as an :math:`X\sim Y`
   path.

+-----+-----+
| (1) |     |
+-----+-----+
|     |     |
+-----+-----+
| (2) | (3) |
+-----+-----+

1) The :math:`X\sim Y` path is either a
   :math:`X\sim A \rightarrow \mathcal{Z \rightarrow}B\sim Y` path or a
   :math:`X\sim A \leftarrow \mathcal{Z \leftarrow}B\sim Y` path, where
   it is possible that :math:`X = A,Y = B`; we also say there is a
   serial connection at :math:`W \notin \mathcal{Z}`;

2) The :math:`X\sim Y` path is a
   :math:`X\sim A \leftarrow \mathcal{Z \rightarrow}B\sim Y` path, where
   it is possible that :math:`X = A,Y = B`; we also say there exists a
   diverging connection at :math:`\mathcal{Z}`;

3) The :math:`X\sim Y` path has a fourth RV :math:`W \notin \mathcal{Z}`
   in the path s.t. :math:`W` and its descendants (those nodes reachable
   from :math:`W` by directed paths) are not in :math:`\mathcal{Z}`, and
   the path has the form
   :math:`X\sim A \rightarrow \mathcal{Z \leftarrow}B\sim Y`; we also
   say there exists a converging connection at :math:`W`. For this
   condition, it is possible that :math:`\mathcal{Z = \varnothing}`.
   This condition is the reason that we say “\ :math:`X,Y` d-separated
   w.r.t. :math:`\mathcal{Z}`\ ” rather than “\ :math:`X,Y` d-separated
   by :math:`\mathcal{Z}`\ ”, because :math:`\mathcal{Z}` might not be
   in the path. For the previous two conditions, we might say “blocked
   by”.

..

   We say two sets of RV :math:`\mathcal{X,Y}` are d-separated by a
   third set :math:`\mathcal{Z}` if :math:`X,Y` are d-separated by
   :math:`\mathcal{Z}` for every
   :math:`\left( X,Y \right)\mathcal{\in X \times Y}`. Taking *Figure
   0‑2* for example,

+-----------------+-----------------+-----------------+-----------------+
| .. math:: \math | .. math:: \math | .. math:: \math | d-separated?    |
| cal{X}          | cal{Y}          | cal{Z}          |                 |
+=================+=================+=================+=================+
| .. math:: C     | .. math:: C     | Any             | No. A RV is     |
|                 |                 |                 | always not      |
|                 |                 |                 | d-separated     |
|                 |                 |                 | from itself,    |
|                 |                 |                 | regardless of   |
|                 |                 |                 | :math:`\mathcal |
|                 |                 |                 | {Z}`.           |
+-----------------+-----------------+-----------------+-----------------+
| .. math:: C     | .. math:: E     | Any             | No. Adjacent    |
|                 |                 |                 | RVs are always  |
|                 |                 |                 | not             |
|                 |                 |                 | d-separated,    |
|                 |                 |                 | regardless of   |
|                 |                 |                 | :math:`\mathcal |
|                 |                 |                 | {Z}`.           |
+-----------------+-----------------+-----------------+-----------------+
| .. math:: L     | .. math:: E     | .. math:: \{ F, | No. Converging  |
|                 |                 | C\}             | connection at   |
|                 |                 |                 | :math:`\{ F,C\} |
|                 |                 |                 | `               |
|                 |                 |                 | in the only     |
|                 |                 |                 | path.           |
+-----------------+-----------------+-----------------+-----------------+
| .. math:: I     | .. math:: D     | .. math:: \{ M, | No.             |
|                 |                 | K\}             | :math:`I \right |
|                 |                 |                 | arrow E \righta |
|                 |                 |                 | rrow B \rightar |
|                 |                 |                 | row E`          |
|                 |                 |                 | path is not     |
|                 |                 |                 | blocked by      |
|                 |                 |                 | :math:`\{ M,K\} |
|                 |                 |                 | `               |
+-----------------+-----------------+-----------------+-----------------+
| .. math:: F     | .. math:: E     | .. math:: B     | Yes. Only path  |
|                 |                 |                 | has converging  |
|                 |                 |                 | connection at a |
|                 |                 |                 | fourth node     |
|                 |                 |                 | :math:`C`.      |
+-----------------+-----------------+-----------------+-----------------+
| .. math:: \{ L, | .. math:: \{ A, | .. math:: E     | Yes. All        |
| G\}             | M\}             |                 | :math:`\{ A,M\} |
|                 |                 |                 | \sim I \rightar |
|                 |                 |                 | row E \rightarr |
|                 |                 |                 | ow C\sim\{ L,G\ |
|                 |                 |                 | }`              |
|                 |                 |                 | paths are       |
|                 |                 |                 | blocked by a    |
|                 |                 |                 | serial          |
|                 |                 |                 | connection at   |
|                 |                 |                 | :math:`E`; all  |
|                 |                 |                 | :math:`\{ A,M\} |
|                 |                 |                 | \sim B \leftarr |
|                 |                 |                 | ow E \rightarro |
|                 |                 |                 | w C\sim\{ L,B\} |
|                 |                 |                 | `               |
|                 |                 |                 | paths blocked   |
|                 |                 |                 | by a diverging  |
|                 |                 |                 | connection at   |
|                 |                 |                 | :math:`E`.      |
+-----------------+-----------------+-----------------+-----------------+
| .. math:: I     | .. math:: \{ F, | .. math:: \{ K, | Yes. The only   |
|                 | A\}             | B\}             | :math:`I\sim F` |
|                 |                 |                 | path has a      |
|                 |                 |                 | converging      |
|                 |                 |                 | connection at   |
|                 |                 |                 | :math:`C` and   |
|                 |                 |                 | :math:`C \notin |
|                 |                 |                 |  \{ K,B\}`;     |
|                 |                 |                 | the             |
|                 |                 |                 | :math:`I\sim B\ |
|                 |                 |                 | sim A`          |
|                 |                 |                 | path is serial  |
|                 |                 |                 | at :math:`B`;   |
|                 |                 |                 | the             |
|                 |                 |                 | :math:`I\sim K\ |
|                 |                 |                 | sim A`          |
|                 |                 |                 | path has a      |
|                 |                 |                 | diverging       |
|                 |                 |                 | connection at   |
|                 |                 |                 | :math:`K`.      |
+-----------------+-----------------+-----------------+-----------------+

*Lemma* *0‑4* Let :math:`\mathcal{X,Y,Z}` be a partition of
:math:`\mathcal{N}`, if :math:`\mathcal{Z}` d-separates
:math:`\mathcal{X}` and :math:`\mathcal{Y}`, then
:math:`\mathcal{X\bot Y}`. Let
:math:`\mathcal{Z}_{1} = \{ z \in \mathcal{Z,p}\left( z \right)\mathcal{\subseteq X\}}`
and :math:`\mathcal{Z}_{2}\mathcal{= Z\backslash}\mathcal{Z}_{1}`. Now

.. math:: \forall X \in \mathcal{X\bigcup}\mathcal{Z}_{1}\mathcal{,p}\left( X \right)\mathcal{\subseteq X\bigcup}\mathcal{Z}_{1}

.. math:: \forall Y \in \mathcal{Y\bigcup}\mathcal{Z}_{2}\mathcal{,p}\left( Y \right)\mathcal{\subseteq Y\bigcup}\mathcal{Z}_{2}

Consider

.. math:: \mathbb{P}\left( \mathcal{X,Y,Z} \right) = \prod_{W \in \mathcal{N}}^{}{\mathbb{P(}W|\mathcal{p(}W))} = \prod_{W \in \mathcal{X\bigcup}\mathcal{Z}_{1}}^{}{\mathbb{P(}W|\mathcal{p(}W))}\prod_{W \in \mathcal{Y\bigcup}\mathcal{Z}_{2}}^{}{\mathbb{P(}W|\mathcal{p(}W))}

where
:math:`\prod_{W \in \mathcal{X\bigcup}\mathcal{Z}_{1}}^{}{\mathbb{P(}W|\mathcal{p(}W))}`
is a function dependent on :math:`\mathcal{X,Z}`, and
:math:`\prod_{W \in \mathcal{Y\bigcup}\mathcal{Z}_{2}}^{}{\mathbb{P(}W|\mathcal{p(}W))}`
is a function dependent on :math:`\mathcal{Y,Z}`, thus
:math:`\mathcal{X\bot Y}` (see `remark <#b8>`__ below for proof).

   *Theorem* *0‑2* D-separation is the tool to recognize independence in
   a Bayesian network. We now come to the most important theorem for
   Bayesian network, the **D-Separation Theorem**, or the **Global
   Markov Property for Bayesian Network**. Given two sets of RVs
   :math:`\mathcal{X,Y}` s.t. any two of :math:`\mathcal{X,Y,Z}` have no
   intersection, if every pair of
   :math:`X \in \mathcal{X,}Y \in \mathcal{Y}` are d-separated by
   :math:`\mathcal{Z}`, then :math:`\mathcal{X,Y}` are independent
   conditioned on :math:`\mathcal{Z}`. For example, based on above
   table, we already see
   :math:`F\bot E\left| B,\left\{ L,G \right\}\bot\left\{ A,M \right\} \right|E,I\bot\{ F,A\}|\{ K,B\}`.
   Note this theorem does not imply anything on :math:`\mathcal{X}` and
   :math:`\mathcal{Y}` that are not d-separated. :math:`\mathcal{X}` and
   :math:`\mathcal{Y}` could be conditional independent or
   non-conditional independent when they are not d-separated.

Let :math:`\mathcal{X}^{'}` be the set of all nodes that are not
d-separated from :math:`\mathcal{X}` by :math:`\mathcal{Z}`, and note
:math:`\mathcal{X \subseteq}\mathcal{X}^{'}`. Let
:math:`\mathcal{Y' = N\backslash}(\mathcal{X}^{'}\mathcal{\bigcup Z) \Rightarrow Y \subseteq Y'}`,
then by previous lemma we have
:math:`\mathcal{X}^{'}\bot\mathcal{Y}^{'}\mathcal{|Z}`. By *Theorem 0‑1*
there must be functions
:math:`f\left( \mathcal{X}^{'}\mathcal{,Z} \right),g(\mathcal{Z}^{'}\mathcal{,Y)}`
s.t.

.. math:: \mathbb{P}\left( \mathcal{X}^{'},\mathcal{Y}^{'}\mathcal{,Z} \right) = f\left( \mathcal{X}^{'}\mathcal{,Z} \right)g(\mathcal{Z,}\mathcal{Y}^{'})

Let
:math:`\mathcal{X}^{''} = \mathcal{X}^{'}\mathcal{\backslash X,}\mathcal{Y}^{''}\mathcal{= Y'\backslash Y}`,
we have

.. math:: \mathbb{P}\left( \mathcal{X',Y',Z} \right) = f\left( \mathcal{X}^{''}\mathcal{,X,Z} \right)g\left( \mathcal{Z,}\mathcal{Y}^{''}\mathcal{,Y} \right)

Then

.. math:: \mathbb{P}\left( \mathcal{X,Y,Z} \right) = \sum_{\mathcal{X}^{''},\mathcal{Y}^{''}}^{}{\mathbb{P}\left( \mathcal{X}^{'},\mathcal{Y}^{'}\mathcal{,Z} \right)} = \sum_{\mathcal{X}^{''},\mathcal{Y}^{''}}^{}{f\left( \mathcal{X}^{''}\mathcal{,X,Z} \right)g\left( \mathcal{Z,}\mathcal{Y}^{''}\mathcal{,Y} \right)} = \left( \sum_{\mathcal{X}^{''}}^{}{f\left( \mathcal{X}^{''}\mathcal{,X,Z} \right)} \right)\left( \sum_{\mathcal{Y}^{''}}^{}{g\left( \mathcal{Z,}\mathcal{Y}^{''}\mathcal{,Y} \right)} \right)

We see
:math:`\sum_{\mathcal{X}^{''}}^{}{f\left( \mathcal{X}^{''}\mathcal{,X,Z} \right)}`
is a function not dependent on
:math:`\mathcal{Y}`,\ :math:`\sum_{\mathcal{Y}^{''}}^{}{g\left( \mathcal{Z,}\mathcal{Y}^{''}\mathcal{,Y} \right)}`
is function not dependent on :math:`\mathcal{X}`, again by *Theorem 0‑1*
we have :math:`\mathcal{X\bot Y|Z}`.

However, please note the converse might not be true. Conditionally
independent RVs might not be d-separated, due to that :math:`\mathbb{P}`
might respect multiple graphs. A simple example is *Figure 0‑1* *(a)*.
Any :math:`\mathbb{P}` of four random variables respects this graph,
representation of chain rule, even when :math:`X_{2}` and :math:`X_{3}`
are independent given :math:`X_{1}`, however, :math:`X_{3}` and
:math:`X_{3}` are clearly not d-separated in *Figure 0‑1* *(a)* w.r.t.
:math:`X_{1}`.

-  In a Bayesian network :math:`\mathcal{N}`, the **Markov blanket** of
   a node :math:`X`, denoted by :math:`\mathcal{b(}X)`, is the set of
   RVs consisting of the parents of :math:`X`, the children of
   :math:`X`, and the parents of children of :math:`X`. The following
   gives an example, the
   :math:`\mathcal{b}\left( I \right) = \left\{ E \right\}\bigcup\{ L,K\}\bigcup\{ J,H\}`,
   where :math:`\left\{ E \right\}` is the parent, :math:`\{ L,K\}` are
   the children, and :math:`\{ J,H\}` are the parents of children. For a
   set of RVs :math:`\mathcal{X}`, define its Markov blanket as
   :math:`\mathcal{b}\left( \mathcal{X} \right) = \bigcup_{X \in \mathcal{X}}^{}{\mathcal{b(}X)}`.

   |image22|

   *Figure* *0‑3 Illustration of a Markov blanket*

   *Lemma* *0‑5* :math:`\mathcal{b(X)}` d-separates :math:`\mathcal{X}`
   from all other nodes. In order to reach a RV in :math:`\mathcal{X}`,
   a path can, as illustrated by *Figure 0‑4*,

1) go through a parent :math:`P` of :math:`\mathcal{X}`, which is either
   serially or divergently blocked by :math:`P`;

2) go through a child :math:`C` of :math:`\mathcal{X}`,

   a. go through an outbound edge of :math:`C`; such path is blocked by
      the serial connection at :math:`C`.

   b. first go through a parent :math:`Q` of :math:`C`, then go through
      :math:`C`; such path is either serially blocked by :math:`Q`, or
      divergently blocked by :math:`Q`.

      *Figure* *0‑4 Markov blanket* :math:`\mathcal{b(X)}` *d-separates*
      :math:`\mathcal{X}` *from other nodes*

      *Theorem* *0‑3* A set of RVs :math:`\mathcal{X}` is conditionally
      independent of all other RVs given its Markov blanket. This
      immediately follows from above lemma by d-separation theorem.

-  *Theorem* *0‑4* L\ **ocal Markov Property for Bayesian Network**: a
   RV :math:`X` is independent from all non-descendants given its
   parents. The proof is similar to *Lemma 0‑5*. In order to reach
   :math:`X`, a path can, as illustrated by *Figure 0‑5* *(a)*,

1) go through a parent :math:`P` of :math:`X`, which is either serially
   or divergently blocked by :math:`P`;

2) go through a node :math:`S` that has an outbound edge to some
   descendant, say :math:`D`, of :math:`X`, which has a convergent
   connection at :math:`D` (:math:`D` cannot have any descendants in
   :math:`\mathcal{p(}X)`, otherwise there would be a cycle in the
   graph).

+-----+-----+
| (a) | (b) |
+-----+-----+

*Figure* *0‑5* :math:`\mathcal{p(}X)` *d-separates* :math:`X` *from
non-descendants*

Note this theorem does not hold for a set :math:`\mathcal{X}`, because
we cannot guarantee there is a cycle when :math:`D` has a descendant in
:math:`\mathcal{p(X)}`. As illustrated by *Figure 0‑5* *(b)*, :math:`S`
is not independent from :math:`X_{1}` given
:math:`\mathcal{p(\{}X_{1},X_{2}\})`, since the convergent connection
:math:`D` has a descendant :math:`X_{2}`.

-  *Theorem* *0‑5* **Factorization Theorem**: the following three
   statements are equivalent. Since d-separation is not straightforward,
   the following equivalence validates the more intuitive local Markov
   property as a modeling criterion.

1) :math:`\mathbb{P}` respects graph :math:`G`, or :math:`\mathbb{P}`
   factorizes according to :math:`G`, i.e.
   :math:`\mathbb{P}\left( G \right) = \prod_{X \in G}^{}{\mathbb{P(}X|\mathcal{p(}X))}`
   where we overload :math:`G` to also represent the RVs in the graph;

2) :math:`G` satisfies d-separation (global Markov property) according
   w.r.t. :math:`\mathbb{P}`;

3) :math:`G` satisfies local Markov property according w.r.t.
   :math:`\mathbb{P}`.

..

   *Theorem 0‑2* guarantees 1) :math:`\Rightarrow` 2) holds, and
   *Theorem 0‑3* ensures 2) :math:`\Rightarrow` 3), we need to prove 3)
   :math:`\Rightarrow` 1). We do this by induction. For one-node graph
   :math:`G_{1}`, 3) :math:`\Rightarrow` 1) trivially holds. Now
   consider that 3) :math:`\Rightarrow` 1) holds for a graph
   :math:`G_{n - 1}` of :math:`n - 1` nodes, then for graph
   :math:`G_{n}` of :math:`n` nodes, and let the additional node be
   :math:`X`, which has to be a leaf, then by 3) :math:`X` is
   independent from all other nodes given :math:`\mathcal{p(}X)`, and so

.. math:: \mathbb{P}\left( G_{n} \right)\mathbb{= P}\left( X,G_{n - 1} \right)\mathbb{= P}\left( G_{n - 1} \right)\mathbb{P}\left( X \middle| G_{n - 1} \right)\mathbb{= P}\left( G_{n - 1} \right)\mathbb{P}\left( X \middle| \mathcal{p(}X) \right)

   Since we assume :math:`\mathbb{P}\left( G_{n - 1} \right)` respects
   :math:`G_{n - 1}`, then clearly
   :math:`\mathbb{P}\left( G_{n} \right)` factorizes according to
   :math:`G_{n}`.

-  As mentioned `above <#b10>`__, although d-separation implies
   independence, independence does not necessarily imply d-separation.
   Given a distribution :math:`\mathbb{P(}V)` where :math:`V` is a set
   of RVs, if a DAG :math:`G = (V,E)` satisfies the d-separation
   theorem, then :math:`G` is called an **I-map** of :math:`\mathbb{P}`.
   Clearly every Bayesian network is an I-map. On the contrary, if
   independence always implies d-separation on :math:`G`, then :math:`G`
   is a **D-map** of :math:`\mathbb{P}`. :math:`G` is a **perfect map**
   of :math:`\mathbb{P}` if it is both an I-map and a D-map.

   *Theorem* *0‑6* Adding an edge in an I-map results in another I-map;
   deleting an edge in a D-map results in another D-map. Prove is FUTURE
   WORK.

+-----------------------------------------------------------------------+
| REMARK: Pairwise (Marginal) Independence & Mutual Independence        |
+=======================================================================+
| It is well known that pairwise (marginal) independence does not imply |
| mutual independence. For example, suppose :math:`X,Y` are two         |
| independent tosses of coins, and :math:`X,Y = 1` if they take head,   |
| or :math:`X,Y = 0` if they take tail. Let :math:`Z` be a third RV     |
| s.t. :math:`Z = \left( X + Y \right)\ \text{mod}\ 2` (:math:`Z` takes |
| :math:`1` when exactly one of :math:`X,Y` takes :math:`1`). We see    |
| that                                                                  |
|                                                                       |
| .. math:: \mathbb{P}\left( Z = 1 \right)\mathbb{= P}\left( X = 1,Y =  |
| 0 \right)\mathbb{+ P}\left( X = 0,Y = 1 \right) = \frac{1}{2} \Righta |
| rrow \mathbb{P}\left( Z = 0 \right) = \frac{1}{2}                     |
|                                                                       |
| Thus                                                                  |
|                                                                       |
| .. math:: \mathbb{P}\left( X = 1,Z = 1 \right)\mathbb{= P}\left( X =  |
| 1,Y = 0 \right) = \frac{1}{4}\mathbb{= P}\left( X = 1 \right)\mathbb{ |
| P(}Z = 1)                                                             |
|                                                                       |
| .. math:: \mathbb{P}\left( X = 1,Z = 0 \right)\mathbb{= P}\left( X =  |
| 1,Y = 1 \right) = \frac{1}{4}\mathbb{= P}\left( X = 1 \right)\mathbb{ |
| P(}Z = 0)                                                             |
|                                                                       |
| .. math:: \mathbb{P}\left( X = 0,Z = 1 \right)\mathbb{= P}\left( X =  |
| 0,Y = 1 \right) = \frac{1}{4}\mathbb{= P}\left( X = 0 \right)\mathbb{ |
| P(}Z = 1)                                                             |
|                                                                       |
| .. math:: \mathbb{P}\left( X = 0,Z = 0 \right)\mathbb{= P}\left( X =  |
| 0,Y = 0 \right) = \frac{1}{4}\mathbb{= P}\left( X = 0 \right)\mathbb{ |
| P(}Z = 0)                                                             |
|                                                                       |
| which implies                                                         |
| :math:`\mathbb{P}\left( X,Z \right)\mathbb{= P}\left( X \right)\mathb |
| b{P(}Z)`.                                                             |
| It is simple to show                                                  |
| :math:`\mathbb{P}\left( Y,Z \right)\mathbb{= P}\left( Y \right)\mathb |
| b{P(}Z)`                                                              |
| as well. However,                                                     |
|                                                                       |
| .. math:: \mathbb{P}\left( X = 1,Y = 1,Z = 1 \right) = 0 \neq \mathbb |
| {P}\left( X = 1 \right)\mathbb{P}\left( Y = 1 \right)\mathbb{P}\left( |
|  Z = 1 \right) = \frac{1}{8}                                          |
|                                                                       |
| Conversely, mutual independence always implies marginal independence. |
| Given a set of RVs :math:`\mathcal{X}`, let                           |
| :math:`\mathcal{Y \subseteq X}`, then                                 |
|                                                                       |
| .. math:: \mathbb{P}\left( \mathcal{Y} \right) = \sum_{W \in \mathcal |
| {X\backslash Y}}^{}{\mathbb{P}\left( \mathcal{X} \right)} = \sum_{W \ |
| in \mathcal{X\backslash Y}}^{}{\prod_{W \in \mathcal{X}}^{}{\mathbb{P |
| (}W)}} = \left( \prod_{W \in \mathcal{Y}}^{}{\mathbb{P}\left( W \righ |
| t)} \right)\left( \sum_{W \in \mathcal{X\backslash Y}}^{}{\prod_{W \i |
| n \mathcal{X\backslash Y}}^{}{\mathbb{P}\left( W \right)}} \right) =  |
| \left( \prod_{W \in \mathcal{Y}}^{}{\mathbb{P}\left( W \right)} \righ |
| t)\left( \prod_{W \in \mathcal{X\backslash Y}}^{}{\sum_{W \in \mathca |
| l{X\backslash Y}}^{}{\mathbb{P}\left( W \right)}} \right) = \prod_{W  |
| \in \mathcal{Y}}^{}{\mathbb{P}\left( W \right)}                       |
|                                                                       |
| *Theorem* *0‑7* We further claim that, a set of RVs                   |
| :math:`\mathcal{N = (}X_{1},\ldots,X_{n})` are independent iff for    |
| any :math:`\mathcal{X \subseteq N,Y \subseteq N}`,                    |
| :math:`\mathcal{X\bot Y}`. *Sufficiency* is simply by chain rule,     |
|                                                                       |
| .. math:: \mathbb{P}\left( X_{1},\ldots,X_{n} \right)\mathbb{= P}\lef |
| t( X_{n} \middle| X_{1},\ldots,X_{n - 1} \right)\mathbb{\ldots P}\lef |
| t( X_{3} \middle| X_{1},X_{2} \right)\mathbb{P}\left( X_{2} \middle|  |
| X_{1} \right)\mathbb{P}\left( X_{1} \right)\mathbb{= P(}X_{n}\mathbb{ |
| )\ldots P(}X_{3}\mathbb{)P(}X_{2}\mathbb{)P}\left( X_{1} \right)      |
|                                                                       |
| *Necessity*.                                                          |
| :math:`\mathbb{P}\left( \mathcal{X,Y} \right) = \prod_{W \in \mathcal |
| {X\bigcup Y}}^{}{\mathbb{P}\left( W \right)} = \prod_{W \in \mathcal{ |
| X}}^{}{\mathbb{P}\left( W \right)}\prod_{W \in \mathcal{Y}}^{}{\mathb |
| b{P}\left( W \right)}`,                                               |
| where                                                                 |
| :math:`\prod_{W \in \mathcal{X}}^{}{\mathbb{P}\left( W \right)}` is a |
| function only dependent on :math:`\mathcal{X}`, and                   |
| :math:`\prod_{W \in \mathcal{Y}}^{}{\mathbb{P}\left( W \right)}` is a |
| function only dependent on :math:`\mathcal{Y}`, thus                  |
| :math:`\mathcal{X\bot Y}`.                                            |
+-----------------------------------------------------------------------+

EX 1. Prove that if
:math:`\mathcal{p}\left( \mathcal{X} \right) = \{ Y\}` is a singleton,
then :math:`\mathcal{X}` are mutually independent given :math:`Y`.

Key. For any two :math:`\mathcal{X}`, say :math:`X_{1},X_{2}`, either
there is no :math:`X_{1}\sim X_{2}` a :math:`X_{1}\sim X_{2}` path
either goes through a node in :math:`\mathcal{p(X)}`, which is blocked,
or we claim the path must have a converging connection. If so, the
converging connection cannot have a descendant in
:math:`\mathcal{p(}X)`, otherwise there would be a cycle.

An :math:`X_{1}\sim X_{2}` path not going through :math:`\mathcal{p(X)}`
must have the form
:math:`X_{1} \rightarrow C_{1}\sim D \leftarrow X_{2}`. If
:math:`C_{1} = D`, the above claim holds. If :math:`C_{1} \neq D`, and
suppose there is no convergent connecting, then the path must have the
form
:math:`X_{1} \rightarrow C_{1} \rightarrow C_{2}\sim D \leftarrow X_{2}`,
where :math:`C_{2} \neq D` by assumption. Continue this construction the
path will be of infinite length. Thus a :math:`X_{1}\sim X_{2}` path not
going through :math:`\mathcal{p(X)}` must have a convergent connection.
As a result, :math:`X_{1},X_{2}` are d-separated by :math:`Y`. Let
:math:`\mathcal{A,B}` be any two subsets of :math:`\mathcal{X}`, then
clearly :math:`\mathcal{A,B}` are d-separated by :math:`Y`, and hence
:math:`\mathcal{A\bot B|}Y`. By *Theorem 0‑7*, :math:`\mathcal{X}` are
mutually independent given :math:`Y`.

EX 2. Let’s exercise on the following network about d-separation.

|image23|

+-----------------+-----------------+-----------------+-----------------+
| .. math:: \math | .. math:: \math | .. math:: \math | d-separated?    |
| cal{X}          | cal{Y}          | cal{Z}          |                 |
+=================+=================+=================+=================+
| .. math:: A     | .. math:: G     | .. math:: \{ B, | Yes. Any path   |
|                 |                 | M\}             | through         |
|                 |                 |                 | :math:`G \right |
|                 |                 |                 | arrow J \leftar |
|                 |                 |                 | row F`          |
|                 |                 |                 | is blocked at   |
|                 |                 |                 | :math:`J`; any  |
|                 |                 |                 | path through    |
|                 |                 |                 | :math:`G \right |
|                 |                 |                 | arrow I \righta |
|                 |                 |                 | rrow L \leftarr |
|                 |                 |                 | ow I`           |
|                 |                 |                 | is blocked at   |
|                 |                 |                 | :math:`L`.      |
+-----------------+-----------------+-----------------+-----------------+
| .. math:: A     | .. math:: M     | .. math:: \{ E, | Yes. Any        |
|                 |                 | K,L\}           | :math:`A\sim M` |
|                 |                 |                 | path is blocked |
|                 |                 |                 | at :math:`K`,   |
|                 |                 |                 | since both      |
|                 |                 |                 | :math:`I \right |
|                 |                 |                 | arrow K \righta |
|                 |                 |                 | rrow M`         |
|                 |                 |                 | and             |
|                 |                 |                 | :math:`H \right |
|                 |                 |                 | arrow K - M`    |
|                 |                 |                 | are serial.     |
+-----------------+-----------------+-----------------+-----------------+
| .. math:: A     | .. math:: \{ C, | .. math:: \varn | Yes. Any        |
|                 | F,G,J\}         | othing          | :math:`A\sim\{  |
|                 |                 |                 | C,F,G,J\}`      |
|                 |                 |                 | path has to go  |
|                 |                 |                 | through a       |
|                 |                 |                 | converging      |
|                 |                 |                 | connection,     |
|                 |                 |                 | either          |
|                 |                 |                 | :math:`E` or    |
|                 |                 |                 | :math:`L`.      |
+-----------------+-----------------+-----------------+-----------------+
| .. math:: A     | .. math:: \{ E, | .. math:: \{ B, | No.             |
|                 | I\}             | M\}             | :math:`A\sim H  |
|                 |                 |                 | \rightarrow K \ |
|                 |                 |                 | leftarrow I`    |
|                 |                 |                 | path is not     |
|                 |                 |                 | blocked at      |
|                 |                 |                 | :math:`K` since |
|                 |                 |                 | :math:`M` is a  |
|                 |                 |                 | descendant of   |
|                 |                 |                 | the converging  |
|                 |                 |                 | connection      |
|                 |                 |                 | :math:`K`.      |
+-----------------+-----------------+-----------------+-----------------+
| .. math:: \{ C, | .. math:: \{ B, | .. math:: \{ E, | No.             |
| L\}             | K\}             | I\}             | :math:`C \right |
|                 |                 |                 | arrow E \leftar |
|                 |                 |                 | row B`          |
|                 |                 |                 | path is not     |
|                 |                 |                 | blocked at      |
|                 |                 |                 | :math:`E`.      |
+-----------------+-----------------+-----------------+-----------------+

EX 3. Gives a counter example that
:math:`\mathbb{P}\left( X,Y \middle| Z \right)\mathbb{= P}\left( X \middle| Z \right)\mathbb{P(}Y|Z)`
does not imply
:math:`\mathbb{P}\left( X,Y \middle| Z,W \right)\mathbb{= P}\left( X \middle| Z,Z' \right)\mathbb{P(}Y|Z,W)`
for any :math:`W \notin \{ X,Y\}`.

Key. In the network of our previous exercise, :math:`I\bot J|C`, since
:math:`I\sim C\sim J\ `\ path is divergently blocked by :math:`C`, and
convergently blocked at :math:`L`. However, :math:`I,J` are clearly not
independent given :math:`\{ C,L\}`.

A concrete example would be like the following, suppose
:math:`X,Y,Z,W \in \{ 0,1\}`, and the distribution of
:math:`\mathbb{P(}X,Y|Z)` and an illustration of the whole sample space
are given below,

+---------------------------------------------------------------------+--+
| +-----------------------------+-----------------+-----------------+ |  |
| | .. math:: \mathbb{P(}X,Y|Z) |                                     |  |
| +=============================+=================+=================+ |  |
| |                             | .. math:: X = 0 | .. math:: X = 1 | |  |
| +-----------------------------+-----------------+-----------------+ |  |
| | .. math:: Y = 0             | 0.25            | 0.25            | |  |
| +-----------------------------+-----------------+-----------------+ |  |
| | .. math:: Y = 1             | 0.25            | 0.25            | |  |
| +-----------------------------+-----------------+-----------------+ |  |
+---------------------------------------------------------------------+--+

Clearly we have
:math:`\mathbb{P}\left( X = x,Y = y \middle| Z = z \right)\mathbb{= P}\left( X = x \middle| Z = z \right)\mathbb{P}\left( Y = y \middle| Z = z \right) = 0.25,\ \forall x,y,z \in \{ 0,1\}`.
However, check that :math:`W` gives information to :math:`X,Y` and make
them dependent,

.. math:: \mathbb{P}\left( X = 0,Y = 0 \middle| Z = 0,W = 0 \right) = 0.5

.. math:: \mathbb{P}\left( X = 0, \middle| Z = 0,W = 0 \right)\mathbb{= P}\left( Y = 0, \middle| Z = 0,W = 0 \right) = 0.75

EX 4. In a Bayesian network, suppose siblings :math:`X,Y` are
independent given
:math:`\mathcal{p}\left( \left\{ X,Y \right\} \right)`. Would it be
possible that the independence is broken given additional RVs?

Key. Yes, it is possible by adding descendants. In the network of EX 2,
we see that
:math:`F\bot E|\mathcal{p}\left( \left\{ F,E \right\} = C \right)`.
However, :math:`F,E` are no longer independent given :math:`\{ C,L\}`.
However, by local Markov property, adding any non-descendant will not
break independence.

Hidden Markov Model. **Hidden Markov Model** (HMM), which assumes the states are latent but yields some observable outputs, is an extension on Markov chain. Recall a Markov chain is characterized by :math:`(V,\mathbf{\mu},\mathbf{P})` where :math:`V = \{ v_{1},\ldots,v_{m}\}` is the state space, :math:`\mu` is the initial distribution, and :math:`\mathbf{P}` is the transition matrix. HMM extends this to a four tuple :math:`(V,\mathbf{\mu},\mathbf{P,}W,\mathbf{Q})` where :math:`W = \{ w_{1},\ldots,w_{n}\}` is the set of all possible observations and :math:`\mathbf{Q}` is an :math:`m \times n` state-observation probability matrix, or **emission distributions**, where :math:`\mathbf{Q}\left( i,j \right),i = 1,\ldots,m,j = 1,\ldots,n` is the probability of observing output :math:`x_{j}` given state :math:`z_{i}`. For a concrete simple example, suppose a machine can jump between two states :math:`Z = \{ 1, - 1\}` but tend to maintain the current states, say the transition matrix is :math:`\mathbf{P} = \begin{pmatrix}
0.9 & 0.1 \\
0.1 & 0.9 \\
\end{pmatrix}`. However, the current state of the machine is not directly observable, but it yields some observable output based on some Gaussian distribution, as illustrated in Figure 0‑1. Although the true states are assumed latent, they can be guessed from the output to a large extent.
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

+-----------------------------------------------------------------------+
| |E:\OneDrive\Pictures\figure_3.png|                                   |
+=======================================================================+
| *Figure* *0‑1 A simple illustration of HMM. The red dots are observed |
| outputs. The blue dots are actual states which are assumed to be      |
| unobservable.*                                                        |
+-----------------------------------------------------------------------+

The Bayesian network representing the HMM is shown in *Figure 0‑2*,
where :math:`Z = (Z_{1},\ldots,Z_{k})` are the latent states at time
:math:`t = 1,\ldots,k`, and :math:`X = (X_{1},\ldots,X_{k})` are the
observed outputs at each corresponding time. Here
:math:`Z_{t} \in V,X_{t} \in W`. Also note this is not the graph
representation of the Markov chain, whose nodes are the states. The
factorization is given below, and our objective is to estimate the
probability of latent variables :math:`Z` given observation :math:`X`,
i.e. :math:`\mathbb{P(}Z|X)`.

.. math:: \mathbb{P}\left( V \right)\mathbb{= P}\left( Z_{1} \right)\mathbb{P}\left( X_{1} \middle| Z_{1} \right)\prod_{t = 2}^{k}{\mathbb{P}\left( Z_{t} \middle| Z_{t - 1} \right)\mathbb{P(}X_{t}|Z_{t})} = \mathbf{\mu}(z_{1})\mathbf{Q}(z_{1},x_{1})\prod_{t = 2}^{k}{\mathbf{P}(z_{t - 1},z_{t})\mathbf{Q}(z_{t},x_{t})}

*Figure* *0‑2 Bayesian network of* HMM\ *, where shaded nodes are
observable RVs.*

-  If :math:`\mathbf{P},\mathbf{Q},\mathbf{\mu}` are known, we can use
   the so-called **forward-backward algorithm** to exactly compute
   :math:`\mathbb{P(}Z_{t}|X)` for every :math:`t = 1,\ldots,k`, where
   the **forward part** computes
   :math:`\mathbb{P(}Z_{t},X_{1},\ldots,X_{t})`, and the **backward
   part** computes :math:`\mathbb{P(}X_{t + 1},\ldots,X_{k}|Z_{t})`.
   Denote :math:`\left( X_{i},X_{i + 1},\ldots,X_{j} \right)` as
   :math:`X_{i:j}`, the reason for this algorithm is shown below,

.. math:: \mathbb{P}\left( Z_{t} \middle| X \right)\mathbb{\propto P}\left( Z_{t},X \right)\mathbb{= P}\left( X_{t + 1:k}|Z_{t},X_{1:t} \right)\mathbb{P}\left( Z_{t},X_{1:t} \right) = \mathbb{P}\left( X_{t + 1:k}|Z_{t} \right)\mathbb{P}\left( Z_{t},X_{1:t} \right)

   which is due to :math:`X_{1:t}\bot X_{t + 1:k}|Z_{t}` by d-separation
   theorem. The forward part is a recursion shown below, which can be
   solved by a simple dynamic programming algorithm.

.. math:: \mathbb{P}\left( Z_{t},X_{1:t} \right) = \sum_{z_{t - 1} \in V}^{}{\mathbb{P(}Z_{t - 1},Z_{t},X_{1:t})} = \sum_{z_{t - 1} \in V}^{}{\mathbb{P}\left( X_{t} \middle| Z_{t - 1},Z_{t},X_{1:t - 1} \right)\mathbb{P}\left( Z_{t} \middle| Z_{t - 1},X_{1:t - 1} \right)\mathbb{P}\left( Z_{t - 1},X_{1:t - 1} \right)} = \sum_{z_{t - 1} \in V}^{}{\mathbb{P}\left( X_{t} \middle| Z_{t} \right)\mathbb{P}\left( Z_{t} \middle| Z_{t - 1} \right)\mathbb{P}\left( Z_{t - 1},X_{1:t - 1} \right)} = \sum_{z_{t - 1} \in V}^{}{\mathbf{Q}(z_{t},x_{t})\mathbf{P}(z_{t - 1},z_{t})\mathbb{P}\left( Z_{t - 1},X_{1:t - 1} \right)}

   where the recursion terminates at

.. math:: \mathbb{P}\left( Z_{1},X_{1} \right)\mathbb{= P}\left( X_{1} \middle| Z_{1} \right)\mathbb{P}\left( Z_{1} \right) = \mathbf{Q}(z_{1},x_{1})\mathbf{\mu}(z_{1})

   The time complexity for the forward part is :math:`\Theta(km^{2})`,
   since we need to compute
   :math:`\mathbb{P}\left( Z_{t},X_{1:t} \right)` for each
   :math:`t = 1,\ldots,k`, for each :math:`z_{t} \in V`, and sum over
   each :math:`z_{t - 1} \in V`. Now we compute the backward part
   :math:`\mathbb{P(}X_{t + 1:k}|Z_{t})`, and we also try to setup a
   recursion,

.. math:: \mathbb{P}\left( X_{t + 1:k} \middle| Z_{t} \right) = \sum_{z_{t + 1} \in V}^{}{\mathbb{P}\left( X_{t + 1:k},Z_{t + 1} \middle| Z_{t} \right)} = \sum_{z_{t + 1} \in V}^{}{\mathbb{P(}X_{t + 2:k}|X_{t + 1},Z_{t},Z_{t + 1}\mathbb{)P}\left( X_{t + 1} \middle| Z_{t},Z_{t + 1} \right)\mathbb{P}\left( Z_{t + 1} \middle| Z_{t} \right)} = \sum_{z_{t + 1} \in V}^{}{\mathbb{P(}X_{t + 2:k}|Z_{t + 1}\mathbb{)P}\left( X_{t + 1} \middle| Z_{t + 1} \right)\mathbb{P}\left( Z_{t + 1} \middle| Z_{t} \right)} = \sum_{z_{t + 1} \in V}^{}{\mathbb{P}\left( X_{t + 2:k} \middle| Z_{t + 1} \right)\mathbf{Q}(z_{t + 1}\mathbf{,}x_{t + 1}\mathbf{)P}(z_{t},z_{t + 1})}

   where the recursion terminates at

.. math:: \mathbb{P}\left( X_{k} \middle| Z_{k - 1} \right) = \sum_{z_{k} \in V}^{}{\mathbb{P}\left( X_{k},Z_{k} \middle| Z_{k} \right)} = \sum_{z_{k} \in V}^{}{\mathbb{P}\left( X_{k} \middle| Z_{k} \right)} = \sum_{z_{k} \in V}^{}{\mathbf{Q}(z_{k},x_{k})}

   and this is a sum over the column :math:`\mathbf{Q}( \cdot ,x_{k})`.
   The time complexity for backward part is also :math:`\Theta(km^{2})`,
   since we need to compute
   :math:`\mathbb{P(}X_{t + 1},\ldots,X_{k}|Z_{t})` for each
   :math:`t = k,\ldots,1`, for each :math:`z_{t} \in V`, and sum over
   each :math:`z_{t + 1} \in V`. Thus the time complexity for the whole
   forward-backward algorithm is also :math:`\Theta(km^{2})`.

Again, the forward-backward algorithm can find the conditional
distribution :math:`\mathbb{P(}Z_{t}|X)` for any time :math:`t`, and
therefore find
:math:`z_{t}^{*} = \arg{\operatorname{}{\mathbb{P(}Z_{t} = z_{t}|X)}}`.
Note an important feature here is that :math:`z_{t}^{*}` can be updated
when given more observations :math:`X`. In applications like voice
recognition, we might update recognized words in the past after the
program collects more voice from user.

-  Sometimes we want to find out
   :math:`\mathbf{z}_{1:t} = \arg{\operatorname{}{\mathbb{P(}Z_{1:t} = \mathbf{z}_{1:t}|X_{1:t})}}`.
   An algorithm to find this is the **Viterbi Algorithm**. As before we
   try to setup the recursion in the form

.. math:: \arg{\operatorname{}{\mathbb{P(}Z_{1:t} = \mathbf{z}_{1:t}|X_{1:t})}} = \ldots\arg{\operatorname{}{\mathbb{P}\left( Z_{1:t} = \mathbf{z}_{1:t} \middle| X_{1:t} \right)}}\ldots

The trick here is to fix :math:`Z_{t} = z_{t}` for some
:math:`z_{t} \in V`, then

.. math:: \arg{\operatorname{}{\mathbb{P(}Z_{1:t}|X_{1:t})}} = \arg{\operatorname{}{\mathbb{P(}Z_{1:t},X_{1:t}\ )}} = \arg{\operatorname{}{\mathbb{P(}X_{t}|Z_{1:t},X_{1:t - 1}\mathbb{)P(}Z_{t}|Z_{1:t - 1},X_{1:t - 1}\mathbb{)P(}Z_{1:t - 1},X_{1:t - 1})}} = \arg{\operatorname{}{\mathbb{P(}X_{t}|Z_{t}\mathbb{)P(}Z_{t}|Z_{t - 1}\mathbb{)P(}Z_{1:t - 1},X_{1:t - 1})}}

Note for any function :math:`f,g` we have

.. math:: \operatorname{}{f(a)g(a,b)} = \operatorname{}\left( \operatorname{}{f\left( a \right)g(a,b)} \right) = \operatorname{}\left( f\left( a \right)\operatorname{}{g(a,b)} \right)

And thus we have

.. math:: \operatorname{}{\mathbb{P(}Z_{1:t} = \mathbf{z}_{1:t},X_{1:t})} = \operatorname{}{\mathbb{P(}X_{t}|Z_{t}\mathbb{)P(}Z_{t}|Z_{t - 1}\mathbb{)P(}Z_{1:t - 1},X_{1:t - 1})} = \operatorname{}{\mathbb{P(}X_{t}|Z_{t} = z_{t}\mathbb{)P(}Z_{t} = z_{t}|Z_{t - 1}\mathbb{)P(}Z_{1:t - 1} = \mathbf{z}_{1:t - 1},X_{1:t - 1})} = \operatorname{}\left( \mathbb{P}\left( X_{t} \middle| Z_{t} = z_{t} \right)\mathbb{P}\left( Z_{t} = z_{t} \middle| Z_{t - 1} \right)\operatorname{}{\mathbb{P}\left( Z_{1:t - 1} = \mathbf{z}_{1:t - 1},X_{1:t - 1} \right)} \right) = \operatorname{}\left( \mathbf{Q}\left( z_{t},x_{t} \right)\mathbf{P}\left( z_{t - 1},z_{t} \right)\operatorname{}{\mathbb{P}\left( Z_{1:t - 1} = \mathbf{z}_{1:t - 1},X_{1:t - 1} \right)} \right)

This is a valid recursion since the original problem is reduced to
:math:`\operatorname{}{\mathbb{P(}Z_{1:t - 1} = \mathbf{z}_{1:t - 1},X_{1:t - 1})}`
by fixing :math:`Z_{t - 1} = z_{t - 1}`. The recursion stops when
:math:`t = 2`,

.. math:: \operatorname{}{\mathbb{P(}Z_{1},Z_{2},X_{1},X_{2})} = \operatorname{}(\mathbf{Q}(z_{1},x_{1})\mathbf{P}(z_{1},z_{2})\mathbf{Q}(z_{2},x_{2})) = \operatorname{}(\mathbf{Q}(z_{1},x_{1})\mathbf{P}(z_{1},z_{2}))

where the last identity is a due to :math:`\mathbf{Q}(z_{2},x_{2})` is
constant since :math:`z_{2},x_{2}` are given. We can visualize the above
recursion by **maximum probability paths** shown below,

+-----------------------------------+-----------------------------------+
|                                   | *(1) For every*                   |
|                                   | :math:`z_{2} \in V`\ *, find*     |
|                                   |                                   |
|                                   | .. math:: z_{1}^{*}(z_{2}) = \arg |
|                                   | {\operatorname{}{\mathbb{P(}Z_{1: |
|                                   | 2},X_{1:2})}}                     |
|                                   |                                   |
|                                   | *and draw a line from*            |
|                                   | :math:`z_{2}` *to*                |
|                                   | :math:`z_{1}^{*}`\ *. Store the   |
|                                   | maximum values*                   |
|                                   | :math:`z_{1}^{*}(z_{2})` *at      |
|                                   | corresponding* :math:`z_{2}`      |
|                                   | *node.*                           |
+===================================+===================================+
|                                   | *(2) For every*                   |
|                                   | :math:`z_{3} \in V`\ *, find*     |
|                                   |                                   |
|                                   | .. math:: z_{2}^{*}\left( z_{3} \ |
|                                   | right) = \arg{\operatorname{}{\ma |
|                                   | thbb{P}\left( Z_{1:3},X_{1:3} \ri |
|                                   | ght)}} = \operatorname{}\left( \m |
|                                   | athbf{Q}\left( z_{3},x_{3} \right |
|                                   | )\mathbf{P}\left( z_{2},z_{3} \ri |
|                                   | ght)z_{1}^{*}(z_{2}) \right)      |
|                                   |                                   |
|                                   | *and draw a line from*            |
|                                   | :math:`z_{3}` *to*                |
|                                   | :math:`z_{2}^{*}`\ *. We simply   |
|                                   | use the previously found max      |
|                                   | values stored at each*            |
|                                   | :math:`z_{2}` *node, and do not   |
|                                   | need to compute the full          |
|                                   | recursion. Store the maximum      |
|                                   | values*                           |
|                                   | :math:`z_{2}^{*}\left( z_{3} \rig |
|                                   | ht)`                              |
|                                   | *at corresponding* :math:`z_{3}`  |
|                                   | *node.*                           |
+-----------------------------------+-----------------------------------+
|                                   | *(3) Continue until for every*    |
|                                   | :math:`z_{t} \in V`\ *we find*    |
|                                   |                                   |
|                                   | .. math:: z_{t - 1}^{*}(z_{t}) =  |
|                                   | \operatorname{}\left( \mathbf{Q}\ |
|                                   | left( z_{t},x_{t} \right)\mathbf{ |
|                                   | P}\left( z_{t - 1},z_{t} \right)z |
|                                   | _{t - 2}^{*}(z_{t - 1}) \right)   |
|                                   |                                   |
|                                   | *Then simply find*                |
|                                   | :math:`z_{t}^{*} = \arg{\max{z_{t |
|                                   |  - 1}^{*}(z_{t})}}`               |
|                                   | *and the corresponding path as    |
|                                   | our optimal solution of*          |
|                                   | :math:`\mathbf{z}_{1:t} = \arg{\o |
|                                   | peratorname{}{\mathbb{P(}Z_{1:t}  |
|                                   | = \mathbf{z}_{1:t}|X_{1:t})}}`\ * |
|                                   | .*                                |
+-----------------------------------+-----------------------------------+

The time complexity is :math:`tm^{2}`, since at the :math:`i`\ th step
we need to maximize over :math:`z_{i} \in V` for every
:math:`z_{i + 1} \in V`, and there are total :math:`t` steps; and space
complexity is clearly :math:`\text{tm}`.

+-----------------------------------------------------------------------+
| REMARK: Underflow Problem                                             |
+=======================================================================+
| In general, to compute :math:`\log{\sum_{i}^{}e^{- a_{i}}}` for large |
| :math:`a_{i}`, if we brutally add them up, some terms would be so     |
| small that they are truncated when running the sum on a computer.     |
| Although one truncation of small quantity might be negligible,        |
| truncations of many such small quantities could be significant when   |
| added up. A solution is to extract :math:`b = \max{\{ a_{i}\}}` from  |
| the sum, which is called the **log-sum-exp trick**.                   |
|                                                                       |
| .. math:: \log{\sum_{i}^{}e^{- a_{i}}} = \log{e^{- b}\sum_{i}^{}e^{b  |
| - a_{i}}} = - b + \log{\sum_{i}^{}e^{b - a_{i}}}                      |
|                                                                       |
| For example, the following codes would yield slightly different       |
| results in python, where the first one using log-sum-exp trick yields |
| more accurate result.                                                 |
|                                                                       |
| +--------------------------------+--------------------------------+   |
| | print \\                       | b = math.exp(-2)               |   |
| |                                |                                |   |
| | (                              | for i in range(0,1000000):     |   |
| |                                |                                |   |
| |     -30 + \\                   |     b += math.exp(-30)         |   |
| |                                |                                |   |
| |     math.log(math.exp(30 - 2)  | print(math.log(b))             |   |
| | + \\                           |                                |   |
| |                                |                                |   |
| |     1000000 * math.exp(30 - 30 |                                |   |
| | )) \\                          |                                |   |
| |                                |                                |   |
| | )                              |                                |   |
| +================================+================================+   |
| | *output: -1.999999308560227*   | *output: -1.9999993086502585*  |   |
| +--------------------------------+--------------------------------+   |
| | *true value:                   |                                |   |
| | -1.999999308560228350513703368 |                                |   |
| | 44588381767981954557023664913… |                                |   |
| | *                              |                                |   |
| +--------------------------------+--------------------------------+   |
|                                                                       |
| In the forward-backward algorithm, the probabilities                  |
| :math:`\mathbb{P}\left( Z_{t},X_{1},\ldots,X_{t} \right)` and         |
| :math:`\mathbb{P(}X_{t + 1},\ldots,X_{k}|Z_{t})` could be very small  |
| when :math:`m,n` are very large s.t. the probability is distributed   |
| over large matrices :math:`\mathbf{P},\mathbf{Q}`.                    |
| :math:`\mathbb{P}\left( Z_{t},X_{1},\ldots,X_{t} \right)` could also  |
| be very small when :math:`t` approaches :math:`k` if :math:`k` is     |
| very large, and :math:`\mathbb{P(}X_{t + 1},\ldots,X_{k}|Z_{t})`      |
| could also be very small when :math:`t` approaches :math:`1` if       |
| :math:`k` are very large. Taking the forward part for example, let    |
| :math:`\mathbb{P}\left( Z_{t},X_{1},\ldots,X_{t} \right) = p_{t}` and |
| :math:`\mathbf{Q}\left( z_{t},x_{t} \right)\mathbf{P}\left( z_{t - 1} |
| ,z_{t} \right) = c_{t}`,                                              |
| then                                                                  |
|                                                                       |
| .. math:: \mathbb{P}\left( Z_{t},X_{1},\ldots,X_{t} \right) = \sum_{z |
| _{t - 1} \in V}^{}{\mathbf{Q}\left( z_{t},x_{t} \right)\mathbf{P}\lef |
| t( z_{t - 1},z_{t} \right)\mathbb{P}\left( Z_{t - 1},X_{1},\ldots,X_{ |
| t - 1} \right)} \Rightarrow p_{t} = \sum_{z_{t - 1} \in V}^{}{c_{t}p_ |
| {t - 1}}                                                              |
|                                                                       |
| The underflow problem happens when some :math:`c_{t}p_{t - 1}` are    |
| very small and truncated by the sum when running a computer program.  |
| In this case, we can use the above log-sum-exp trick to cope with the |
| small probability problem. Let                                        |
| :math:`b = \operatorname{}{\{\log c_{t} + \log p_{t - 1}\}}`, and we  |
| have                                                                  |
|                                                                       |
| .. math:: \log p_{t} = \log{\sum_{z_{t - 1} \in V}^{}{c_{t}p_{t - 1}} |
| } = \log{\sum_{z_{t - 1} \in V}^{}e^{\log{c_{t}p_{t - 1}}}} = \log{\s |
| um_{z_{t - 1} \in V}^{}e^{\log c_{t} + \log p_{t - 1}}} = b + \log{\s |
| um_{z_{t - 1} \in V}^{}e^{\log c_{t} + \log p_{t - 1} - b}}           |
+-----------------------------------------------------------------------+

**Markov Random Fields**
^^^^^^^^^^^^^^^^^^^^^^^^

Stochastic Block Mode
~~~~~~~~~~~~~~~~~~~~~

**The Standard Model**
^^^^^^^^^^^^^^^^^^^^^^

**Stochastic Block Model** (SBM) is a probabilistic generative model
that clusters vertexes of a graph :math:`G = (V,E)` into :math:`k`
groups by modeling generation of the graph edges :math:`E`, i.e. in this
model when all parameters are given, the probability of a particular
edge set is determined. Here :math:`k` is pre-defined, each group is
also named a **block** or a **community**, and the finite sample space
for this model is all possible edge set. The sample space is very large,
of size :math:`2^{\left| V \right|^{2} - |V|}` for a directed graph or
:math:`2^{\left| V \right|^{2} - \left| V \right| - 1}` for an
undirected graph. WLOG, let the vertexes in :math:`V` be integers
:math:`V = \{ 1,\ldots,|V|\}`, then the model parameters include

1) A membership vector
   :math:`\mathbf{z} = \left( z_{1},\ldots,z_{\left| V \right|} \right)`
   s.t.
   :math:`z_{u} \in \left\{ 1,\ldots,k \right\},\forall u = 1,\ldots,\left| V \right|`,
   indicating the membership of each vertex, i.e. :math:`u` is in group
   :math:`z_{u}`.

2) A :math:`k \times k` matrix named **stochastic block matrix**
   :math:`\mathbf{P}` s.t. :math:`\mathbf{P}(i,j)` is the probability of
   a vertex in group :math:`i` has an edge with a vertex in group
   :math:`j`. If :math:`i = j`, we call :math:`\mathbf{P}(i,j)` as the
   **intra-group edge occurrence probability**; if :math:`i \neq j`, we
   call :math:`\mathbf{P}(i,j)` as the **inter-group edge occurrence
   probability**. Note :math:`\mathbf{P}` is not a stochastic matrix
   although it has the word “stochastic” in its name, but we do have
   :math:`\sum_{i}^{}{\mathbf{P}(i,j)} \leq 1`, because
   :math:`1 - \sum_{i}^{}{\mathbf{P}(i,j)}` is the probability that a
   vertex in group :math:`i` has no edge with other nodes.

   For undirected graph, :math:`\mathbf{P}` should be symmetric. In this
   case, the event that “a vertex in group :math:`i` is connected with a
   vertex in group :math:`j`\ ”, whose probability is specified by
   :math:`\mathbf{P}(i,j)`, is identical to the event that “a vertex in
   group :math:`j` is connected with a vertex in group :math:`i`\ ”,
   whose probability is specified by :math:`\mathbf{P}(j,i)`, then
   :math:`\mathbf{P}` should clearly be a symmetric matrix in order for
   the two events to have the same probability.

Assume edge formation between every pair of vertexes are independent.
Then :math:`E` can be viewed as being generalized by this process: for
every pair of vertexes :math:`u,v`, let :math:`\left( u,v \right) \in E`
by probability :math:`\mathbf{P}(z_{u},z_{v})`. Now given
:math:`\mathbf{z}` and :math:`\mathbf{P}`, the likelihood function
:math:`\mathcal{L}\left( \mathbf{z},\mathbf{P} \right)\mathbb{= P}\left( E|\mathbf{z},\mathbf{P} \right) = \prod_{\left( u,v \right) \in E}^{}{\mathbf{P}(z_{u},z_{v})}\prod_{\left( u,v \right) \notin E}^{}\left( 1 - \mathbf{P}\left( z_{u},z_{v} \right) \right)`
will be determined for any :math:`E`, thus and we have finished the
modeling.

Let :math:`C_{i},i = 1,\ldots,k` be the set of all nodes in group
:math:`i` and let :math:`n_{i} = |C_{i}|`. The number of all possible
edges from group :math:`i` to group :math:`j` would be
:math:`n_{i,j} = \left\{ \begin{matrix}
n_{i}n_{j} & i \neq j \\
\begin{pmatrix}
n_{i} \\
2 \\
\end{pmatrix} & i = 1 \\
\end{matrix} \right.\ ` for undirected graph and
:math:`n_{i,j} = \left\{ \begin{matrix}
n_{i}n_{j} & i \neq j \\
2\begin{pmatrix}
n_{i} \\
2 \\
\end{pmatrix} & i = 1 \\
\end{matrix} \right.\ ` for directed graph. Let :math:`e_{i,j}` be the
number of actual edges between group :math:`i` and group :math:`j`. Note
all :math:`n_{i,j}` and :math:`e_{i,j}` are functions of
:math:`\mathbf{z}`. We can in addition let
:math:`\mathbf{E} = \left( e_{i,j} \right),\mathbf{N} = (n_{i,j})` be
two :math:`k \times k` matrices to store above-mentioned values, and let
:math:`\mathbf{n} = (n_{i})` be a vector of length :math:`k`.

Under the independence assumption, the order of edge formations does not
matter, thus we can first form edges between a pair :math:`C_{i},C_{j}`
altogether, and then form edges between another pair, then we have

+-----------------------+-----------------------+-----------------------+
| .. math:: \mathcal{L( |                       | (17‑1)                |
| }\mathbf{z},\mathbf{P |                       |                       |
| }\mathbb{) = P}\left( |                       |                       |
|  E|\mathbf{z},\mathbf |                       |                       |
| {P} \right) = \prod_{ |                       |                       |
| i,j \in \{ 1,\ldots,k |                       |                       |
| \}}^{}{{\mathbf{P}(i, |                       |                       |
| j)}^{e_{i,j}}\left( 1 |                       |                       |
|  - \mathbf{P}(i,j) \r |                       |                       |
| ight)^{n_{i,j} - e_{i |                       |                       |
| ,j}}}                 |                       |                       |
+-----------------------+-----------------------+-----------------------+

Each formation is a Bernoulli experiment governed by probability
:math:`\mathbf{P}\left( i,j \right)`, represented by one factor
:math:`{\mathbf{P}\left( i,j \right)}^{e_{i,j}}\left( 1 - \mathbf{P}\left( i,j \right) \right)^{n_{i,j} - e_{i,j}}`
in the above product, and it is easy to verify that MLE for
:math:`\mathbf{P}\left( i,j \right)` in above :math:`\mathcal{L}` is
:math:`\widehat{\mathbf{P}}\left( i,j \right) = \frac{e_{i,j}}{n_{i,j}}`
by EX 5, which is a function of :math:`\mathbf{z}`, and we can restrict
the original :math:`\mathcal{L}` to
:math:`\mathcal{L(}\mathbf{z},\widehat{\mathbf{P}})`,

+-----------------------+-----------------------+-----------------------+
| .. math:: \mathcal{L} |                       | (17‑2)                |
| \left( \mathbf{z},\wi |                       |                       |
| dehat{\mathbf{P}} \ri |                       |                       |
| ght) = \prod_{i,j \in |                       |                       |
|  \left\{ 1,\ldots,k \ |                       |                       |
| right\}}^{}\left( \fr |                       |                       |
| ac{e_{i,j}}{n_{i,j}}  |                       |                       |
| \right)^{e_{i,j}}\lef |                       |                       |
| t( 1 - \frac{e_{i,j}} |                       |                       |
| {n_{i,j}} \right)^{n_ |                       |                       |
| {i,j} - e_{i,j}} \Rig |                       |                       |
| htarrow \ln\mathcal{L |                       |                       |
| } = \sum_{i,j \in \le |                       |                       |
| ft\{ 1,\ldots,k \righ |                       |                       |
| t\}}^{}{e_{i,j}\left( |                       |                       |
|  \ln e_{i,j} - \ln n_ |                       |                       |
| {i,j} \right) + \left |                       |                       |
| ( n_{i,j} - e_{i,j} \ |                       |                       |
| right)\left( \ln\left |                       |                       |
| ( n_{i,j} - e_{z_{i}, |                       |                       |
| z_{j}} \right) - \ln  |                       |                       |
| n_{i,j} \right)} = \s |                       |                       |
| um_{i,j \in \left\{ 1 |                       |                       |
| ,\ldots,k \right\}}^{ |                       |                       |
| }{e_{i,j}\ln e_{i,j}  |                       |                       |
| - e_{i,j}\ln n_{i,j}  |                       |                       |
| + \left( n_{i,j} - e_ |                       |                       |
| {i,j} \right)\ln\left |                       |                       |
| ( n_{i,j} - e_{i,j} \ |                       |                       |
| right) - n_{i,j}\ln n |                       |                       |
| _{i,j} + e_{i,j}\ln n |                       |                       |
| _{i,j}} = \sum_{i,j \ |                       |                       |
| in \left\{ 1,\ldots,k |                       |                       |
|  \right\}}^{}{e_{i,j} |                       |                       |
| \ln e_{i,j} - n_{i,j} |                       |                       |
| \ln n_{i,j} + \left(  |                       |                       |
| n_{i,j} - e_{i,j} \ri |                       |                       |
| ght)\ln\left( n_{i,j} |                       |                       |
|  - e_{i,j} \right)}   |                       |                       |
+-----------------------+-----------------------+-----------------------+

To solve :math:`\mathbf{z}`, the basic approach is coordinate ascent.
First, we initialize :math:`\mathbf{z},\mathbf{P}` as
:math:`\mathbf{z}^{\left( 0 \right)},\mathbf{P}^{(0)}`, which could be
random or based on some heuristics. Then we will have the first
component of :math:`\mathbf{z}^{(0)}` updated and have
:math:`\mathbf{z}^{(1)}`, which might be equal to
:math:`\mathbf{z}^{(0)}` or differ from :math:`\mathbf{z}^{(0)}` only at
its first component. We then go on to update the 2\ :sup:`nd`,
3\ :sup:`rd`, … component in order and have
:math:`\mathbf{z}^{\left( 2 \right)},\mathbf{z}^{(3)}`,… At the end of
one sweep of updating all components of :math:`\mathbf{z}` and having
:math:`\mathbf{z}^{(|V|)}`, we will then update :math:`\mathbf{P}^{(0)}`
as :math:`\mathbf{P}^{(1)}` based on :math:`\mathbf{z}^{(|V|)}`,
although we will then show updating :math:`\mathbf{P}` is not necessary
except for the end. The entire process will repeat again and again until
there is no significant change in both :math:`\mathbf{P}` and
:math:`\mathbf{z}`, or equivalently there is no significant increase in
:math:`\ln\mathcal{L}`.

Let :math:`s = (t\ \text{mod}\ |V|)`, and let :math:`\mathbf{z}^{(t,r)}`
be derived from replacing the :math:`s`\ th component of
:math:`\mathbf{z}^{(t - 1)}` by :math:`r`. Let :math:`n_{i,j}^{(t,r)}`
and :math:`e_{i,j}^{(t,r)}` be the counts based on
:math:`\mathbf{z}^{(t,r)}`, then the update formula would simply be

+-----------------------+-----------------------+-----------------------+
| .. math:: \mathbf{z}^ |                       | (17‑3)                |
| {(t)} = \operatorname |                       |                       |
| {}{\operatorname{}{\m |                       |                       |
| athcal{L}\left( \math |                       |                       |
| bf{z}^{\left( t,r \ri |                       |                       |
| ght)},\mathbf{P}^{\le |                       |                       |
| ft( \left\lfloor \fra |                       |                       |
| c{t}{\left| V \right| |                       |                       |
| } \right\rfloor \righ |                       |                       |
| t)} \right)}} = \oper |                       |                       |
| atorname{}{\operatorn |                       |                       |
| ame{}\left( \sum_{i,j |                       |                       |
|  \in \left\{ 1,\ldots |                       |                       |
| ,k \right\}}^{}{e_{i, |                       |                       |
| j}^{(t,r)}\ln e_{i,j} |                       |                       |
| ^{(t,r)} - n_{i,j}^{( |                       |                       |
| t,r)}\ln n_{i,j}^{(t, |                       |                       |
| r)} + \left( n_{i,j}^ |                       |                       |
| {(t,r)} - e_{i,j}^{(t |                       |                       |
| ,r)} \right)\ln\left( |                       |                       |
|  n_{i,j}^{(t,r)} - e_ |                       |                       |
| {i,j}^{(t,r)} \right) |                       |                       |
| } \right)}            |                       |                       |
+-----------------------+-----------------------+-----------------------+

Now we want to derive the time complexity for computing above
maximization. Assume we keep track of the values of matrix
:math:`\mathbf{E}` and :math:`\mathbf{N}`, and the group vertex count
vector :math:`\mathbf{n}`, which costs space complexity
:math:`\Theta(2k^{2} + k)`. For simplicity denote
:math:`r^{'} = z_{s}^{(t - 1)}`, and again, we attempt to change the
membership assignment of :math:`v_{s}` from :math:`r^{'}` to
:math:`r \in \{ 1,\ldots,k\}` s.t. the re-assignment maximally increases
:math:`\ln\mathcal{L}`.

We need to re-compute some :math:`e_{i,j}`\ s and :math:`n_{i,j}`\ s in
order to calculate the new :math:`\ln\mathcal{L}`. There is no change in
:math:`\ln\mathcal{L}` when :math:`r^{'} = r`, so we only consider the
case of :math:`r^{'} \neq r`.

1) *The first term*
   :math:`\sum_{i,j \in \left\{ 1,\ldots,k \right\}}^{}{e_{i,j}\ln e_{i,j}}`.
   Recall :math:`e_{i,j}` is the actually number of edges between group
   :math:`i` and group :math:`j`. Let :math:`\mathcal{N}_{o}(v_{s})`
   denote the out-neighborhood of :math:`v_{s}`, i.e.
   :math:`\mathcal{N}_{o}\left( v_{s} \right) = \{ v \in V:\left( v_{s},v \right) \in E\}`,
   and let
   :math:`\mathcal{K}_{o}\left( v_{s} \right) = \{ z_{v}:v \in \mathcal{N}_{o}\left( v_{s} \right)\}`
   be the out-neighboring groups of :math:`v_{s}`. It is easy to see the
   re-assignment only affects :math:`e_{r^{'},j}` and :math:`e_{r,j}`
   s.t. :math:`j \in \mathcal{K}_{o}\left( v_{s} \right)`. For example,
   as shown below, suppose :math:`k = 4` and we change the membership of
   :math:`v_{s}` from :math:`r^{'} = 1` to :math:`r = 2`. If
   :math:`v_{s}` has two outbound edges as shown in the graph, then the
   affected :math:`e_{i,j}` are :math:`e_{1,2},e_{1,3},e_{2,2},e_{2,3}`,
   because group 1 and 4 are neighboring groups of node :math:`v_{s}`.

   |image25|

   *Figure* *17‑1 Illustration of membership re-assignment affects
   cross-group edge counts* :math:`e_{i,j}`\ *.*

   For the same argument, we similarly define the in-neighborhood as
   :math:`\mathcal{N}_{i}\left( v_{s} \right) = \{ v \in V:\left( v,v_{s} \right) \in E\}`
   and in-neighboring groups as
   :math:`\mathcal{K}_{i}\left( v_{s} \right) = \{ z_{v}:v \in \mathcal{N}_{i}\left( v_{s} \right)\}`,
   then the re-assignment only affects :math:`e_{i,r^{'}}` and
   :math:`e_{i,r}` s.t.
   :math:`i \in \mathcal{K}_{i}\left( v_{s} \right)`. In above example,
   the re-assignment affect :math:`e_{1,1},e_{1,2}`. Thus, in total, the
   we need re-computation of
   :math:`2\left| \mathcal{N}_{o}\left( v_{s} \right) \right| + 2\left| \mathcal{N}_{i}\left( v_{s} \right) \right| \leq 2\left( \operatorname{}v_{s} + \operatorname{}v_{s} \right) = 2\deg v_{s}`
   of those :math:`e_{i,j}`\ s and corresponding change in
   :math:`e_{i,j}\ln e_{i,j}`, where :math:`\operatorname{}v_{s}` is the
   out-degree of :math:`v_{s}`, :math:`\operatorname{}v_{s}` is the
   in-degree of :math:`v_{s}`, and
   :math:`\deg v_{s} = \operatorname{}v_{s} + \operatorname{}v_{s}`.

2) *The remaining terms*
   “\ :math:`\sum_{i,j \in \left\{ 1,\ldots,k \right\}}^{}{- n_{i,j}\ln n_{i,j} + \left( n_{i,j} - e_{i,j} \right)\ln\left( n_{i,j} - e_{i,j} \right)}`\ ”.
   For :math:`n_{i,j}`, the re-assignment requires re-computation of all
   :math:`n_{r^{'},j},n_{r,j},n_{i,r^{'}},n_{i,r}` where
   :math:`i,j \in \{ 1,\ldots,k\}`. It is re-computing two rows and two
   columns of matrix :math:`\mathbf{N}`, and it is easy to see there are
   :math:`2k + \left( 2k - 4 \right) = 4k - 4 \leq 4k` times of
   re-computations.

The maximum is computed over :math:`r \in \{ 1,\ldots,k\}`. As a result,
decision on the best :math:`r` for :math:`\mathbf{z}^{(t)}` requires at
most :math:`k\deg v_{s} + 4k^{2}` time complexity, and thus sweep of
coordinate ascent requires at most
:math:`k\sum_{v \in V}^{}{\deg v} + 4k^{2}\left| V \right| = 2k\left| E \right| + 4k^{2}\left| V \right| = O\left( k\left| E \right| + k^{2}\left| V \right| \right)`.
For a big graph, usually we have
:math:`\frac{\left| E \right|}{|V|} \ll k`, i.e. the average graph
degree is far smaller than :math:`k`, then
:math:`k\left| E \right| \ll k^{2}|V|` and
:math:`O\left( k\left| E \right| + k^{2}\left| V \right| \right) \approx O\left( k^{2}\left| V \right| \right)`.

The update of :math:`\mathbf{P}` is not necessary since
:math:`\mathbf{P}` has been represented by :math:`e_{i,j}`\ s and
:math:`n_{i,j}`\ s and is never directly used in updating
:math:`\mathbf{z}`. We only need to update :math:`\mathbf{P}` once based
on :math:`\mathbf{P}\left( i,j \right) = \frac{e_{i,j}}{n_{i,j}}` at the
end of the whole algorithm. Thus, one sweep of above-defined coordinate
ascent takes :math:`O\left( k^{2}|V| \right)` time, which is in practice
a very high running time. If we let :math:`k = \sqrt{|V|}`, we can see
one sweep requires :math:`O(\left| V \right|^{2})` time. If we assume
the convergence at least takes :math:`k` rounds, then this Gibbs
sampling would require at least :math:`O(\left| V \right|^{2.5})` time.

+-----------------------------------------------------------------------+
| For the second term,                                                  |
| :math:`\sum_{i,j \in \left\{ 1,\ldots,k \right\}}^{}{n_{i,j}\ln n_{i, |
| j}}`,                                                                 |
| the time complexity cannot be reduced by expanding it. For directed   |
| graph, we have                                                        |
|                                                                       |
| .. math:: \sum_{i,j \in \left\{ 1,\ldots,k \right\}}^{}{n_{i,j}\ln n_ |
| {i,j}} = \sum_{i = 1}^{k}{n_{i}(n_{i} - 1)\ln{n_{i}(n_{i} - 1)}} + \s |
| um_{i \neq j}^{}{n_{i}n_{j}\ln n_{i}} + \sum_{i \neq j}^{}{n_{i}n_{j} |
| \ln n_{j}}                                                            |
|                                                                       |
| Although we only need to update :math:`n_{r^{'}},n_{r}` to re-compute |
| the above sum, we still need to re-compute                            |
| :math:`2 + 2k - 1 + 2k - 1 = 4k` addends in the sum.                  |
|                                                                       |
| As we can see, the culprit that drags down the time complexity is     |
| :math:`n_{i,j}` (not :math:`\ln n_{i,j}`, which can be decomposed     |
| into :math:`\ln n_{i} + \ln n_{j}`), since the membership             |
| reassignment will always affect :math:`2k - 1` such quantities. In    |
| the `next section <#_The_Poisson_Variant>`__, we will develop a       |
| log-likelihood without this quantity.                                 |
+-----------------------------------------------------------------------+

A random initialization of :math:`\mathbf{z}^{\mathbf{(}0\mathbf{)}}`
takes :math:`O(|V|)`, and computing of :math:`e_{i,j}`\ s and
:math:`n_{i,j}`\ s can be done by iterating through the outbound edges
:math:`\{\left( v,u \right):\left( v,u \right) \in E\}` of each node
:math:`v \in V`, and hence take :math:`O(|E|)` time. Altogether the
initialization takes
:math:`O\left( \max\left( \left| V \right|,\left| E \right| \right) \right) = O(|E|)`
time since usually :math:`\left| V \right| < |E|`. The space complexity
is
:math:`O\left( \max\left\{ 3k^{2},\left| V \right| \right\} \right) = O(|V|)`
for storage of :math:`\mathbf{z},\mathbf{P},e_{i,j},n_{i,j}` since
usually :math:`k \ll \left| V \right|`.

EX 5. Given i.i.d. sample :math:`X_{1},\ldots,X_{n} \in \{ 0,1\}`, show
the MLE for the parameter :math:`p` of a Bernoulli distribution
:math:`\text{Bernoulli}(p)` is
:math:`\widehat{p} = \frac{\sum_{i = 1}^{n}X_{i}}{n}`.

Key. The likelihood function is

.. math:: \mathcal{L}\left( p \right)\mathbb{= P}\left( X_{1},\ldots,X_{n} \middle| p \right) = p^{\sum_{i = 1}^{n}X_{i}}\left( 1 - p \right)^{n - \sum_{i = 1}^{n}X_{i}}

When :math:`p \in (0,1)`, the log-likelihood is

.. math:: \ln\mathcal{L} = \left( \sum_{i = 1}^{n}X_{i} \right)\ln p + \left( n - \sum_{i = 1}^{n}X_{i} \right)\ln{(1 - p)}

Taking derivative of :math:`\ln\mathcal{L}` w.r.t. :math:`p` and we have

.. math:: \frac{\partial\ln\mathcal{L}}{\partial p} = 0 \Rightarrow \frac{\sum_{i = 1}^{n}X_{i}}{p} - \frac{n - \sum_{i = 1}^{n}X_{i}}{1 - p} = 0 \Rightarrow \widehat{p} = \frac{\sum_{i = 1}^{n}X_{i}}{n}

meaning :math:`\mathcal{L}` is a concave function w.r.t. :math:`p`. Thus
:math:`\widehat{p} = \frac{\sum_{i = 1}^{n}X_{i}}{n}` is indeed a
maximum for :math:`p \in (0,1)`. It is easy to verify this holds for the
special cases when :math:`p \in \{ 0,1\}`.

**The Poisson Variant**
^^^^^^^^^^^^^^^^^^^^^^^

We now start developing a slightly different version. In this version,
we allow every two nodes in the graph :math:`G = (V,E)` to have multiple
edges, and we also allow self-loops, i.e. the graph is a **multi-graph**
or **pseudo-graph**. In this case, it is not appropriate to view
:math:`E` as set, but it can be viewed as a
:math:`E:V \times V\mathbb{\rightarrow N}` function, where
:math:`E(u,v)` is the number of edges between :math:`u,v`, and
“\ :math:`(u,v) \in E`\ ” can be viewed as a notation for
:math:`E\left( u,v \right) > 0`, and
“\ :math:`\left( u,v \right) \notin E`\ ” means
:math:`E\left( u,v \right) = 0`, and
:math:`\left| E \right| = \sum_{u,v \in V}^{}{E(u,v)}`.

We keep :math:`\mathbf{z}` as the membership vector, but change the
meaning of stochastic block matrix :math:`\mathbf{P}`. If one node
:math:`u` comes from group :math:`i`, i.e. :math:`\mathbf{z}_{u} = i`,
and the othe node :math:`v` comes from group :math:`j`, i.e.
:math:`\mathbf{z}_{v}\mathbf{=}j`, then the number of edges between
:math:`u,v` obeys Poisson distribution parameterized by
:math:`\mathbf{P}(i,j)`. Recall the Poisson parameter is the mean of the
distribution, then\ :math:`\mathbf{\ }\mathbf{P}(i,j)` is the mean of
number of edges between a node from group :math:`i`, and a node from
group :math:`j`.

In comparison, recall that in the standard model, multi-edges and
self-loops are not allowed, and every model :math:`\mathbf{P}(i,j)` is
the parameter of the Bernoulli experiments on edge formation between the
two groups. In the new setup, the probability of observing :math:`E`
given :math:`\mathbf{z},\mathbf{P}` becomes

.. math:: \mathcal{L}\left( \mathbf{z},\mathbf{P} \right)\mathbb{= P}\left( E|\mathbf{z},\mathbf{P} \right) = \prod_{u,v \in V}^{}{\frac{\left( \mathbf{P}\left( z_{u},z_{v} \right) \right)^{E\left( u,v \right)}}{E\left( u,v \right)!}e^{- \mathbf{P}\left( z_{u},z_{v} \right)}} = \frac{1}{\prod_{u,v \in V}^{}{E\left( u,v \right)!}}\prod_{i,j \in \{ 1,\ldots,k\}}^{}{\left( \mathbf{P}\left( i,j \right) \right)^{e_{i,j}}e^{- n_{i,j}\mathbf{P}(i,j)}}

Here :math:`e_{i,j}` is the actual number of edges between group
:math:`i` and group :math:`j`, just as in the standard model, and
:math:`n_{i,j}` is the maximum possible number of “unique edges” between
group :math:`i` and group :math:`j`, i.e.
:math:`n_{i,j} = \left\{ \begin{matrix}
n_{i}n_{j} & i \neq j \\
n_{i} + \begin{pmatrix}
n_{i} \\
2 \\
\end{pmatrix} & i = j \\
\end{matrix} \right.\ ` for undirected graphs and
:math:`n_{i,j} = n_{i}n_{j}` for directed graphs, where :math:`n_{i}` is
the number of nodes in group :math:`i`. The formula for :math:`n_{i,i}`
is slightly different from the standard model since we now allow
self-loops. Ignore the constant coefficient, and take the logarithm, we
have

.. math:: \ln\mathcal{L} = \sum_{i,j \in \{ 1,\ldots,k\}}^{}{e_{i,j}\ln{\mathbf{P}\left( i,j \right)} - n_{i,j}\mathbf{P}(i,j)}

Recall the meaning of :math:`\mathbf{P}(i,j)` is the Poisson parameter
of the Poisson distribution of the number of edges between a node from
group :math:`i` and a node from group :math:`j`, then by EX 6 the MLE
for :math:`\mathbf{P}(i,j)` is the average number of edges between a
node from group :math:`i` and a node from group :math:`j`, i.e.

.. math:: \widehat{\mathbf{P}}\left( i,j \right) = \frac{e_{i,j}}{n_{i,j}}

which is the same as the standard SBM. Again, just like what we did for
the standard, we can plug this back in :math:`\mathcal{L}` and have

.. math:: \ln\mathcal{L} = \sum_{i,j \in \{ 1,\ldots,k\}}^{}{e_{i,j}\ln\frac{e_{i,j}}{n_{i,j}} - n_{i,j}\frac{e_{i,j}}{n_{i,j}}} = \left( \sum_{i,j \in \left\{ 1,\ldots,k \right\}}^{}{e_{i,j}\ln\frac{e_{i,j}}{n_{i,j}}} \right) - \left( \sum_{i,j \in \left\{ 1,\ldots,k \right\}}^{}e_{i,j} \right)

Note :math:`\sum_{i,j \in \left\{ 1,\ldots,k \right\}}^{}e_{i,j} = |E|`
is the total number of edges in the graph, and hence a constant given
:math:`E`, thus :math:`\ln\mathcal{L}` can be further simplified as

+-----------------------+-----------------------+-----------------------+
| .. math:: \ln\mathcal |                       | (17‑4)                |
| {L} = \sum_{i,j \in \ |                       |                       |
| left\{ 1,\ldots,k \ri |                       |                       |
| ght\}}^{}{e_{i,j}\ln\ |                       |                       |
| frac{e_{i,j}}{n_{i,j} |                       |                       |
| }}                    |                       |                       |
+-----------------------+-----------------------+-----------------------+

This is an even simpler form than the one in standard SBM. The
membership vector :math:`z` can be inferred using the same coordinate
ascent process as `previously
described <file:///F:\Machine%20Learning%2091.docx#SBM_coordiante_ascent>`__,
with less time complexity as shown below. For directed graph
:math:`n_{i,j} = n_{i}n_{j}`

.. math:: \sum_{i,j \in \left\{ 1,\ldots,k \right\}}^{}{e_{i,j}\ln\frac{e_{i,j}}{n_{i,j}}} = \sum_{i,j \in \left\{ 1,\ldots,k \right\}}^{}{e_{i,j}\ln e_{i,j}} - \sum_{i,j \in \left\{ 1,\ldots,k \right\}}^{}{e_{i,j}\ln n_{i}} - \sum_{i,j \in \left\{ 1,\ldots,k \right\}}^{}{e_{i,j}\ln n_{j}} = \sum_{i,j \in \left\{ 1,\ldots,k \right\}}^{}{e_{i,j}\ln e_{i,j}} - \sum_{i = 1}^{k}{\ln n_{i}\sum_{j = 1}^{k}e_{i,j}} - \sum_{j = 1}^{k}{\ln n_{j}\sum_{i = 1}^{k}e_{i,j}} = \sum_{i,j \in \left\{ 1,\ldots,k \right\}}^{}{e_{i,j}\ln e_{i,j}} - \sum_{i = 1}^{k}{e_{i,*}\ln n_{i}} - \sum_{j = 1}^{k}{e_{*,j}\ln n_{j}}

When computing the maximum like, after an attempt of membership
re-assignment of some vertex :math:`v` from group :math:`r'` to group
:math:`r`, the first term along with :math:`e_{i,*},e_{*,j}` (just sums
of :math:`e_{i,j}`) require only at most :math:`\deg v` re-computations
of the addends, as shown
`earlier <file:///F:\Machine%20Learning%2091.docx#SBM_standard_time_complexity>`__;
the second term and the third term only requires updating
:math:`n_{r^{'}},n_{r}`. Recall the culprit dragging down the time
complexity is :math:`n_{i,j}` in the standard model, which is not
present in the above sum.

As a result, the time complexity for one membership re-assignment is at
most :math:`O(k\deg v)` and the time complexity of one sweep of
coordinate ascent is decreased to
:math:`O\left( k\left| E \right| \right)`. For a large graph where
:math:`k \gg \frac{\left| E \right|}{|V|}`, we have
:math:`O\left( k\left| E \right| \right) = O\left( k\frac{\left| E \right|}{\left| V \right|}\left| V \right| \right) \approx O(k\left| V \right|)`.

+-----------------------------------------------------------------------+
| REMARK: Connection to KL-Divergence                                   |
+=======================================================================+
| Given a set of vertexes :math:`V`, suppose we know there are          |
| :math:`\left| E \right|` edges, and the edges are generated in the    |
| following fashion: for the :math:`i`\ th edge, randomly choose one    |
| :math:`\mathcal{U}_{i} \in V` and then randomly choose one            |
| :math:`\mathcal{V}_{i} \in V` where                                   |
| :math:`\mathcal{U}_{i},\mathcal{V}_{i}` are random variables and      |
| :math:`i = 1,\ldots,\left| E \right|`. Since it is pseudo-graph with  |
| self-loops, then each choice of vertex can be assumed to have no      |
| impact on other choices, i.e. all choices of vertexes are mutual      |
| independent, then the generation of these edges are also independent  |
| since mutual independence implies pairwise independence. Check that   |
|                                                                       |
| .. math:: \mathbb{P}\left( \mathcal{U}_{1} = u_{1},\mathcal{V}_{1} =  |
| v_{1},\ldots,\mathcal{U}_{m} = u_{m},\mathcal{V}_{m} = v_{m} \right)  |
| = \prod_{i = 1}^{m}{\mathbb{P}\left( \mathcal{U}_{i} = u_{i} \right)\ |
| mathbb{P}\left( \mathcal{V}_{i} = v_{i} \right)} = \prod_{i = 1}^{m}{ |
| \mathbb{P}\left( \mathcal{U}_{i} = u_{i},\mathcal{V}_{i} = v_{i} \rig |
| ht)} = \left( \frac{1}{\left| V \right|^{2}} \right)^{2}              |
|                                                                       |
| Now randomly draw an edge :math:`\mathcal{(U,V)}` from the generated  |
| graph, it is easy to see                                              |
| :math:`\mathbb{P}\left( \mathcal{U =}u,\mathcal{V =}v \right) = \frac |
| {1}{\left| V \right|^{2}}`                                            |
| due to independence. We know the vertexes and the total number of     |
| edges :math:`\left| E \right|`. When in addition we can observe       |
| membership assignment                                                 |
| :math:`\mathbf{z}\mathbf{= (}z_{1}\mathbf{,\ldots,}z_{|V|}\mathbf{)}` |
| where                                                                 |
| :math:`z_{i} \in \left\{ 1,\ldots,k \right\},\forall i = 1,\ldots,|V| |
| `,                                                                    |
| then we will have the knowledge of                                    |
| :math:`n_{i},\forall i = 1,\ldots,k` and                              |
| :math:`\mathbf{N} = \left( n_{i,j} \right)` where                     |
| :math:`n_{i,j} = \sum_{u,v \in V}^{}\mathbb{I}_{z_{u} = i,z_{v} = j}` |
| is the number of all possible “unique” edges between group :math:`i`  |
| and group :math:`j`. Note we assume the vertex membership             |
| :math:`\mathbf{z}` is independent from edge generation and sampling,  |
| that is                                                               |
|                                                                       |
| .. math::                                                             |
|                                                                       |
|    \left\{ \begin{matrix}                                             |
|    \mathbb{P}\left( E|\mathbf{z} \right)\mathbb{= P(}E) \\            |
|    \mathbb{P}\left( \mathcal{U =}u,\mathcal{V =}v|\mathbf{z} \right)\ |
| mathbb{= P}\left( \mathcal{U =}u,\mathcal{V =}v \right) \\            |
|    \mathbb{P}\left( \mathcal{U =}u,\mathcal{V =}v,E|\mathbf{z} \right |
| )\mathbb{= P}\left( \mathcal{U =}u,\mathcal{V =}v,E \right) \\        |
|    \end{matrix} \right.\                                              |
|                                                                       |
| so then the probability of                                            |
| :math:`\mathcal{U \in}C_{i}\mathcal{,V \in}C_{j}` is                  |
|                                                                       |
| .. math:: \mathbb{P}\left( z_{\mathcal{U}} = i,z_{\mathcal{V}} = j \m |
| iddle| \mathbf{z} \right) = \sum_{u,v \in V}^{}{\mathbb{P}\left( \mat |
| hcal{U =}u,\mathcal{V =}v,z_{u} = i,z_{v} = j \middle| \mathbf{z} \ri |
| ght)} = \sum_{u,v \in V}^{}{\mathbb{P}\left( \mathcal{U =}u,\mathcal{ |
| V =}v|\mathbf{z} \right)\mathbb{I}_{z_{u} = i,z_{v} = j}} = \sum_{u,v |
|  \in V}^{}{\mathbb{P}\left( \mathcal{U =}u,\mathcal{V =}v \right)\mat |
| hbb{I}_{z_{u} = i,z_{v} = j}} = \sum_{u,v \in V}^{}{\frac{1}{\left| V |
|  \right|^{2}}\mathbb{I}_{z_{u} = i,z_{v} = j}} = \frac{n_{i,j}}{\left |
| | V \right|^{2}}                                                      |
|                                                                       |
| If we further observe the entire edge set :math:`E`, and hence have   |
| the knowledge :math:`\mathbf{E} = \left( e_{i,j} \right)` for all     |
| :math:`i,j`, where                                                    |
| :math:`e_{i,j} = \sum_{(u,v) \in E}^{}{E\left( u,v \right)\mathbb{I}_ |
| {z_{u} = i,z_{v} = j}}`                                               |
| is the number of actual edges between group :math:`i` and group       |
| :math:`j`. Note                                                       |
|                                                                       |
| .. math:: \mathbb{P}\left( \mathcal{U =}u,\mathcal{V =}v,z_{u} = i,z_ |
| {v} = j \middle| \mathbf{z},E \right) = 0,\forall\left( u,v \right) \ |
| notin E                                                               |
|                                                                       |
| as well as by the assumption that :math:`\mathbf{z}` is independent   |
| from :math:`E,\mathcal{U,V}`                                          |
| `above <file:///F:\Machine%20Learning%2091.docx#b207>`__              |
|                                                                       |
| .. math:: \mathbb{P}\left( \mathcal{U =}u,\mathcal{V =}v|\mathbf{z},E |
|  \right)\mathbb{= P}\left( \mathcal{U =}u,\mathcal{V =}v|E \right)    |
|                                                                       |
| and use the fact that                                                 |
|                                                                       |
| .. math:: \mathbb{P}\left( \mathcal{U =}u,\mathcal{V =}v|E \right) =  |
| \frac{E\left( u,v \right)}{|E|}                                       |
|                                                                       |
| then                                                                  |
|                                                                       |
| .. math:: \mathbb{P}\left( z_{\mathcal{U}} = i,z_{\mathcal{V}} = j \m |
| iddle| \mathbf{z},E \right) = \sum_{\left( u,v \right) \in E}^{}{\mat |
| hbb{P}\left( \mathcal{U =}u,\mathcal{V =}v,z_{u} = i,z_{v} = j \middl |
| e| \mathbf{z},E \right)} = \sum_{\left( u,v \right) \in E}^{}{\mathbb |
| {P}\left( \mathcal{U =}u,\mathcal{V =}v|\mathbf{z},E \right)\mathbb{I |
| }_{z_{u} = i,z_{v} = j}} = \sum_{\left( u,v \right) \in E}^{}{\mathbb |
| {P}\left( \mathcal{U =}u,\mathcal{V =}v|E \right)\mathbb{I}_{z_{u} =  |
| i,z_{v} = j}} = \sum_{\left( u,v \right) \in E}^{}{\frac{E\left( u,v  |
| \right)}{\left| E \right|}\mathbb{I}_{z_{u} = i,z_{v} = j}} = \frac{e |
| _{i,j}}{\left| E \right|}                                             |
|                                                                       |
| Now rewrite (17‑4) as (adding those denominators is just scaling      |
| :math:`\ln\mathcal{L}`)                                               |
|                                                                       |
| .. math:: \ln\mathcal{L} = \sum_{i,j \in \left\{ 1,\ldots,k \right\}} |
| ^{}{\frac{e_{i,j}}{|E|}\ln\frac{\frac{e_{i,j}}{|E|}}{\frac{n_{i,j}}{\ |
| left| V \right|^{2}}}} = \sum_{i,j \in \left\{ 1,\ldots,k \right\}}^{ |
| }{\mathbb{P}\left( z_{\mathcal{U}} = i,z_{\mathcal{V}} = j \middle| \ |
| mathbf{z},E \right)\ln\frac{\mathbb{P}\left( z_{\mathcal{U}} = i,z_{\ |
| mathcal{V}} = j \middle| \mathbf{z},E \right)}{\mathbb{P}\left( z_{\m |
| athcal{U}} = i,z_{\mathcal{V}} = j \middle| \mathbf{z} \right)}}      |
|                                                                       |
| which is the KL-divergence between two distributions                  |
| :math:`\mathbb{P}\left( z_{\mathcal{U}},z_{\mathcal{V}} \middle| \mat |
| hbf{z},E \right)`                                                     |
| and                                                                   |
| :math:`\mathbb{P}\left( z_{\mathcal{U}},z_{\mathcal{V}} \middle| \mat |
| hbf{z} \right)`.                                                      |
| Recall KL-divergence measures the distance between two distributions, |
| if the distance between                                               |
| :math:`\mathbb{P}\left( z_{\mathcal{U}},z_{\mathcal{V}} \middle| \mat |
| hbf{z},E \right)`                                                     |
| and                                                                   |
| :math:`\mathbb{P}\left( z_{\mathcal{U}},z_{\mathcal{V}} \middle| \mat |
| hbf{z} \right)`                                                       |
| is small, that means observation of :math:`E` does not bring much new |
| information. Thus, the goal of optimizing the MLE over all possible   |
| :math:`\mathbf{z}` is equivalent to maximizing the “information gain” |
| of observing :math:`E` given the above model.                         |
+-----------------------------------------------------------------------+

EX 6. Given i.i.d. sample :math:`X_{1},\ldots,X_{n}\mathbb{\in N}`, show
the MLE for the parameter :math:`\lambda` of a Poisson distribution
:math:`\text{Poisson}(\lambda)` is
:math:`\widehat{\lambda} = \frac{\sum_{i = 1}^{n}X_{i}}{n}`.

Key. The likelihood function is

.. math:: \mathcal{L}\left( \lambda \right)\mathbb{= P}\left( X_{1},\ldots,X_{n} \middle| \lambda \right) = \prod_{i = 1}^{n}{\frac{\lambda^{X_{i}}}{X_{i}!}e^{- \lambda}} = \frac{1}{\prod_{i = 1}^{n}X_{i}}\prod_{i = 1}^{n}{\lambda^{X_{i}}e^{- \lambda}}

Ignore the coefficient :math:`\frac{1}{\prod_{i = 1}^{n}X_{i}}` and take
logarithm of :math:`\mathcal{L}`, we have

.. math:: \ln\mathcal{L} = \sum_{i = 1}^{n}\left( \left( X_{i}\ln\lambda \right) - \lambda \right) = \left( \ln\lambda\sum_{i = 1}^{n}X_{i} \right) - n\lambda

Thus

.. math:: \frac{\partial\ln\mathcal{L}}{\partial\lambda} = \frac{\sum_{i = 1}^{n}X_{i}}{\lambda} - n \Rightarrow \widehat{\lambda} = \frac{\sum_{i = 1}^{n}X_{i}}{n}

Taking second derivative of :math:`\ln\mathcal{L}` w.r.t. :math:`p` and
we have

.. math:: \frac{\partial^{2}\ln\mathcal{L}}{\partial\lambda^{2}} = - \left( \sum_{i = 1}^{n}X_{i} \right)\lambda^{- 2} - n < 0

meaning :math:`\mathcal{L}` is a concave function w.r.t.
:math:`\lambda`. Thus
:math:`\widehat{\lambda} = \frac{\sum_{i = 1}^{n}X_{i}}{n}` is indeed
the MLE for :math:`\text{Poisson}(\lambda)`.

**Degree Corrected SBM**
^^^^^^^^^^^^^^^^^^^^^^^^

We now introduce the degree-corrected SBM. It not only finds an MLE
model for the SBM, moreover it finds a model under which the expected
degrees of vertices are exactly the degrees observed in :math:`E`, and
thus “better fit” (although possibly over-fit) the data.

Randomly draw one edge :math:`\mathcal{(U,V)}` from :math:`E`, then
:math:`U,V` are both random variables ranging over :math:`V`. For
directed graph, we have
:math:`\mathbb{P}\left( \mathcal{U =}u \middle| E \right) = \frac{\operatorname{}u}{|E|}`
and
:math:`\mathbb{P}\left( \mathcal{V =}v \middle| E \right) = \frac{\operatorname{}v}{|E|}`,
where :math:`\operatorname{}u` is the out-degree of :math:`u` and
:math:`\operatorname{}v` is the in-degree of :math:`v`. Recall we use
:math:`C_{i},i = 1,\ldots,k` to denote the set of vertexes in group
:math:`i`. Note
:math:`\sum_{x \in C_{z_{u}}}^{}{\operatorname{}x} = \sum_{j = 1,\ldots,k}^{}e_{z_{u},j}`,
and we further have

.. math:: \mathbb{P}\left( \mathcal{U =}u \middle| z_{\mathcal{U}} = z_{u},E \right) = \frac{\operatorname{}u}{\sum_{x \in C_{z_{u}}}^{}{\operatorname{}x}} = \frac{\operatorname{}u}{\sum_{j = 1,\ldots,k}^{}e_{z_{u},j}}

.. math:: \mathbb{P}\left( \mathcal{V =}v \middle| z_{V} = z_{v},E \right) = \frac{\operatorname{}v}{\sum_{x \in C_{z_{v}}}^{}{\operatorname{}x}} = \frac{\operatorname{}v}{\sum_{i = 1,\ldots,k}^{}e_{i,z_{v}}}

For undirected graph, we do not distinguish start point and end point,
and we
have\ :math:`\mathbb{\text{\ P}}\left( U = v \middle| E \right)\mathbb{= P}\left( V = v \middle| E \right) = \frac{\deg v}{2|E|}`
and

.. math:: \mathbb{P}\left( U = v \middle| z_{U} = z_{v},E \right)\mathbb{= P}\left( V = v \middle| z_{V} = z_{v},E \right) = \frac{\deg v}{\sum_{x \in C_{z_{v}}}^{}{\deg x}} = \frac{\deg v}{\sum_{j = 1,\ldots,k}^{}e_{z_{v},j}}

.. math:: \mathbb{P}\left( U = v \middle| z_{U} = z_{v},E \right)\mathbb{= P}\left( V = v \middle| z_{V} = z_{v},E \right) = \frac{\deg v}{\sum_{x \in C_{z_{v}}}^{}{\deg x}} = \frac{\deg v}{\sum_{j = 1,\ldots,k}^{}e_{z_{v},j}}

For generality, we mainly consider directed graph and the result
immediately extends to undirected graph. For simplicity, denote

.. math:: e_{i, \cdot} = \sum_{j = 1}^{k}e_{i,j} = \sum_{x \in C_{i}}^{}{\operatorname{}x} = \sum_{u \in V}^{}{\left( \operatorname{}u \right)\mathbb{I}_{z_{u} = i}}

.. math:: e_{\cdot ,j} = \sum_{i = 1}^{k}e_{i,j} = \sum_{x \in C_{z_{v}}}^{}{\operatorname{}x} = \sum_{v \in V}^{}{\left( \operatorname{}v \right)\mathbb{I}_{z_{v} = i}}

as the total out-degree of nodes in group :math:`i` and the total
in-degree of nodes in group :math:`i` respectively. Now given two nodes
:math:`u,v`, we assume
:math:`E\left( u,v \right)\sim Poisson(\theta_{u}^{\left( o \right)}\theta_{v}^{(i)}\mathbf{P}(z_{u},z_{v}))`
where

.. math:: \theta_{u}^{\left( o \right)}\mathbb{= P}\left( U = u \middle| z_{U} = z_{u},E \right) = \frac{\operatorname{}u}{e_{z_{u}, \cdot}},\theta_{v}^{(i)}\mathbb{= P}\left( V = v \middle| z_{V} = z_{v},E \right) = \frac{\operatorname{}v}{e_{\cdot ,z_{v}}}

Recall in previous setup,
:math:`\mathbb{E}\left\lbrack E\left( u,v \right) \right\rbrack = \mathbf{P}(z_{u},z_{v})`,
i.e. the expected number of edges between every pair of nodes in
:math:`C_{i} \times C_{j}` will all equal to :math:`\mathbf{P}(i,j)`. In
the new setup, this expectation is not constant across
:math:`C_{i} \times C_{j}` because they are also dependent on
node-specific degree information. Now the likelihood is

.. math:: \mathcal{L}\left( \mathbf{z},\mathbf{P} \right)\mathbb{= P}\left( E|\mathbf{z},\mathbf{P} \right) = \prod_{u,v \in V}^{}{\frac{\left( \theta_{u}^{\left( o \right)}\theta_{v}^{(i)}\mathbf{P}\left( z_{u},z_{v} \right) \right)^{E\left( u,v \right)}}{E\left( u,v \right)!}\exp\left( - \theta_{u}^{\left( o \right)}\theta_{v}^{\left( i \right)}\mathbf{P}\left( z_{u},z_{v} \right) \right)} = \frac{1}{\prod_{u,v \in V}^{}{E\left( u,v \right)!}}\prod_{u \in V}^{}\left( \theta_{u}^{\left( o \right)} \right)^{\operatorname{}u}\prod_{v \in V}^{}\left( \theta_{v}^{\left( i \right)} \right)^{\operatorname{}v}\prod_{i,j \in \{ 1,\ldots,k\}}^{}\left( \mathbf{P}\left( i,j \right) \right)^{e_{i,j}}\prod_{u,v \in V}^{}{\exp\left( - \theta_{u}^{\left( o \right)}\theta_{v}^{\left( i \right)}\mathbf{P}\left( z_{u},z_{v} \right) \right)}

As before, dropping the coefficient and take logarithm, and we have

.. math:: \ln\mathcal{L} = \sum_{u \in V}^{}{\left( \operatorname{}u \right)\ln\theta_{u}^{\left( o \right)}} + \sum_{v \in V}^{}{\left( \operatorname{}v \right)\ln\theta_{v}^{\left( i \right)}} + \sum_{i,j \in \left\{ 1,\ldots,k \right\}}^{}\left( e_{i,j}\ln{\mathbf{P}\left( i,j \right)} - \mathbf{P}(i,j) \right)

where

.. math:: \ln{\prod_{u,v \in V}^{}e^{- \theta_{u}^{\left( o \right)}\theta_{v}^{(i)}\mathbf{P}\left( z_{u},z_{v} \right)}} = - \sum_{u,v \in V}^{}{\theta_{u}^{\left( o \right)}\theta_{v}^{\left( i \right)}\mathbf{P}\left( z_{u},z_{v} \right)} = - \sum_{i,j \in \left\{ 1,\ldots,k \right\}}^{}{\sum_{u \in C_{i}}^{}{\sum_{v \in C_{j}}^{}{\theta_{u}^{\left( o \right)}\theta_{v}^{\left( i \right)}\mathbf{P}\left( i,j \right)}}} = - \sum_{i,j \in \left\{ 1,\ldots,k \right\}}^{}{\mathbf{P}\left( i,j \right)\sum_{u \in C_{i}}^{}\theta_{u}^{\left( o \right)}\sum_{v \in C_{j}}^{}\theta_{v}^{\left( i \right)}} = - \sum_{i,j \in \left\{ 1,\ldots,k \right\}}^{}{\mathbf{P}\left( i,j \right)}

since
:math:`\sum_{u \in C_{i}}^{}\theta_{u}^{\left( o \right)} = \sum_{u \in C_{i}}^{}\frac{\operatorname{}u}{\sum_{x \in C_{i}}^{}{\operatorname{}x}} = 1`
and :math:`\sum_{v \in C_{j}}^{}\theta_{v}^{\left( i \right)} = 1` as
well. Now it is easy to see

.. math::

   \left\{ \begin{matrix}
   \frac{\partial\ln\mathcal{L}}{\partial\mathbf{P}(i,j)} = 0 \Rightarrow \frac{e_{i,j}}{\mathbf{P}\left( i,j \right)} = 1 \\
   \frac{\partial^{2}\ln\mathcal{L}}{\partial\left( \mathbf{P}\left( i,j \right) \right)^{2}} = - e_{i,j}\mathbf{P}^{- 2}\left( i,j \right) < 0 \\
   \end{matrix} \right.\  \Rightarrow \widehat{\mathbf{P}}\left( i,j \right) = e_{i,j}

Plug this back into :math:`\ln\mathcal{L}` and dropped the last constant
term :math:`\sum_{i,j \in \left\{ 1,\ldots,k \right\}}^{}e_{i,j} = |E|`,
then

.. math:: \ln\mathcal{L} = \sum_{u \in V}^{}{\left( \operatorname{}u \right)\ln\theta_{u}^{\left( o \right)}} + \sum_{v \in V}^{}{\left( \operatorname{}v \right)\ln\theta_{v}^{\left( i \right)}} + \sum_{i,j \in \left\{ 1,\ldots,k \right\}}^{}{e_{i,j}\ln e_{i,j}}

We now legitimize our above choice of

.. math:: \theta_{u}^{\left( o \right)} = \frac{\operatorname{}u}{\sum_{x \in C_{z_{u}}}^{}{\operatorname{}x}},\theta_{v}^{(i)} = \frac{\operatorname{}v}{\sum_{x \in C_{z_{v}}}^{}{\operatorname{}x}}

by showing :math:`\theta_{u}^{\left( o \right)},\theta_{v}^{(i)}` are
actually the MLE of :math:`\mathcal{L}` with restriction
:math:`\sum_{u \in C_{i}}^{}\theta_{u}^{\left( o \right)} = 1,\forall i = 1,\ldots,k`
and
:math:`\sum_{v \in C_{j}}^{}\theta_{v}^{\left( i \right)} = 1,\forall j = 1,\ldots,k`.
To see this, for every :math:`i \in 1,\ldots,k`, using Lagrange
multiplier :math:`\lambda` for the summation over :math:`u \in C_{i}`
and then taking derivative w.r.t. :math:`\theta_{u}^{\left( o \right)}`
for every :math:`u \in C_{i}`, we have the following,

.. math:: \sum_{u \in C_{i}}^{}{\left( \operatorname{}u \right)\ln\theta_{u}^{\left( o \right)}} = \lambda\left( \sum_{u \in C_{i}}^{}\theta_{u}^{\left( o \right)} - 1 \right) \Rightarrow \frac{\operatorname{}u}{\theta_{u}^{\left( o \right)}} = \lambda,\forall u \in C_{i} \Rightarrow \sum_{u \in C_{z_{u}}}^{}{\operatorname{}u} = \lambda\sum_{u \in C_{z_{u}}}^{}\theta_{u}^{\left( o \right)} = \lambda \Rightarrow \frac{\operatorname{}u}{\theta_{u}^{\left( o \right)}} = \sum_{u \in C_{z_{u}}}^{}{\operatorname{}u} \Rightarrow \widehat{\theta_{u}^{\left( o \right)}} = \frac{\operatorname{}u}{\sum_{u \in C_{z_{u}}}^{}{\operatorname{}u}}

Proof for
:math:`\widehat{\theta_{v}^{\left( i \right)}} = \frac{\operatorname{}v}{\sum_{x \in C_{z_{v}}}^{}{\operatorname{}x}}`
is the same. Now we do further simplification as the following, note
:math:`e_{i, \cdot} = \sum_{u \in V}^{}{\left( \operatorname{}u \right)\mathbb{I}_{z_{u} = i}}`
as well

.. math:: \sum_{u \in V}^{}{\left( \operatorname{}u \right)\ln\theta_{u}^{\left( o \right)}} = \sum_{u \in V}^{}{\left( \operatorname{}u \right)\ln\frac{\operatorname{}u}{e_{z_{u}, \cdot}}} = \sum_{u \in V}^{}{\left( \operatorname{}u \right)\ln{\operatorname{}u}} - \sum_{u \in V}^{}{\left( \operatorname{}u \right)\left( \ln e_{z_{u}, \cdot} \right)} = \sum_{u \in V}^{}{\left( \operatorname{}u \right)\ln{\operatorname{}u}} - \sum_{u \in V}^{}{\left( \operatorname{}u \right)\sum_{i = 1}^{k}{\mathbb{I}_{z_{u} = i}\left( \ln e_{i, \cdot} \right)}} = \sum_{u \in V}^{}{\left( \operatorname{}u \right)\ln{\operatorname{}u}} - \sum_{u \in V}^{}{\sum_{i = 1}^{k}{\left( \operatorname{}u \right)\mathbb{I}_{z_{u} = i}\ln e_{i, \cdot}}} = \sum_{u \in V}^{}{\left( \operatorname{}u \right)\ln{\operatorname{}u}} - \sum_{i = 1}^{k}\left( \ln e_{i, \cdot}\sum_{u \in V}^{}{\left( \operatorname{}u \right)\mathbb{I}_{z_{u} = i}} \right) = \sum_{u \in V}^{}{\left( \operatorname{}u \right)\ln{\operatorname{}u}} - \sum_{i = 1}^{k}{e_{i, \cdot}\ln e_{i, \cdot}}

Similarly,

.. math:: \sum_{v \in V}^{}{\left( \operatorname{}v \right)\ln\theta_{v}^{\left( i \right)}} = \sum_{v \in V}^{}{\left( \operatorname{}v \right)\ln{\operatorname{}v}} - \sum_{v \in V}^{}{\left( \operatorname{}v \right)\left( \ln e_{\cdot ,z_{v}} \right)} = \sum_{v \in V}^{}{\left( \operatorname{}v \right)\ln{\operatorname{}v}} - \sum_{j = 1}^{k}{e_{\cdot ,j}\ln e_{\cdot ,j}}

As a result

.. math:: \sum_{u \in V}^{}{\left( \operatorname{}u \right)\ln\theta_{u}^{\left( o \right)}} + \sum_{v \in V}^{}{\left( \operatorname{}v \right)\ln\theta_{v}^{\left( i \right)}} = \sum_{u \in V}^{}{\left( \operatorname{}u \right)\ln{\operatorname{}u}} + \sum_{v \in V}^{}{\left( \operatorname{}v \right)\ln{\operatorname{}v}} - \sum_{i = 1}^{k}{e_{i, \cdot}\ln e_{i, \cdot}} - \sum_{j = 1}^{k}{e_{\cdot ,j}\ln e_{\cdot ,j}} = \left( \sum_{v \in V}^{}{\left( \operatorname{}v \right)\ln{\operatorname{}v} + \left( \operatorname{}v \right)\ln{\operatorname{}v}} \right) - \sum_{i = 1}^{k}{e_{i, \cdot}\ln e_{i, \cdot}} - \sum_{j = 1}^{k}{e_{\cdot ,j}\ln e_{\cdot ,j}}

Plug above back into :math:`\ln\mathcal{L}` and note the first term of
above equation is a constant given the edge set :math:`E`, so it can be
dropped, then

.. math:: \ln\mathcal{L} = \sum_{i,j \in \left\{ 1,\ldots,k \right\}}^{}{e_{i,j}\ln e_{i,j}} - \sum_{i = 1}^{k}{e_{i, \cdot}\ln e_{i, \cdot}} - \sum_{j = 1}^{k}{e_{\cdot ,j}\ln e_{\cdot ,j}}

Last, observe that

.. math:: \sum_{i = 1}^{k}{e_{i, \cdot}\ln e_{i, \cdot}} = \sum_{i = 1}^{k}\left( \left( \sum_{j = 1}^{k}e_{i,j} \right)\ln e_{i, \cdot} \right) = \sum_{i,j \in \left\{ 1,\ldots,k \right\}}^{}{e_{i,j}\ln e_{i, \cdot}}

and similarly,
:math:`\sum_{j = 1}^{k}{e_{\cdot ,j}\ln e_{\cdot ,j}} = \sum_{i,j \in \left\{ 1,\ldots,k \right\}}^{}{e_{i,j}\ln e_{\cdot ,j}}`,
then finally

.. math:: \ln\mathcal{L} = \sum_{i,j \in \left\{ 1,\ldots,k \right\}}^{}{e_{i,j}\ln e_{i,j}} - \sum_{i,j \in \left\{ 1,\ldots,k \right\}}^{}{e_{i,j}\ln e_{i,*}} - \sum_{i,j \in \left\{ 1,\ldots,k \right\}}^{}{e_{i,j}\ln e_{*,j}} = \sum_{i,j \in \left\{ 1,\ldots,k \right\}}^{}{e_{i,j}\ln e_{i,j}} - \sum_{i,j \in \left\{ 1,\ldots,k \right\}}^{}{e_{i,j}\ln{e_{i,*}e_{*,j}}} = \sum_{i,j \in \left\{ 1,\ldots,k \right\}}^{}{e_{i,j}\ln\frac{e_{i,j}}{e_{i,*}e_{*,j}}}

That is our final log-likelihood function has a very simple form. In
comparison with (17‑4) the only difference is the numerator of the
logarithm “\ :math:`n_{i,j}`\ ” is replaced by
“\ :math:`e_{i,*}e_{*,j}`\ ”.

+-----------------------+-----------------------+-----------------------+
| .. math:: \ln\mathcal |                       | (17‑5)                |
| {L} = \sum_{i,j \in \ |                       |                       |
| left\{ 1,\ldots,k \ri |                       |                       |
| ght\}}^{}{e_{i,j}\ln\ |                       |                       |
| frac{e_{i,j}}{e_{i,*} |                       |                       |
| e_{*,j}}}             |                       |                       |
+-----------------------+-----------------------+-----------------------+

+-----------------------------------------------------------------------+
| This equation also has a KL-divergence interpretation (see earlier    |
| `remark <file:///F:\Machine%20Learning%2091.docx#REMARK_Connection_to |
| _KL_Divergence>`__).                                                  |
| Suppose we cannot observe :math:`E` but instead degrees of each node, |
| denoted by :math:`D`. Then given the fact that                        |
|                                                                       |
| .. math:: \mathbb{P}\left( \mathcal{U =}u,\mathcal{V =}v|D \right) =  |
| \frac{\operatorname{}u}{\sum_{x \in V}^{}{\operatorname{}x}}\frac{\op |
| eratorname{}v}{\sum_{x \in V}^{}{\operatorname{}x}}                   |
|                                                                       |
| Then                                                                  |
|                                                                       |
| .. math:: \mathbb{P}\left( z_{\mathcal{U}} = i,z_{\mathcal{V}} = j \m |
| iddle| \mathbf{z},D \right) = \sum_{u,v \in V}^{}{\mathbb{P}\left( \m |
| athcal{U =}u,\mathcal{V =}v,z_{u} = i,z_{v} = j \middle| \mathbf{z},D |
|  \right)} = \sum_{u,v \in V}^{}{\mathbb{P}\left( \mathcal{U =}u,\math |
| cal{V =}v|D \right)\mathbb{I}_{z_{u} = i,z_{v} = j}} = \sum_{u \in C_ |
| {i},v \in C_{j}}^{}{\frac{\operatorname{}u}{\sum_{x \in V}^{}{\operat |
| orname{}x}}\frac{\operatorname{}v}{\sum_{x \in V}^{}{\operatorname{}x |
| }}} = \sum_{u \in C_{i}}^{}\left( \frac{\operatorname{}u}{\sum_{x \in |
|  V}^{}{\operatorname{}x}}\sum_{v \in C_{j}}^{}\frac{\operatorname{}v} |
| {\sum_{x \in V}^{}{\operatorname{}x}} \right) = \frac{e_{i,*}}{\sum_{ |
| x \in V}^{}{\operatorname{}x}}\frac{e_{*,j}}{\sum_{x \in V}^{}{\opera |
| torname{}x}}                                                          |
|                                                                       |
| Thus, we can re-write (17‑5) as                                       |
|                                                                       |
| .. math:: \ln\mathcal{L} = \sum_{i,j \in \left\{ 1,\ldots,k \right\}} |
| ^{}{\frac{e_{i,j}}{|E|}\ln\frac{\frac{e_{i,j}}{\left| E \right|}}{\fr |
| ac{e_{i,*}}{\sum_{x \in V}^{}{\operatorname{}x}}\frac{e_{*,j}}{\sum_{ |
| x \in V}^{}{\operatorname{}x}}}} = \sum_{i,j \in \left\{ 1,\ldots,k \ |
| right\}}^{}{\mathbb{P}\left( z_{\mathcal{U}} = i,z_{\mathcal{V}} = j  |
| \middle| \mathbf{z},E \right)\ln\frac{\mathbb{P}\left( z_{\mathcal{U} |
| } = i,z_{\mathcal{V}} = j \middle| \mathbf{z},E \right)}{\mathbb{P}\l |
| eft( z_{\mathcal{U}} = i,z_{\mathcal{V}} = j \middle| \mathbf{z},D \r |
| ight)}}                                                               |
|                                                                       |
| Now the degree-corrected model is maximizing the “information gain”   |
| of observing the whole edge set :math:`E` over observing just the     |
| degree information :math:`D`.                                         |
+-----------------------------------------------------------------------+

-  **Property** **17‑1** The expected number of edges between group
   :math:`i` and group :math:`j` for graphs generated by the
   degree-corrected Poisson SBM is
   :math:`\mathbb{E}e_{i,j}\mathbf{= P}(i,j)`, since

.. math:: \mathbb{E}e_{i,j} = \sum_{u \in C_{i},v \in C_{j}}^{}{\mathbb{E\lbrack}E(u,v)\rbrack} = \sum_{u \in C_{i},v \in C_{j}}^{}{\theta_{u}^{\left( o \right)}\theta_{v}^{\left( i \right)}\mathbf{P}(i,j)} = \mathbf{P}(i,j)

-  **Property** **17‑2** The expected out-degree or in-degree of a
   vertex :math:`v` for graphs generated by the degree-corrected Poisson
   SBM is
   :math:`\mathbb{E}\left\lbrack \operatorname{}v \right\rbrack = \theta_{u}^{\left( o \right)}\sum_{j = 1,\ldots,k}^{}{\mathbf{P}(z_{u},j)}`
   and
   :math:`\mathbb{E}\left\lbrack \operatorname{}v \right\rbrack = \theta_{v}^{\left( i \right)}\sum_{j = 1,\ldots,k}^{}{\mathbf{P}(i,z_{v})}`,
   since

.. math:: \mathbb{E}\left\lbrack \operatorname{}u \right\rbrack = \sum_{v \in V}^{}{\theta_{u}^{\left( o \right)}\theta_{v}^{\left( i \right)}\mathbf{P}(z_{u},z_{v})} = \theta_{u}^{\left( o \right)}\sum_{j = 1,\ldots,k}^{}{\sum_{v \in C_{i}}^{}{\theta_{v}^{\left( i \right)}\mathbf{P}(z_{u},j)}} = \theta_{u}^{\left( o \right)}\sum_{j = 1,\ldots,k}^{}{\mathbf{P}(z_{u},j)}

.. math:: \mathbb{E}\left\lbrack \operatorname{}v \right\rbrack = \sum_{u \in V}^{}{\theta_{u}^{\left( o \right)}\theta_{v}^{\left( i \right)}\mathbf{P}(z_{u},z_{v})} = \theta_{v}^{\left( i \right)}\sum_{j = 1,\ldots,k}^{}{\sum_{u \in C_{i}}^{}{\theta_{u}^{\left( o \right)}\mathbf{P}(i,z_{v})}} = \theta_{v}^{\left( i \right)}\sum_{j = 1,\ldots,k}^{}{\mathbf{P}(i,z_{v})}

As a special case, for the MLE model we have
:math:`\mathbb{E}\left\lbrack \operatorname{}u \right\rbrack = \operatorname{}{\lbrack u|E\rbrack}\mathbb{,E}\left\lbrack \operatorname{}v \right\rbrack = \operatorname{}{\lbrack v|E\rbrack}`,

.. math:: \mathbb{E}\left\lbrack \operatorname{}u \right\rbrack = \frac{\operatorname{}{\lbrack u|E\rbrack}}{\sum_{u \in C_{z_{u}}}^{}{\operatorname{}{\lbrack u|E\rbrack}}}\sum_{j = 1,\ldots,k}^{}{\lbrack e_{z_{u},j}|E\rbrack} = \operatorname{}{\lbrack u|E\rbrack}

.. math:: \mathbb{E}\left\lbrack \operatorname{}v \right\rbrack = \frac{\operatorname{}{\lbrack v|E\rbrack}}{\sum_{x \in C_{z_{v}}}^{}{\operatorname{}{\lbrack x|E\rbrack}}}\sum_{i = 1,\ldots,k}^{}{\lbrack e_{i,z_{v}}|E\rbrack} = \operatorname{}{\lbrack v|E\rbrack}

where we use :math:`\lbrack \cdot |E\rbrack` to denote an object or
quantity from the observation of :math:`E`. Thus, the degree corrected
model not only just finds an MLE model, moreover it finds a model under
which the expected degrees are exactly the degrees observed in
:math:`E`.

+-----------------------------------------------------------------------+
| INTUITION                                                             |
+=======================================================================+
| The main purpose of degree correction is to prevent “weakly           |
| connected” vertexes of large degrees to be clustered together. For    |
| real-world networks, in most situations, such weakly connected        |
| large-degree nodes is intuitively more likely to be placed in         |
| different groups. For example, in social network, avid supporters of  |
| two different political opinions might occasionally exchange some     |
| words, but they still belong to two groups.                           |
|                                                                       |
| Consider the standard models without degree correction, where the     |
| expected number of edges between two groups are completely controlled |
| by :math:`\mathbf{P}` when :math:`\mathbf{z}` is fixed. Suppose we    |
| are currently reassigning the membership of a high-degree node        |
| :math:`u`, and :math:`u` has an edge :math:`(u,v)` where :math:`v` is |
| also a high-degree node. Assignment of some :math:`z_{u} \neq z_{v}`  |
| will have high impact on :math:`\mathbf{P}(z_{u}, \cdot )` and        |
| :math:`\mathbf{P}( \cdot ,z_{u})` since it introduces many edges to   |
| the group :math:`C_{z_{u}}`. This would sometimes make the            |
| observations of other edges from or into :math:`C_{z_{u}}` become     |
| less likely, and therefore the maximization might suggest             |
| :math:`z_{u} = z_{v}`. Once the high-degree node :math:`u` is         |
| assigned to :math:`z_{v}`, :math:`\mathbf{P}(z_{v}, \cdot )` and      |
| :math:`\mathbf{P}( \cdot ,z_{v})` will be adjusted to be more         |
| accommodating to large-degree nodes (e.g. many numbers in             |
| both\ :math:`\mathbf{\text{\ P}}(z_{v}, \cdot )` and                  |
| :math:`\mathbf{P}( \cdot ,z_{v})` will increase), and hence absorb    |
| even more large-degree nodes in the future and expels low-degree      |
| nodes from the group. As a result, high-degree nodes will form a      |
| group itself in the long run.                                         |
|                                                                       |
| In the degree-corrected version, it is the same that assigning        |
| :math:`u` to :math:`z_{u} \neq z_{v}` will introduce many             |
| :math:`(u,w)` or :math:`(w,u)` edges to group :math:`z_{u}`, but the  |
| additional relatively large factor                                    |
| “\ :math:`\theta_{u}^{\left( \text{out} \right)}`\ ” in               |
| “\ :math:`\theta_{u}^{\left( \text{out} \right)}\theta_{w}^{\left( \t |
| ext{in} \right)}\mathbf{P}\left( z_{u},z_{w} \right)`\ ”              |
| and “\ :math:`\theta_{u}^{\left( \text{out} \right)}`\ ” in           |
| “\ :math:`\theta_{w}^{\left( \text{out} \right)}\theta_{\text{in}}^{\ |
| left( \text{in} \right)}\mathbf{P}\left( z_{w},z_{u} \right)`\ ”      |
| mitigate the impact of :math:`u` and hence the adjustment of          |
| :math:`\mathbf{P}\left( z_{u}, \cdot \right)` and                     |
| :math:`\mathbf{P}\left( \cdot ,z_{u} \right)` will be less            |
| significant, which in turn has less influence on other edges, and     |
| hence “\ :math:`z_{u} \neq z_{v}`\ ” becomes more likely in           |
| comparison to the standard models. Even if the maximization decides   |
| :math:`z_{u} = z_{v}`, similarly due to the relatively large factor   |
| “\ :math:`\theta_{u}^{\left( \text{out} \right)}\theta_{v}^{\left( \t |
| ext{in} \right)}`\ ”                                                  |
| in                                                                    |
| “\ :math:`\theta_{u}^{\left( \text{out} \right)}\theta_{v}^{\left( \t |
| ext{in} \right)}\mathbf{P}\left( z_{u},z_{v} \right)`\ ”,             |
| the change of :math:`\mathbf{P}\left( z_{v}, \cdot \right)` and       |
| :math:`\mathbf{P}\left( \cdot ,z_{v} \right)` will also be less       |
| significant.                                                          |
|                                                                       |
| For a concrete example, consider                                      |
+-----------------------------------------------------------------------+

**The Entropy Interpretation of SBM**
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

We start with simple graphs where no multiple edges and self-loops are
allowed. Let :math:`\Omega_{i,j}` denote the set of all possible edge
sets between group :math:`i` and group :math:`j`, and let :math:`\Omega`
denote the sample space of the SBM model, i.e. the set of all different
possible :math:`E`, then use :math:`\Omega|X` denote the subspace
conditioned on :math:`X`.

Given membership :math:`\mathbf{z}` and
:math:`\mathbf{E} = \left( e_{i,j} \right)`, the number of all possible
edges sets between group :math:`i` and group :math:`j` is denoted as
:math:`\mathcal{E}_{i,j} = |\Omega_{i,j}| = \begin{pmatrix}
n_{i,j} \\
e_{i,j} \\
\end{pmatrix}`, and the number of different graphs conditioned by
:math:`\mathbf{z},\mathbf{E}` is denoted as

.. math:: \mathcal{E =}\left| \left( \Omega \middle| \mathbf{z},\mathbf{E} \right) \right| = \prod_{i,j \in \{ 1,\ldots,k\}}^{}\mathcal{E}_{i,j}

Check the standard model and (17‑1) again and verify that every graph in
:math:`\Omega|\mathbf{z},\mathbf{E}` has equal probability
:math:`\mathbb{P}\left( E \middle| \mathbf{z},\mathbf{E} \right) = \prod_{i,j \in \{ 1,\ldots,k\}}^{}{{\mathbf{P}(i,j)}^{e_{i,j}}\left( 1 - \mathbf{P}(i,j) \right)^{n_{i,j} - e_{i,j}}}`
regardless of the values of :math:`\mathbf{P}`.

-  **Lemma** **17‑1** :math:`\ln\begin{pmatrix}
   n \\
   m \\
   \end{pmatrix} = nH\left( \frac{m}{n} \right) + O(n)` as
   :math:`m \rightarrow \infty,n \rightarrow \infty,\frac{m}{n} \rightarrow 0`,
   where :math:`H` is the binary entropy function
   :math:`H\left( x \right) = - x\ln x - \left( 1 - x \right)\ln{(1 - x)}`.
   It is not hard to verify
   :math:`\frac{m}{n} \rightarrow 0 \Longleftrightarrow n - m \rightarrow \infty`
   through :math:`\epsilon`-argument, then by Stirling’s formula,

.. math:: \ln\left( \frac{n}{m} \right)\sim\ln\frac{\sqrt{2\pi n}\left( \frac{n}{e} \right)^{n}}{\sqrt{2\pi m}\left( \frac{m}{e} \right)^{m}\sqrt{2\pi\left( n - m \right)}\left( \frac{n - m}{e} \right)^{n - m}}\  = \ln\left( \sqrt{\frac{n}{2\pi m\left( n - m \right)}}\frac{n^{n}e^{m}e^{n - m}}{m^{m}e^{n}\left( n - m \right)^{n - m}} \right) = \frac{1}{2}\left( \ln n - \ln m - \ln\left( n - m \right) - \ln{2\pi} \right) + n\ln n - m\ln m - \left( n - m \right)\ln{(n - m)}

where

.. math:: \frac{1}{2}\left( \ln n - \ln m - \ln\left( n - m \right) - \ln{2\pi} \right) = O(n)

and

.. math:: n\ln n - m\ln m - \left( n - m \right)\ln\left( n - m \right) = - m\ln m + m\ln n + n\ln n - m\ln n - \left( n - m \right)\ln\left( n - m \right) = - m\ln\frac{m}{n} - \left( n - m \right)\ln\frac{n - m}{n} = nH\left( \frac{m}{n} \right)

Recall that given a finite sample space :math:`\Omega`, if every element
in :math:`\Omega` has the same probability to occur w.r.t. some
probability measure :math:`\mathbb{P}`, then the entropy of
:math:`\mathbb{P}` is simply :math:`\ln{|\Omega|}`. As before, let
:math:`\mathbb{P}` measures the probability of the edge sets, then the
entropy of :math:`\mathbb{P}` conditioned on
:math:`\mathbf{z},\mathbf{E}`, denoted by
:math:`\mathbb{H}\left\lbrack \mathbb{P} \middle| \mathbf{z},\mathbf{E} \right\rbrack`,
is simply :math:`\ln\mathcal{E}` since every edge set in
:math:`\Omega|\mathbf{z},\mathbf{E}` has the same probability to occur,
i.e.

+-----------------------+-----------------------+-----------------------+
| .. math::             |                       | (17‑6)                |
|                       |                       |                       |
|    \mathbb{H}\left\lb |                       |                       |
| rack \mathbb{P} \midd |                       |                       |
| le| \mathbf{z},\mathb |                       |                       |
| f{E} \right\rbrack =  |                       |                       |
| \sum_{i,j \in \{ 1,\l |                       |                       |
| dots,k\}}^{}{\ln\math |                       |                       |
| cal{E}_{i,j}} = \sum_ |                       |                       |
| {i,j \in \{ 1,\ldots, |                       |                       |
| k\}}^{}{\ln\begin{pma |                       |                       |
| trix}                 |                       |                       |
|    n_{i,j} \\         |                       |                       |
|    e_{i,j} \\         |                       |                       |
|    \end{pmatrix}} \ap |                       |                       |
| prox \sum_{i,j \in \l |                       |                       |
| eft\{ 1,\ldots,k \rig |                       |                       |
| ht\}}^{}{n_{i,j}H\lef |                       |                       |
| t( \frac{e_{i,j}}{n_{ |                       |                       |
| i,j}} \right)} = \sum |                       |                       |
| _{i,j \in \left\{ 1,\ |                       |                       |
| ldots,k \right\}}^{}{ |                       |                       |
| - e_{i,j}\ln\frac{e_{ |                       |                       |
| i,j}}{n_{i,j}} - \lef |                       |                       |
| t( n_{i,j} - e_{i,j}  |                       |                       |
| \right)\ln\frac{n_{i, |                       |                       |
| j} - e_{i,j}}{n_{i,j} |                       |                       |
| }} = \sum_{i,j \in \l |                       |                       |
| eft\{ 1,\ldots,k \rig |                       |                       |
| ht\}}^{}{- e_{i,j}\le |                       |                       |
| ft( \ln e_{i,j} - \ln |                       |                       |
|  n_{i,j} \right) - \l |                       |                       |
| eft( n_{i,j} - e_{i,j |                       |                       |
| } \right)\ln\left( n_ |                       |                       |
| {i,j} - e_{i,j} \righ |                       |                       |
| t)} + \left( n_{i,j}  |                       |                       |
| - e_{i,j} \right)\ln  |                       |                       |
| n_{i,j} = \sum_{i,j \ |                       |                       |
| in \left\{ 1,\ldots,k |                       |                       |
|  \right\}}^{}{- e_{i, |                       |                       |
| j}\ln e_{i,j} + e_{i, |                       |                       |
| j}\ln n_{i,j} - \left |                       |                       |
| ( n_{i,j} - e_{i,j} \ |                       |                       |
| right)\ln\left( n_{i, |                       |                       |
| j} - e_{i,j} \right)} |                       |                       |
|  + n_{i,j}\ln n_{i,j} |                       |                       |
|  - e_{i,j}\ln n_{i,j} |                       |                       |
|  = \sum_{i,j \in \lef |                       |                       |
| t\{ 1,\ldots,k \right |                       |                       |
| \}}^{}{- e_{i,j}\ln e |                       |                       |
| _{i,j} + n_{i,j}\ln n |                       |                       |
| _{i,j} - \left( n_{i, |                       |                       |
| j} - e_{i,j} \right)\ |                       |                       |
| ln\left( n_{i,j} - e_ |                       |                       |
| {i,j} \right)}        |                       |                       |
+-----------------------+-----------------------+-----------------------+

where we *assume*

+-----------------------+-----------------------+-----------------------+
| .. math:: n_{i,j} \ri |                       | (17‑7)                |
| ghtarrow \infty,e_{i, |                       |                       |
| j} \rightarrow \infty |                       |                       |
| ,\frac{e_{i,j}}{n_{i, |                       |                       |
| j}} \rightarrow 0,\fo |                       |                       |
| rall i,j \in \{ 1,\ld |                       |                       |
| ots,k\}               |                       |                       |
+-----------------------+-----------------------+-----------------------+

This assumption is very important as all approximations discussed here
are based on this. Equation (17‑6) is exactly the log-likelihood of the
standard SBM we derived in (17‑2) except for the sign, and thus
maximizing (17‑2) is approximately equivalent to minimizing
:math:`\mathbb{H}\left\lbrack \mathbb{P} \middle| \mathbf{z},\mathbf{E} \right\rbrack`.
Recall Taylor expansion

.. math:: \ln{(1 - x)} = - \sum_{n = 1}^{\infty}\frac{x^{n}}{n},\forall\left| x \right| < 1 \Rightarrow H\left( x \right) = - x\ln x + \left( 1 - x \right)\sum_{n = 1}^{\infty}\frac{x^{n}}{n} = - x\ln x + \sum_{n = 1}^{\infty}\frac{x^{n}}{n} - \sum_{n = 1}^{\infty}\frac{x^{n + 1}}{n} = - x\ln x + \left( x + \frac{x^{2}}{2} + \frac{x^{3}}{3} + \frac{x^{4}}{4} + \ldots \right) - \left( x^{2} + \frac{x^{3}}{2} + \frac{x^{4}}{3} + \ldots \right) = - x\ln x + x - \sum_{n = 1}^{\infty}\frac{x^{n + 1}}{n(n + 1)}

Then since :math:`0 \leq \frac{e_{i,j}}{n_{i,j}} \leq 1`, we have

.. math:: \mathbb{H}\left\lbrack \mathbb{P} \middle| \mathbf{z},\mathbf{E} \right\rbrack = \sum_{i,j \in \left\{ 1,\ldots,k \right\}}^{}{n_{i,j}\left( - \frac{e_{i,j}}{n_{i,j}}\ln\frac{e_{i,j}}{n_{i,j}} + \frac{e_{i,j}}{n_{i,j}} - \sum_{n = 1}^{\infty}\frac{\left( \frac{e_{i,j}}{n_{i,j}} \right)^{n + 1}}{n(n + 1)} \right)} = \sum_{i,j \in \left\{ 1,\ldots,k \right\}}^{}\left( - e_{i,j}\ln\frac{e_{i,j}}{n_{i,j}} + e_{i,j} - n_{i,j}\sum_{n = 1}^{\infty}\frac{\left( \frac{e_{i,j}}{n_{i,j}} \right)^{n + 1}}{n\left( n + 1 \right)} \right) = \sum_{i,j \in \left\{ 1,\ldots,k \right\}}^{}e_{i,j} - \sum_{i,j \in \left\{ 1,\ldots,k \right\}}^{}{e_{i,j}\ln\frac{e_{i,j}}{n_{i,j}}} - \sum_{i,j \in \left\{ 1,\ldots,k \right\}}^{}\left( \frac{n_{i,j}}{n\left( n + 1 \right)}\sum_{n = 1}^{\infty}\left( \frac{e_{i,j}}{n_{i,j}} \right)^{n + 1} \right) = \left| E \right| - \sum_{i,j \in \left\{ 1,\ldots,k \right\}}^{}{e_{i,j}\ln\frac{e_{i,j}}{n_{i,j}}} - \sum_{i,j \in \left\{ 1,\ldots,k \right\}}^{}\left( \frac{1}{n\left( n + 1 \right)}\sum_{n = 1}^{\infty}\frac{e_{i,j}^{n + 1}}{n_{i,j}^{n}} \right)

Observe the terms in the last sum in above expansion is of order
:math:`O\left( \frac{e_{i,j}^{2}}{n_{i,j}} \right),\forall i,j` w.r.t.
assumption (17‑7), since the first term of the sum is
:math:`\frac{1}{n\left( n + 1 \right)}\frac{e_{i,j}^{2}}{n_{i,j}}`.
Also, it is easy to verify that
:math:`\frac{\frac{e_{i,j}^{2}}{n_{i,j}}}{e_{i,j}\ln\frac{e_{i,j}}{n_{i,j}}} = \frac{e_{i,j}}{n_{i,j}\ln\frac{e_{i,j}}{n_{i,j}}} = O\left( \frac{e_{i,j}}{n_{i,j}} \right) \rightarrow 0`,
or intuitively as a graph grows very larger and the average number of
edges between group :math:`i,j` becomes very small, the last sum becomes
unimportant in comparison to the previous sum. As a result, w.r.t.
assumption (17‑7), we can write

+-----------------------+-----------------------+-----------------------+
| .. math:: \mathbb{H}\ |                       | (17‑8)                |
| left\lbrack \mathbb{P |                       |                       |
| } \middle| \mathbf{z} |                       |                       |
| ,\mathbf{E} \right\rb |                       |                       |
| rack \approx \left| E |                       |                       |
|  \right| - \sum_{i,j  |                       |                       |
| \in \left\{ 1,\ldots, |                       |                       |
| k \right\}}^{}{e_{i,j |                       |                       |
| }\ln\frac{e_{i,j}}{n_ |                       |                       |
| {i,j}}}               |                       |                       |
+-----------------------+-----------------------+-----------------------+

If we remove the constant term :math:`|E|` in above formula, it
surprisingly looks exactly like the log-likelihood for the
degree-uncorrected multi-graph SBM we derived in (17‑4). Thus,
optimizing (17‑4) is approximately equivalent to minimizing
:math:`\mathbb{H}\left\lbrack \mathbb{P} \middle| \mathbf{z},\mathbf{E} \right\rbrack`.

+-----------------------------------------------------------------------+
| Note :math:`O\left( \frac{e_{i,j}}{n_{i,j}} \right)` means the        |
| vanishing speed is dependent on how fast                              |
| :math:`\frac{e_{i,j}}{n_{i,j}}` goes to zero. So if                   |
| :math:`\frac{e_{i,j}}{n_{i,j}} \rightarrow 0` is slow, we might need  |
| to consider keeps some leading terms in the last summation.           |
+-----------------------------------------------------------------------+

Neural Networks
===============

.. _preliminaries-1:

**Preliminaries**
-----------------

Feed-forward Network
~~~~~~~~~~~~~~~~~~~~

.. _feed-forward-network-1:

**Feed-forward Network**
^^^^^^^^^^^^^^^^^^^^^^^^

Like many classic statistical or machine learning models, a neural
network is a model with parameters :math:`\mathbf{w}` trained over
labelled data :math:`\left( \mathbf{X},\mathbf{y} \right)` and gives
regression :math:`\widehat{\mathbf{y}}`. For each new data
:math:`\mathbf{x}` with unknown label, the model gives a prediction
:math:`\widehat{y}`. One can refer to introduction of
`regression <#linear-regression>`__ and
`classification <#linear-discriminant-analysis>`__ for details. We
denote a general neural network as
:math:`\mathcal{n}\left( \mathbf{x};\mathbf{w} \right)`. A neural
network is intended to approximate complicated problems

The most basic type of neural network is named **feed-forward network**
(FFN), whose mathematically nature is straightforward -- a multiple
composition of various chosen linear and non-linear functions. The chart
of FFN reflects this nature; a simple illustration is given in . We use
superscript like “\ :math:`\left( \cdot \right)^{\left( i \right)}`\ ”
to indicate a parameter is used for the :math:`i`\ th layer. The network
represents two-layer composition of linear functions, where the first
layer is a :math:`\mathbb{R}^{2} \rightarrow \mathbb{R}^{4}` function
:math:`\mathcal{L}^{\left( 1 \right)}\left( \mathbf{x} \right) = \left( \left( \mathbf{w}_{1}^{\left( 1 \right)} \right)^{T}\mathbf{x,}\left( \mathbf{w}_{2}^{\left( 1 \right)} \right)^{T}\mathbf{x,}\left( \mathbf{w}_{3}^{\left( 1 \right)} \right)^{T}\mathbf{x,}\left( \mathbf{w}_{4}^{\left( 1 \right)} \right)^{T}\mathbf{x} \right)^{T}`,
and the second layer is a :math:`\mathbb{R}^{4}\mathbb{\rightarrow R}`
function
:math:`\mathcal{L}^{\left( 2 \right)}\left( \mathbf{x} \right) = \mathbf{w}^{\left( 2 \right)}\mathbf{x}`.
The network is itself a linear function because composition of linear
functions is linear; therefore, it can used for the purpose of linear
regression.

+-----------+--+
| |image26| |  |
+===========+==+
|           |  |
+-----------+--+

The general concept of **back-propagation** is simply running an
iterative optimization process on a chosen loss function to adjust model
parameters, so the model can gradually yield better performance (better
prediction, regression, etc.). Typically, this can be done by gradient
descent. Recall a gradient descent has the general form

.. math:: \mathbf{w}^{\left( k + 1 \right)} = \mathbf{w}^{\left( k \right)} - \alpha_{k}\mathbf{D}_{k}\nabla_{\mathbf{w}}\mathcal{n}

calculating the partial derivative of the loss function w.r.t. each
individual parameter and do gradient descent.

This is the same as what we did for many classic models. In practice,
one typically way for ba

simply taking derivatives against each parameter and

Suppose we have designed a network
:math:`\mathcal{n}\left( \mathbf{x},\mathbf{w} \right)`

**Tensorflow Basics**
^^^^^^^^^^^^^^^^^^^^^

Network construction requires input definitions. Tensorflow uses
**tf.placeholder** to declare input variables before a session is run.
The placeholders are not real input data, but symbols for the data to
input.

+-----------------------------------------------------------------------+
| tf.placeholder(dtype, #defines the type of values in the tensor       |
|                                                                       |
| shape=None, #a tuple to indicate sizes of each dimension              |
|                                                                       |
| name=None) #a string reference to the ph, so the ph can be retrieved  |
| by this name later                                                    |
+-----------------------------------------------------------------------+

**Text Learning**
-----------------

**Graph Learning**
------------------

Again, we review some basic notions for a graph. Given a set
:math:`V = \left\{ v_{1},\ldots,v_{n} \right\}`, define another set of
tuples :math:`E = \left\{ \left( u,v \right):u,v \in V \right\}`, then
the tuple of sets :math:`\left( V,E \right)` is named a **graph**,
:math:`V` is named the **vertex set**, and :math:`E` is named the **edge
set**. For convenience, let :math:`n = \left| V \right|` be the size of
vertex set, :math:`m = \left| E \right|` be the size of edge set. A
graph can be represented by a :math:`n \times n` **adjacency matrix**
:math:`\mathbf{A}` s.t.
:math:`\mathbf{A}\left( i,j \right) = \left\{ \begin{matrix}
1 & \left( v_{i},v_{j} \right) \in E \\
0 & \left( v_{i},v_{j} \right) \notin E \\
\end{matrix} \right.\ `. A weighted graph

**Graph Convolutional Network**
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

It is not appropriate to consider an infrequent subgraph as an anomaly,
because larger subgraphs are infrequent.

autoencoder: find representation, reduce dimension

reduce is better than fold for long sequence

graph convolution, share parameter

**Computer Vision**
-------------------

**Anomaly Learning**
--------------------

.. |image0| image:: media/image1.png
   :width: 2.69484in
   :height: 2.59494in
.. |image1| image:: media/image1.png
   :width: 2.69484in
   :height: 2.59494in
.. |image2| image:: media/image2.png
   :width: 2.39126in
   :height: 1.49951in
.. |image3| image:: media/image3.png
   :width: 2.46398in
   :height: 1.5in
.. |image4| image:: media/image2.png
   :width: 2.39126in
   :height: 1.49951in
.. |image5| image:: media/image3.png
   :width: 2.46398in
   :height: 1.5in
.. |image6| image:: media/image7.png
   :width: 2.41085in
   :height: 1.5in
.. |image7| image:: media/image8.png
   :width: 2.40013in
   :height: 1.5in
.. |image8| image:: media/image7.png
   :width: 2.41085in
   :height: 1.5in
.. |image9| image:: media/image8.png
   :width: 2.40013in
   :height: 1.5in
.. |image10| image:: media/image10.png
   :width: 3.39855in
   :height: 2.15229in
.. |image11| image:: media/image11.emf
   :width: 2.61789in
   :height: 1.5in
.. |image12| image:: media/image12.png
   :width: 2.47718in
   :height: 1.5in
.. |image13| image:: media/image11.emf
   :width: 2.61789in
   :height: 1.5in
.. |image14| image:: media/image12.png
   :width: 2.47718in
   :height: 1.5in
.. |image15| image:: media/image13.png
   :width: 3.86623in
   :height: 2.4in
.. |image16| image:: media/image13.png
   :width: 3.86623in
   :height: 2.4in
.. |image17| image:: media/image15.emf
   :width: 1.73776in
   :height: 1.74205in
.. |image18| image:: media/image16.emf
   :width: 1.7375in
   :height: 1.7046in
.. |image19| image:: media/image15.emf
   :width: 1.73776in
   :height: 1.74205in
.. |image20| image:: media/image16.emf
   :width: 1.7375in
   :height: 1.7046in
.. |image21| image:: media/image17.emf
   :width: 4.74068in
   :height: 2.21523in
.. |image22| image:: media/image22.emf
   :width: 5.02765in
   :height: 2.38112in
.. |image23| image:: media/image26.emf
   :width: 5.2999in
   :height: 2.53147in
.. |E:\OneDrive\Pictures\figure_3.png| image:: media/image28.png
   :width: 3.60267in
   :height: 3.5in
.. |image25| image:: media/image34.png
   :width: 5.33081in
   :height: 1.94in
.. |image26| image:: media/image35.png
   :width: 3.67755in
   :height: 2.14653in
